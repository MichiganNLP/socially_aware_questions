{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test copy model\n",
    "Through some prior tests, we have determined that the model struggles to \"remember\" the input. \n",
    "\n",
    "We hope that adding a copy mechanism, in the style of pointer-generator networks, will help the model directly copy the input words to the generated question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base', cache_dir='../../data/nyt_comments/model_cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## debugging forward pass\n",
    "# from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "# from transformers.modeling_outputs import BaseModelOutput, Seq2SeqModelOutput\n",
    "# def forward(\n",
    "#         model,\n",
    "#         input_ids=None,\n",
    "#         attention_mask=None,\n",
    "#         decoder_input_ids=None,\n",
    "#         decoder_attention_mask=None,\n",
    "#         encoder_outputs=None,\n",
    "#         past_key_values=None,\n",
    "#         inputs_embeds=None,\n",
    "#         decoder_inputs_embeds=None,\n",
    "#         use_cache=None,\n",
    "#         output_attentions=None,\n",
    "#         output_hidden_states=None,\n",
    "#         return_dict=None,\n",
    "#     ):\n",
    "\n",
    "#         # different to other models, Bart automatically creates decoder_input_ids from\n",
    "#         # input_ids if no decoder_input_ids are provided\n",
    "#         if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "#             decoder_input_ids = shift_tokens_right(\n",
    "#                 input_ids, model.config.pad_token_id, model.config.decoder_start_token_id\n",
    "#             )\n",
    "\n",
    "#         output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
    "#         output_hidden_states = (\n",
    "#             output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states\n",
    "#         )\n",
    "#         use_cache = use_cache if use_cache is not None else model.config.use_cache\n",
    "#         return_dict = return_dict if return_dict is not None else model.config.use_return_dict\n",
    "\n",
    "#         if encoder_outputs is None:\n",
    "#             encoder_outputs = model.encoder(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 inputs_embeds=inputs_embeds,\n",
    "#                 output_attentions=output_attentions,\n",
    "#                 output_hidden_states=output_hidden_states,\n",
    "#                 return_dict=return_dict,\n",
    "#             )\n",
    "#         # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "#         elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "#             encoder_outputs = BaseModelOutput(\n",
    "#                 last_hidden_state=encoder_outputs[0],\n",
    "#                 hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "#                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "#             )\n",
    "\n",
    "#         # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
    "#         ## TODO: add prior copy probabilities to decoder FML\n",
    "#         decoder_outputs = model.decoder(\n",
    "#             input_ids=decoder_input_ids,\n",
    "#             attention_mask=decoder_attention_mask,\n",
    "#             encoder_hidden_states=encoder_outputs[0],\n",
    "#             encoder_attention_mask=attention_mask,\n",
    "#             past_key_values=past_key_values,\n",
    "#             inputs_embeds=decoder_inputs_embeds,\n",
    "#             use_cache=use_cache,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "#         print(f'decoder outputs {decoder_outputs.keys()}')\n",
    "        \n",
    "#         if not return_dict:\n",
    "#             return decoder_outputs + encoder_outputs\n",
    "\n",
    "#         return Seq2SeqModelOutput(\n",
    "#             last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "#             past_key_values=decoder_outputs.past_key_values,\n",
    "#             decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "#             decoder_attentions=decoder_outputs.attentions,\n",
    "#             cross_attentions=decoder_outputs.cross_attentions,\n",
    "#             encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "#             encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "#             encoder_attentions=encoder_outputs.attentions,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir='../../data/nyt_comments/model_cache/')\n",
    "test_sents = ['this is a sentence', 'this is another sentence']\n",
    "test_target_sents = ['this sentence follows', 'this sentence follows another sentence']\n",
    "max_source_length = 1024#model.config.encoder_ffn_dim\n",
    "max_target_length = 64\n",
    "test_input = tokenizer.batch_encode_plus(test_sents, max_length=max_source_length, padding='max_length', return_tensors='pt')['input_ids']\n",
    "test_target = tokenizer.batch_encode_plus(test_target_sents, max_length=max_target_length, padding='max_length', return_tensors='pt')['input_ids']\n",
    "test_input_attention = torch.ones(test_input.shape)\n",
    "# test_target = torch.cat([torch.LongTensor(tokenizer.encode(x, add_special_tokens=True)).unsqueeze(0) for x in test_target_sents], dim=0)\n",
    "# print(test_input)\n",
    "# fix target as labels\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "test_target_shift = shift_tokens_right(test_target, model.config.pad_token_id, model.config.decoder_start_token_id)\n",
    "test_output = model.model.forward(test_input, attention_mask=test_input_attention,\n",
    "                                  decoder_input_ids=test_target_shift,\n",
    "                                  output_attentions=True, output_hidden_states=True)\n",
    "# print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _get_copy_scores(encoder_outputs, decoder_hidden, _output_copying_layer):\n",
    "    trim_encoder_outputs = encoder_outputs[:, 1:-1] # remove START/END chars\n",
    "    copy_projection = _output_copying_layer(trim_encoder_outputs)\n",
    "    copy_projection = torch.tanh(copy_projection)\n",
    "    print(f'copy projection shape {copy_projection.shape}')\n",
    "    copy_scores = copy_projection.bmm(decoder_hidden.unsqueeze(-1)).squeeze(-1) # weighted sum\n",
    "    return copy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_decoder_attention = test_output.decoder_attentions[-1].mean(axis=1)\n",
    "test_output_decoder_hidden = test_output.decoder_hidden_states[-1].mean(axis=1)\n",
    "test_output_encoder_hidden = test_output.encoder_hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "print(test_output_encoder_hidden.shape)\n",
    "print(test_output_decoder_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "decoder_dim = model.model.decoder.layers[-1].fc2.out_features\n",
    "output_layer_size = model.model.encoder.layers[-1].fc2.out_features\n",
    "# max_source_length = 1024\n",
    "_output_copying_layer = torch.nn.Linear(output_layer_size, decoder_dim)\n",
    "print(_output_copying_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy projection shape torch.Size([2, 1022, 768])\n",
      "torch.Size([2, 1022])\n"
     ]
    }
   ],
   "source": [
    "copy_scores = _get_copy_scores(test_output_encoder_hidden, test_output_decoder_hidden, _output_copying_layer)\n",
    "print(copy_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM output = torch.Size([2, 64, 50265])\n"
     ]
    }
   ],
   "source": [
    "## combine copy scores and generation scores\n",
    "test_output_lm_logits = model.lm_head(test_output[0]) + model.final_logits_bias\n",
    "print(f'LM output = {test_output_lm_logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_output_copy_gen_score = test_output_lm_logits.clone().detach()\n",
    "for i in range(copy_scores.shape[1]-1):\n",
    "    token_idx_i = test_input[:, i+1]\n",
    "    # combine copy + gen scores\n",
    "    test_output_copy_gen_score[:, :, token_idx_i] = copy_scores[:, i] + test_output_copy_gen_score[:, :, token_idx_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the loss against the target text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 50265])\n",
      "torch.Size([2, 64, 50265])\n",
      "torch.Size([128, 50265])\n"
     ]
    }
   ],
   "source": [
    "print(test_output_lm_logits.shape)\n",
    "print(test_output_copy_gen_score.shape)\n",
    "# print(clip_test_output_copy_gen_score.shape)\n",
    "print(test_output_copy_gen_score.view(-1, model.config.vocab_size).shape)\n",
    "# print(clip_test_output_copy_gen_score.reshape(-1, model.config.vocab_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tmp debugging: how to convert target-length labels to source-length to match model output?\n",
    "# import sys\n",
    "# if('question_generation' not in sys.path):\n",
    "#     sys.path.append('question_generation')\n",
    "# from data_collator import T2TDataCollator\n",
    "# data_collator = T2TDataCollator(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model_type='bart',\n",
    "#     mode='training',\n",
    "#     using_tpu=False,\n",
    "# )\n",
    "# from trainer import Trainer\n",
    "# from train_basic_question_generation import load_training_args\n",
    "# training_args = load_training_args(\n",
    "#     '../../data/nyt_comments/', '../../data/nyt_comments/no_author_data/NYT_train_data.csv',\n",
    "#     '../../data/nyt_comments/model_cache/', '../../data/nyt_comments/no_author_data/NYT_val_data.csv', \n",
    "#     1024, 64,\n",
    ")\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(242.1796, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "# clip_test_output_copy_gen_score = test_output_copy_gen_score[:, :test_target.shape[1], :]\n",
    "loss_func = CrossEntropyLoss()\n",
    "loss = loss_func(test_output_copy_gen_score.reshape(-1, model.config.vocab_size), test_target.view(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add a coverage loss, to penalize the model learning to copy too much. Something like this?\n",
    "\n",
    "$c_{t} = \\sum_{i}min(a_i, c_{t-i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: coverage loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test basic copy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this code to the model and try out the model with a basic copy task, copying names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def load_copy_data(tokenizer):\n",
    "    input_sent_structure = 'Hi my name is NAME'\n",
    "    output_sent_structure = 'Nice to meet you NAME'\n",
    "    name_data_url = 'https://raw.githubusercontent.com/smashew/NameDatabases/master/NamesDatabases/first%20names/us.txt'\n",
    "    name_data_raw = requests.get(name_data_url)\n",
    "    name_data_text = name_data_raw.text.split('\\r\\n')\n",
    "#     print(name_data_text[:10])\n",
    "    copy_input_text = list(map(lambda x: input_sent_structure.replace('NAME', x), name_data_text))\n",
    "    copy_target_text = list(map(lambda x: output_sent_structure.replace('NAME', x), name_data_text))\n",
    "    ## convert to train/test etc.\n",
    "    N_train = int(len(name_data_text) * 0.8)\n",
    "    copy_input_train_text = copy_input_text[:N_train]\n",
    "    copy_target_train_text = copy_target_text[:N_train]\n",
    "    copy_input_test_text = copy_input_text[N_train:]\n",
    "    copy_target_test_text = copy_target_text[N_train:]\n",
    "    ## convert to data\n",
    "    # load tokenizer\n",
    "    from transformers import BartTokenizer\n",
    "    max_input_length = 1024\n",
    "    max_target_length = 64\n",
    "#     tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir=tokenizer_dir)\n",
    "    copy_input_train_data = tokenizer.batch_encode_plus(copy_input_train_text, padding=True, max_length=max_input_length, return_tensors='pt')\n",
    "    copy_target_train_data = tokenizer.batch_encode_plus(copy_target_train_text, padding=True, max_length=max_target_length, return_tensors='pt')\n",
    "    copy_input_test_data = tokenizer.batch_encode_plus(copy_input_test_text, padding=True, max_length=max_input_length, return_tensors='pt')\n",
    "    copy_target_test_data = tokenizer.batch_encode_plus(copy_target_test_text, padding=True, max_length=max_target_length, return_tensors='pt')\n",
    "    from datasets import Dataset\n",
    "    # from nlp import load_dataset\n",
    "    copy_train_data = {'source_text' : copy_input_train_text, 'target_text' : copy_target_train_text,\n",
    "                       'source_ids' : copy_input_train_data['input_ids'], 'target_ids' : copy_target_train_data['input_ids'],\n",
    "                       'input_ids' : copy_input_train_data['input_ids'], 'labels' : copy_target_train_data['input_ids'],\n",
    "                       'attention_mask' : copy_input_train_data['attention_mask']}\n",
    "    copy_test_data = {'source_text' : copy_input_test_text, 'target_text' : copy_target_test_text,\n",
    "                      'source_ids' : copy_input_test_data['input_ids'], 'target_ids' : copy_target_test_data['input_ids'],\n",
    "                      'input_ids' : copy_input_test_data['input_ids'], 'labels' : copy_input_test_data['input_ids'],\n",
    "                      'attention_mask' : copy_input_test_data['attention_mask']}\n",
    "    copy_train_dataset = Dataset.from_dict(copy_train_data)\n",
    "    copy_test_dataset = Dataset.from_dict(copy_test_data)\n",
    "    copy_train_dataset.set_format('pt', columns=['source_ids', 'target_ids', 'attention_mask', 'input_ids', 'labels'])\n",
    "    copy_test_dataset.set_format('pt', columns=['source_ids', 'target_ids', 'attention_mask', 'input_ids', 'labels'])\n",
    "    return copy_train_dataset, copy_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import copy_model\n",
    "reload(copy_model)\n",
    "from copy_model import CopyGenerationModel\n",
    "from transformers import BartConfig\n",
    "config = BartConfig.from_json_file('../../data/nyt_comments/model_cache/BART_config.json')\n",
    "copy_gen_model = CopyGenerationModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir='../../data/nyt_comments/model_cache/')\n",
    "copy_train_dataset, copy_test_dataset = load_copy_data(tokenizer)\n",
    "# from datasets import Dataset\n",
    "# # from nlp import load_dataset\n",
    "# copy_train_data = {'source_text' : copy_input_train_text, 'target_text' : copy_target_train_text,\n",
    "#                    'source_ids' : copy_input_train_data['input_ids'], 'target_ids' : copy_target_train_data['input_ids'],\n",
    "#                    'input_ids' : copy_input_train_data['input_ids'], 'labels' : copy_target_train_data['input_ids'],\n",
    "#                    'attention_mask' : copy_input_train_data['attention_mask']}\n",
    "# copy_test_data = {'source_text' : copy_input_test_text, 'target_text' : copy_target_test_text,\n",
    "#                   'source_ids' : copy_input_test_data['input_ids'], 'target_ids' : copy_target_test_data['input_ids'],\n",
    "#                   'input_ids' : copy_input_test_data['input_ids'], 'labels' : copy_input_test_data['input_ids'],\n",
    "#                   'attention_mask' : copy_input_test_data['attention_mask']}\n",
    "# copy_train_dataset = Dataset.from_dict(copy_train_data)\n",
    "# copy_test_dataset = Dataset.from_dict(copy_test_data)\n",
    "# copy_train_dataset.set_format('pt', columns=['source_ids', 'target_ids', 'attention_mask', 'input_ids', 'labels'])\n",
    "# copy_test_dataset.set_format('pt', columns=['source_ids', 'target_ids', 'attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get trainer etc.\n",
    "import sys\n",
    "if('question_generation' not in sys.path):\n",
    "    sys.path.append('question_generation')\n",
    "import data_collator\n",
    "reload(data_collator)\n",
    "from data_collator import T2TDataCollator\n",
    "import trainer\n",
    "reload(trainer)\n",
    "from trainer import Trainer\n",
    "# from transformers.trainer import Trainer\n",
    "model_type = 'bart'\n",
    "data_collator = T2TDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    model_type=model_type,\n",
    "    mode='training',\n",
    "    using_tpu=False,\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='runs/copy_model/',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    no_cuda=True,\n",
    "    save_total_limit=2,\n",
    "    seed=123,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=copy_gen_model,\n",
    "#     args=training_args,\n",
    "    train_dataset=copy_train_dataset,\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2585' max='2585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2585/2585 30:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2585, training_loss=0.05317395162305703, metrics={'train_runtime': 1857.6261, 'train_samples_per_second': 1.392, 'total_flos': 190827283046400, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## evaluate\n",
    "# copy_gen_model.eval()\n",
    "# copy_target_test_gen = copy_gen_model(copy_input_test_data['input_ids'], \n",
    "#                                       attention_mask=copy_input_test_data['attention_mask'], \n",
    "#                                       labels=copy_target_test_data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(copy_input_test_data['input_ids'].squeeze())\n",
    "# print(copy_input_train_data['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp debugging => how to generate with copy mechanism?\n",
    "# from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "# from transformers.generation_utils import (\n",
    "#     GreedySearchEncoderDecoderOutput, \n",
    "#     GreedySearchDecoderOnlyOutput, \n",
    "#     SampleEncoderDecoderOutput, \n",
    "#     SampleDecoderOnlyOutput, \n",
    "#     BeamSearchEncoderDecoderOutput, \n",
    "#     BeamSearchDecoderOnlyOutput, \n",
    "#     BeamSampleEncoderDecoderOutput, \n",
    "#     BeamSampleDecoderOnlyOutput\n",
    "# )\n",
    "# from transformers.file_utils import ModelOutput\n",
    "# from transformers.generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "# from transformers.generation_logits_process import (\n",
    "#     HammingDiversityLogitsProcessor,\n",
    "#     LogitsProcessorList,\n",
    "#     MinLengthLogitsProcessor,\n",
    "#     NoBadWordsLogitsProcessor,\n",
    "#     NoRepeatNGramLogitsProcessor,\n",
    "#     PrefixConstrainedLogitsProcessor,\n",
    "#     RepetitionPenaltyLogitsProcessor,\n",
    "#     TemperatureLogitsWarper,\n",
    "#     TopKLogitsWarper,\n",
    "#     TopPLogitsWarper,\n",
    "# )\n",
    "# GreedySearchOutput = Union[GreedySearchEncoderDecoderOutput, GreedySearchDecoderOnlyOutput]\n",
    "# SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]\n",
    "# BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]\n",
    "# BeamSampleOutput = Union[BeamSampleEncoderDecoderOutput, BeamSampleDecoderOnlyOutput]\n",
    "# ## debug forward pass\n",
    "# def custom_forward(\n",
    "#         model,\n",
    "#         input_ids=None,\n",
    "#         attention_mask=None,\n",
    "#         decoder_input_ids=None,\n",
    "#         decoder_attention_mask=None,\n",
    "#         encoder_outputs=None,\n",
    "#         past_key_values=None,\n",
    "#         inputs_embeds=None,\n",
    "#         decoder_inputs_embeds=None,\n",
    "#         labels=None,\n",
    "#         use_cache=None,\n",
    "#         output_attentions=True, # need decoder attention to get copy working\n",
    "#         output_hidden_states=True, # need encoder output to get copy working\n",
    "#         return_dict=None,\n",
    "#     ):\n",
    "#     # tmp debugging\n",
    "#     # print(f'passing input IDs forward {input_ids}')\n",
    "#     return_dict = return_dict if return_dict is not None else model.config.use_return_dict\n",
    "\n",
    "#     if labels is not None:\n",
    "#         if decoder_input_ids is None:\n",
    "#             decoder_input_ids = shift_tokens_right(\n",
    "#                 labels, model.config.pad_token_id,\n",
    "#                 model.config.decoder_start_token_id\n",
    "#             )\n",
    "#     # tmp debugging\n",
    "#     print(f'forward: input IDs {input_ids}')\n",
    "#     print(f'forward: encoder outputs {encoder_outputs.last_hidden_state.shape}')\n",
    "# #     print(f'decoder IDs has shape {decoder_input_ids.shape}')\n",
    "#     outputs = model.model(\n",
    "#         input_ids,\n",
    "#         attention_mask=attention_mask,\n",
    "#         decoder_input_ids=decoder_input_ids,\n",
    "#         encoder_outputs=encoder_outputs,\n",
    "#         decoder_attention_mask=decoder_attention_mask,\n",
    "#         past_key_values=past_key_values,\n",
    "#         inputs_embeds=inputs_embeds,\n",
    "#         decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "#         use_cache=use_cache,\n",
    "#         output_attentions=output_attentions,\n",
    "#         output_hidden_states=output_hidden_states,\n",
    "#         return_dict=return_dict,\n",
    "#     )\n",
    "#     lm_logits = model.lm_head(outputs[0]) + model.final_logits_bias\n",
    "#     # decoder_hidden = outputs.decoder_hidden_states[-1]\n",
    "#     # decoder_attention = outputs.decoder_attentions[-1]\n",
    "#     # compute average over all heads\n",
    "#     # decoder_attention = decoder_attention.mean(axis=1)\n",
    "#     # tmp debugging\n",
    "#     print(f'forward: encoder hidden states has shape {outputs.encoder_hidden_states[-1].shape}')\n",
    "#     print(f'forward: decoder hidden states has shape {outputs.decoder_hidden_states[-1].shape}')\n",
    "#     decoder_hidden = outputs.decoder_hidden_states[-1].mean(axis=1) # mean over all dimensions?\n",
    "#     encoder_hidden = outputs.encoder_hidden_states[-1]\n",
    "\n",
    "#     ## compute copy probabilities\n",
    "#     copy_scores = model._get_copy_scores(encoder_hidden, decoder_hidden)\n",
    "#     # target to source\n",
    "#     # target_to_source = input_ids ==\n",
    "#     ## add to generation probabilities\n",
    "#     # copy_gen_score = lm_logits.clone().detach()\n",
    "#     for i in range(copy_scores.shape[1] - 1):\n",
    "#         token_idx_i = input_ids[:, i + 1]\n",
    "#         # combine copy + gen scores\n",
    "#         lm_logits[:, :, token_idx_i] = copy_scores[:, i] + lm_logits[:, :, token_idx_i]\n",
    "#     # lm_logits = copy_gen_score.clone().detach()\n",
    "\n",
    "#     # source_to_target = torch.zeros(input_ids.shape)\n",
    "#     # source_to_target = []\n",
    "#     # for i in range(input_ids.shape[0]):\n",
    "#     #     target_slice = set(labels[i, :])\n",
    "#     #     input_slice = input_ids[i, :]\n",
    "#     #     source_to_target_slice = [source_id if source_id in target_slice else model.oov_index for j, source_id in enumerate(input_slice)]\n",
    "#     #     source_to_target.append(source_to_target_slice)\n",
    "#     # source_to_target = torch.LongTensor(source_to_target)\n",
    "#     #\n",
    "#     # final_log_probs = model._gather_final_log_probs(lm_logits, copy_scores, source_to_target, input_ids)\n",
    "\n",
    "#     # generation_score_mask = outputs[1]\n",
    "#     # log_likelihood, selective_weights = model._get_ll_contrib(\n",
    "#     #     lm_logits, generation_score_mask, copy_scores,\n",
    "#     # )\n",
    "\n",
    "#     masked_lm_loss = None\n",
    "#     if labels is not None:\n",
    "#         loss_fct = CrossEntropyLoss()\n",
    "#         ## TODO: coverage loss from copy scores to avoid repeating the same words; with hyperparameter??\n",
    "#         masked_lm_loss = loss_fct(\n",
    "#             lm_logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "#     if not return_dict:\n",
    "#         output = (lm_logits,) + outputs[1:]\n",
    "#         return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "#     return Seq2SeqLMOutput(\n",
    "#         loss=masked_lm_loss,\n",
    "#         logits=lm_logits,\n",
    "#         past_key_values=outputs.past_key_values,\n",
    "#         decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "#         decoder_attentions=outputs.decoder_attentions,\n",
    "#         cross_attentions=outputs.cross_attentions,\n",
    "#         encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "#         encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "#         encoder_attentions=outputs.encoder_attentions,\n",
    "# )\n",
    "# ## debug beam search FML\n",
    "# def custom_beam_search(\n",
    "#         model,\n",
    "#         input_ids: torch.LongTensor,\n",
    "#         beam_scorer: BeamScorer,\n",
    "#         logits_processor: Optional[LogitsProcessorList] = None,\n",
    "#         max_length: Optional[int] = None,\n",
    "#         pad_token_id: Optional[int] = None,\n",
    "#         eos_token_id: Optional[int] = None,\n",
    "#         output_attentions: Optional[bool] = None,\n",
    "#         output_hidden_states: Optional[bool] = None,\n",
    "#         output_scores: Optional[bool] = None,\n",
    "#         return_dict_in_generate: Optional[bool] = None,\n",
    "#         **model_kwargs,\n",
    "#     ) -> Union[BeamSearchOutput, torch.LongTensor]:\n",
    "#     r\"\"\"\n",
    "#     Generates sequences for models with a language modeling head using beam search decoding.\n",
    "\n",
    "#     Parameters:\n",
    "\n",
    "#         input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "#             The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "#             :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "#         beam_scorer (:obj:`BeamScorer`):\n",
    "#             An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "#             constructed, stored and sorted during generation. For more information, the documentation of\n",
    "#             :class:`~transformers.BeamScorer` should be read.\n",
    "#         logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "#             An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "#             :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "#             head applied at each generation step.\n",
    "#         max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "#             The maximum length of the sequence to be generated.\n",
    "#         pad_token_id (:obj:`int`, `optional`):\n",
    "#             The id of the `padding` token.\n",
    "#         eos_token_id (:obj:`int`, `optional`):\n",
    "#             The id of the `end-of-sequence` token.\n",
    "#         output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#             Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "#             returned tensors for more details.\n",
    "#         output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#             Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "#             for more details.\n",
    "#         output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#             Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "#         return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#             Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "#         model_kwargs:\n",
    "#             Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "#             model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "#     Return:\n",
    "#         :class:`~transformers.generation_utilsBeamSearchDecoderOnlyOutput`,\n",
    "#         :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "#         :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "#         :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "#         ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "#         :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
    "#         ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "\n",
    "#     Examples::\n",
    "\n",
    "#         >>> from transformers import (\n",
    "#         ...    AutoTokenizer,\n",
    "#         ...    AutoModelForSeq2SeqLM,\n",
    "#         ...    LogitsProcessorList,\n",
    "#         ...    MinLengthLogitsProcessor,\n",
    "#         ...    BeamSearchScorer,\n",
    "#         ... )\n",
    "#         >>> import torch\n",
    "\n",
    "#         >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "#         >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "#         >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "#         >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "#         >>> # lets run beam search using 3 beams\n",
    "#         >>> num_beams = 3\n",
    "#         >>> # define decoder start token ids\n",
    "#         >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "#         >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "#         >>> # add encoder_outputs to model keyword arguments\n",
    "#         >>> model_kwargs = {\n",
    "#         ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "#         ... }\n",
    "\n",
    "#         >>> # instantiate beam scorer\n",
    "#         >>> beam_scorer = BeamSearchScorer(\n",
    "#         ...     batch_size=1,\n",
    "#         ...     max_length=model.config.max_length,\n",
    "#         ...     num_beams=num_beams,\n",
    "#         ...     device=model.device,\n",
    "#         ... )\n",
    "\n",
    "#         >>> # instantiate logits processors\n",
    "#         >>> logits_processor = LogitsProcessorList([\n",
    "#         ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "#         ... ])\n",
    "\n",
    "#         >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "#         >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "#     \"\"\"\n",
    "\n",
    "#     # init values\n",
    "#     logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "#     max_length = max_length if max_length is not None else model.config.max_length\n",
    "#     pad_token_id = pad_token_id if pad_token_id is not None else model.config.pad_token_id\n",
    "#     eos_token_id = eos_token_id if eos_token_id is not None else model.config.eos_token_id\n",
    "#     output_scores = output_scores if output_scores is not None else model.config.output_scores\n",
    "#     output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
    "#     output_hidden_states = (\n",
    "#         output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states\n",
    "#     )\n",
    "#     return_dict_in_generate = (\n",
    "#         return_dict_in_generate if return_dict_in_generate is not None else model.config.return_dict_in_generate\n",
    "#     )\n",
    "\n",
    "#     # init attention / hidden states / scores tuples\n",
    "#     scores = () if (return_dict_in_generate and output_scores) else None\n",
    "#     decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "#     decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "#     # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "#     if return_dict_in_generate and model.config.is_encoder_decoder:\n",
    "#         encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "#         encoder_hidden_states = (\n",
    "#             model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "#         )\n",
    "\n",
    "#     batch_size = len(beam_scorer._beam_hyps)\n",
    "#     num_beams = beam_scorer.num_beams\n",
    "\n",
    "#     batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "#     assert (\n",
    "#         num_beams * batch_size == batch_beam_size\n",
    "#     ), \"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "\n",
    "#     beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "#     beam_scores[:, 1:] = -1e9\n",
    "#     beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "#     while cur_len < max_length:\n",
    "#         model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "#         # tmp debugging\n",
    "#         print(f'beam search: model inputs {model_inputs.keys()}')\n",
    "#         print(f'beam search: decoder input {model_inputs[\"decoder_input_ids\"].shape}')\n",
    "# #         print(f'model input shape {}')\n",
    "# #         outputs = model(\n",
    "# #             **model_inputs,\n",
    "# #             return_dict=True,\n",
    "# #             output_attentions=output_attentions,\n",
    "# #             output_hidden_states=output_hidden_states,\n",
    "# #         )\n",
    "#         # tmp debugging\n",
    "#         outputs = custom_forward(\n",
    "#             model,\n",
    "#             **model_inputs,\n",
    "#             return_dict=True,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#         )\n",
    "#         next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "#         # adjust tokens for Bart, *e.g.*\n",
    "#         next_token_logits = model.adjust_logits_during_generation(\n",
    "#             next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "#         )\n",
    "\n",
    "#         next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "#         next_token_scores = logits_processor(input_ids, next_token_scores)\n",
    "#         next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "\n",
    "#         # Store scores, attentions and hidden_states when required\n",
    "#         if return_dict_in_generate:\n",
    "#             if output_scores:\n",
    "#                 scores += (next_token_scores,)\n",
    "#             if output_attentions:\n",
    "#                 decoder_attentions += (\n",
    "#                     (outputs.decoder_attentions,) if model.config.is_encoder_decoder else (outputs.attentions,)\n",
    "#                 )\n",
    "\n",
    "#             if output_hidden_states:\n",
    "#                 decoder_hidden_states += (\n",
    "#                     (outputs.decoder_hidden_states,)\n",
    "#                     if model.config.is_encoder_decoder\n",
    "#                     else (outputs.hidden_states,)\n",
    "#                 )\n",
    "\n",
    "#         # reshape for beam search\n",
    "#         vocab_size = next_token_scores.shape[-1]\n",
    "#         next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "#         next_token_scores, next_tokens = torch.topk(\n",
    "#             next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
    "#         )\n",
    "\n",
    "#         next_indices = next_tokens // vocab_size\n",
    "#         next_tokens = next_tokens % vocab_size\n",
    "\n",
    "#         # stateless\n",
    "#         beam_outputs = beam_scorer.process(\n",
    "#             input_ids,\n",
    "#             next_token_scores,\n",
    "#             next_tokens,\n",
    "#             next_indices,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#         )\n",
    "#         beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "#         beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "#         beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "#         input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "#         cur_len = cur_len + 1\n",
    "\n",
    "#         model_kwargs = model._update_model_kwargs_for_generation(\n",
    "#             outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder\n",
    "#         )\n",
    "#         if model_kwargs[\"past\"] is not None:\n",
    "#             model_kwargs[\"past\"] = model._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "#         if beam_scorer.is_done:\n",
    "#             break\n",
    "\n",
    "#     sequence_outputs = beam_scorer.finalize(\n",
    "#         input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "#     )\n",
    "\n",
    "#     if return_dict_in_generate:\n",
    "#         if not output_scores:\n",
    "#             sequence_outputs[\"sequence_scores\"] = None\n",
    "#         if model.config.is_encoder_decoder:\n",
    "#             return BeamSearchEncoderDecoderOutput(\n",
    "#                 sequences=sequence_outputs[\"sequences\"],\n",
    "#                 sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "#                 scores=scores,\n",
    "#                 encoder_attentions=encoder_attentions,\n",
    "#                 encoder_hidden_states=encoder_hidden_states,\n",
    "#                 decoder_attentions=decoder_attentions,\n",
    "#                 decoder_hidden_states=decoder_hidden_states,\n",
    "#             )\n",
    "#         else:\n",
    "#             return BeamSearchDecoderOnlyOutput(\n",
    "#                 sequences=sequence_outputs[\"sequences\"],\n",
    "#                 sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "#                 scores=scores,\n",
    "#                 attentions=decoder_attentions,\n",
    "#                 hidden_states=decoder_hidden_states,\n",
    "#             )\n",
    "#     else:\n",
    "#         return sequence_outputs[\"sequences\"]\n",
    "# ## debug generation\n",
    "# def custom_generate(\n",
    "#     model,\n",
    "#     input_ids: Optional[torch.LongTensor] = None,\n",
    "#     max_length: Optional[int] = None,\n",
    "#     min_length: Optional[int] = None,\n",
    "#     do_sample: Optional[bool] = None,\n",
    "#     early_stopping: Optional[bool] = None,\n",
    "#     num_beams: Optional[int] = None,\n",
    "#     temperature: Optional[float] = None,\n",
    "#     top_k: Optional[int] = None,\n",
    "#     top_p: Optional[float] = None,\n",
    "#     repetition_penalty: Optional[float] = None,\n",
    "#     bad_words_ids: Optional[Iterable[int]] = None,\n",
    "#     bos_token_id: Optional[int] = None,\n",
    "#     pad_token_id: Optional[int] = None,\n",
    "#     eos_token_id: Optional[int] = None,\n",
    "#     length_penalty: Optional[float] = None,\n",
    "#     no_repeat_ngram_size: Optional[int] = None,\n",
    "#     num_return_sequences: Optional[int] = None,\n",
    "#     decoder_start_token_id: Optional[int] = None,\n",
    "#     use_cache: Optional[bool] = None,\n",
    "#     num_beam_groups: Optional[int] = None,\n",
    "#     diversity_penalty: Optional[float] = None,\n",
    "#     prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "#     output_attentions: Optional[bool] = None,\n",
    "#     output_hidden_states: Optional[bool] = None,\n",
    "#     output_scores: Optional[bool] = None,\n",
    "#     return_dict_in_generate: Optional[bool] = None,\n",
    "#         **model_kwargs,\n",
    "#     ) -> Union[GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput, torch.LongTensor]:\n",
    "#         r\"\"\"\n",
    "#         Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
    "#         multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
    "\n",
    "#         Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
    "#         attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
    "#         indicated are the default values of those config.\n",
    "\n",
    "#         Most of these parameters are explained in more detail in `this blog post\n",
    "#         <https://huggingface.co/blog/how-to-generate>`__.\n",
    "\n",
    "#         Parameters:\n",
    "\n",
    "#             input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "#                 The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "#                 :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "#             max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "#                 The maximum length of the sequence to be generated.\n",
    "#             min_length (:obj:`int`, `optional`, defaults to 10):\n",
    "#                 The minimum length of the sequence to be generated.\n",
    "#             do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "#                 Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "#             early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "#                 Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
    "#             num_beams (:obj:`int`, `optional`, defaults to 1):\n",
    "#                 Number of beams for beam search. 1 means no beam search.\n",
    "#             temperature (:obj:`float`, `optional`, defaults tp 1.0):\n",
    "#                 The value used to module the next token probabilities.\n",
    "#             top_k (:obj:`int`, `optional`, defaults to 50):\n",
    "#                 The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "#             top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
    "#                 If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
    "#                 higher are kept for generation.\n",
    "#             repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "#                 The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
    "#                 <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
    "#             pad_token_id (:obj:`int`, `optional`):\n",
    "#                 The id of the `padding` token.\n",
    "#             bos_token_id (:obj:`int`, `optional`):\n",
    "#                 The id of the `beginning-of-sequence` token.\n",
    "#             eos_token_id (:obj:`int`, `optional`):\n",
    "#                 The id of the `end-of-sequence` token.\n",
    "#             length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "#                 Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
    "#                 model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
    "#                 sequences.\n",
    "#             no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
    "#                 If set to int > 0, all ngrams of that size can only occur once.\n",
    "#             bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
    "#                 List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
    "#                 should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
    "#                 add_prefix_space=True).input_ids`.\n",
    "#             num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
    "#                 The number of independently computed returned sequences for each element in the batch.\n",
    "#             attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "#                 Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
    "#                 tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
    "#                 shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
    "#                 <../glossary.html#attention-mask>`__\n",
    "#             decoder_start_token_id (:obj:`int`, `optional`):\n",
    "#                 If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
    "#             use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "#                 Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
    "#                 speed up decoding.\n",
    "#             num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
    "#                 Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "#                 beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "#             diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
    "#                 This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
    "#                 at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
    "#                 enabled.\n",
    "#             prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
    "#                 If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
    "#                 provided no constraint is applied. This function takes 2 arguments :obj:`inputs_ids` and the batch ID\n",
    "#                 :obj:`batch_id`. It has to return a list with the allowed tokens for the next generation step\n",
    "#                 conditioned on the previously generated tokens :obj:`inputs_ids` and the batch ID :obj:`batch_id`. This\n",
    "#                 argument is useful for constrained generation conditioned on the prefix, as described in\n",
    "#                 `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
    "#             output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#                 Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "#                 returned tensors for more details.\n",
    "#             output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#                 Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "#                 for more details.\n",
    "#             output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#                 Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "#             return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "#                 Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "#             model_kwargs:\n",
    "#                 Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
    "#                 model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
    "#                 kwargs should be prefixed with `decoder_`.\n",
    "\n",
    "#         Return:\n",
    "#             :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
    "#             :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
    "#             ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
    "\n",
    "#                 If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
    "#                 possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "#                     - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
    "#                     - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
    "#                     - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
    "#                     - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
    "\n",
    "#                 If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
    "#                 :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "#                     - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
    "#                     - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
    "#                     - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
    "#                     - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
    "\n",
    "#         Examples::\n",
    "#             >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "#             >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "#             >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "#             >>> # do greedy decoding without providing a prompt\n",
    "#             >>> outputs = model.generate(max_length=40)\n",
    "#             >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "#             >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "#             >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "#             >>> document = (\n",
    "#             ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
    "#             ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
    "#             ... )\n",
    "#             >>> # encode input contex\n",
    "#             >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
    "#             >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
    "#             >>> # with T5 encoder-decoder model conditioned on short news article.\n",
    "#             >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "#             >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "#             >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "#             >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "#             >>> input_context = \"The dog\"\n",
    "#             >>> # encode input context\n",
    "#             >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "#             >>> # generate 3 candidates using sampling\n",
    "#             >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
    "#             >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "#             >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
    "#             >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
    "#             >>> # \"Legal\" is one of the control codes for ctrl\n",
    "#             >>> input_context = \"Legal My neighbor is\"\n",
    "#             >>> # encode input context\n",
    "#             >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "#             >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
    "#             >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "#             >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "#             >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "#             >>> input_context = \"My cute dog\"\n",
    "#             >>> # get tokens of words that should not be generated\n",
    "#             >>> bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [\"idiot\", \"stupid\", \"shut up\"]]\n",
    "#             >>> # encode input context\n",
    "#             >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "#             >>> # generate sequences without allowing bad_words to be generated\n",
    "#             >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
    "#             >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "#         \"\"\"\n",
    "\n",
    "#         # set init values\n",
    "#         num_beams = num_beams if num_beams is not None else model.config.num_beams\n",
    "#         num_beam_groups = num_beam_groups if num_beam_groups is not None else model.config.num_beam_groups\n",
    "#         max_length = max_length if max_length is not None else model.config.max_length\n",
    "#         do_sample = do_sample if do_sample is not None else model.config.do_sample\n",
    "#         num_return_sequences = (\n",
    "#             num_return_sequences if num_return_sequences is not None else model.config.num_return_sequences\n",
    "#         )\n",
    "\n",
    "#         pad_token_id = pad_token_id if pad_token_id is not None else model.config.pad_token_id\n",
    "#         bos_token_id = bos_token_id if bos_token_id is not None else model.config.bos_token_id\n",
    "#         eos_token_id = eos_token_id if eos_token_id is not None else model.config.eos_token_id\n",
    "\n",
    "#         output_scores = output_scores if output_scores is not None else model.config.output_scores\n",
    "#         output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
    "#         output_hidden_states = (\n",
    "#             output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states\n",
    "#         )\n",
    "#         return_dict_in_generate = (\n",
    "#             return_dict_in_generate if return_dict_in_generate is not None else model.config.return_dict_in_generate\n",
    "#         )\n",
    "\n",
    "#         model_kwargs[\"output_attentions\"] = output_attentions\n",
    "#         model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "\n",
    "#         if input_ids is None:\n",
    "#             # init `input_ids` with bos_token_id\n",
    "#             input_ids = model._prepare_input_ids_for_generation(bos_token_id)\n",
    "\n",
    "#         if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "#             # init `attention_mask` depending on `pad_token_id`\n",
    "#             model_kwargs[\"attention_mask\"] = model._prepare_attention_mask_for_generation(\n",
    "#                 input_ids, pad_token_id, eos_token_id\n",
    "#             )\n",
    "\n",
    "#         # special case if pad_token_id is not defined\n",
    "#         if pad_token_id is None and eos_token_id is not None:\n",
    "#             logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "#             pad_token_id = eos_token_id\n",
    "\n",
    "#         if model.config.is_encoder_decoder:\n",
    "#             # add encoder_outputs to model_kwargs\n",
    "#             model_kwargs = model._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n",
    "\n",
    "#             # set input_ids as decoder_input_ids\n",
    "#             if \"decoder_input_ids\" in model_kwargs:\n",
    "#                 input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "#             else:\n",
    "#                 input_ids = model._prepare_decoder_input_ids_for_generation(\n",
    "#                     input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id\n",
    "#                 )\n",
    "\n",
    "#             if \"encoder_outputs\" not in model_kwargs or not isinstance(model_kwargs[\"encoder_outputs\"], ModelOutput):\n",
    "#                 raise ValueError(\"Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.\")\n",
    "\n",
    "#         if input_ids.shape[-1] >= max_length:\n",
    "#             input_ids_string = \"decoder_input_ids\" if model.config.is_encoder_decoder else \"input_ids\"\n",
    "#             logger.warning(\n",
    "#                 f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.\"\n",
    "#                 \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "#             )\n",
    "\n",
    "#         # determine generation mode\n",
    "#         is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
    "#         is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
    "#         is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
    "#         is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
    "#         is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
    "#         if num_beam_groups > num_beams:\n",
    "#             raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "#         if is_group_beam_gen_mode and do_sample is True:\n",
    "#             raise ValueError(\n",
    "#                 \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "#             )\n",
    "\n",
    "#         # set model_kwargs\n",
    "#         model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "#         # get distribution pre_processing samplers\n",
    "#         logits_processor = model._get_logits_processor(\n",
    "#             repetition_penalty=repetition_penalty,\n",
    "#             no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "#             bad_words_ids=bad_words_ids,\n",
    "#             min_length=min_length,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "#             num_beams=num_beams,\n",
    "#             num_beam_groups=num_beam_groups,\n",
    "#             diversity_penalty=diversity_penalty,\n",
    "#         )\n",
    "\n",
    "#         if is_greedy_gen_mode:\n",
    "#             if num_return_sequences > 1:\n",
    "#                 raise ValueError(\n",
    "#                     f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "#                 )\n",
    "\n",
    "#             # greedy search\n",
    "#             return model.greedy_search(\n",
    "#                 input_ids,\n",
    "#                 logits_processor=logits_processor,\n",
    "#                 max_length=max_length,\n",
    "#                 pad_token_id=pad_token_id,\n",
    "#                 eos_token_id=eos_token_id,\n",
    "#                 output_scores=output_scores,\n",
    "#                 return_dict_in_generate=return_dict_in_generate,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#         elif is_sample_gen_mode:\n",
    "#             # get probability distribution warper\n",
    "#             logits_warper = model._get_logits_warper(\n",
    "#                 top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "#             )\n",
    "\n",
    "#             # expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "#             input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "#                 input_ids,\n",
    "#                 expand_size=num_return_sequences,\n",
    "#                 is_encoder_decoder=model.config.is_encoder_decoder,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#             # sample\n",
    "#             return model.sample(\n",
    "#                 input_ids,\n",
    "#                 logits_processor=logits_processor,\n",
    "#                 logits_warper=logits_warper,\n",
    "#                 max_length=max_length,\n",
    "#                 pad_token_id=pad_token_id,\n",
    "#                 eos_token_id=eos_token_id,\n",
    "#                 output_scores=output_scores,\n",
    "#                 return_dict_in_generate=return_dict_in_generate,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#         elif is_beam_gen_mode:\n",
    "#             batch_size = input_ids.shape[0]\n",
    "\n",
    "#             length_penalty = length_penalty if length_penalty is not None else model.config.length_penalty\n",
    "#             early_stopping = early_stopping if early_stopping is not None else model.config.early_stopping\n",
    "\n",
    "#             if num_return_sequences > num_beams:\n",
    "#                 raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "#             beam_scorer = BeamSearchScorer(\n",
    "#                 batch_size=batch_size,\n",
    "#                 max_length=max_length,\n",
    "#                 num_beams=num_beams,\n",
    "#                 device=model.device,\n",
    "#                 length_penalty=length_penalty,\n",
    "#                 do_early_stopping=early_stopping,\n",
    "#                 num_beam_hyps_to_keep=num_return_sequences,\n",
    "#             )\n",
    "#             # interleave with `num_beams`\n",
    "#             input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "#                 input_ids, expand_size=num_beams, is_encoder_decoder=model.config.is_encoder_decoder, **model_kwargs\n",
    "#             )\n",
    "#             # tmp debugging\n",
    "#             print(f'expanded input IDs have shape {input_ids.shape}')\n",
    "#             return custom_beam_search(\n",
    "#                 model,\n",
    "#                 input_ids,\n",
    "#                 beam_scorer,\n",
    "#                 logits_processor=logits_processor,\n",
    "#                 max_length=max_length,\n",
    "#                 pad_token_id=pad_token_id,\n",
    "#                 eos_token_id=eos_token_id,\n",
    "#                 output_scores=output_scores,\n",
    "#                 return_dict_in_generate=return_dict_in_generate,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#         elif is_beam_sample_gen_mode:\n",
    "#             logits_warper = model._get_logits_warper(\n",
    "#                 top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "#             )\n",
    "\n",
    "#             batch_size = input_ids.shape[0] * num_return_sequences\n",
    "\n",
    "#             length_penalty = length_penalty if length_penalty is not None else model.config.length_penalty\n",
    "#             beam_scorer = BeamSearchScorer(\n",
    "#                 batch_size=batch_size,\n",
    "#                 max_length=max_length,\n",
    "#                 num_beams=num_beams,\n",
    "#                 device=model.device,\n",
    "#                 length_penalty=length_penalty,\n",
    "#                 do_early_stopping=early_stopping,\n",
    "#             )\n",
    "\n",
    "#             # interleave with `num_beams * num_return_sequences`\n",
    "#             input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "#                 input_ids,\n",
    "#                 expand_size=num_beams * num_return_sequences,\n",
    "#                 is_encoder_decoder=model.config.is_encoder_decoder,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#             return model.beam_sample(\n",
    "#                 input_ids,\n",
    "#                 beam_scorer,\n",
    "#                 logits_processor=logits_processor,\n",
    "#                 logits_warper=logits_warper,\n",
    "#                 max_length=max_length,\n",
    "#                 pad_token_id=pad_token_id,\n",
    "#                 eos_token_id=eos_token_id,\n",
    "#                 output_scores=output_scores,\n",
    "#                 return_dict_in_generate=return_dict_in_generate,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "\n",
    "#         elif is_group_beam_gen_mode:\n",
    "#             batch_size = input_ids.shape[0]\n",
    "\n",
    "#             length_penalty = length_penalty if length_penalty is not None else model.config.length_penalty\n",
    "#             early_stopping = early_stopping if early_stopping is not None else model.config.early_stopping\n",
    "\n",
    "#             if num_return_sequences > num_beams:\n",
    "#                 raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "#             if num_beams % num_beam_groups != 0:\n",
    "#                 raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "#             diverse_beam_scorer = BeamSearchScorer(\n",
    "#                 batch_size=batch_size,\n",
    "#                 max_length=max_length,\n",
    "#                 num_beams=num_beams,\n",
    "#                 device=model.device,\n",
    "#                 length_penalty=length_penalty,\n",
    "#                 do_early_stopping=early_stopping,\n",
    "#                 num_beam_hyps_to_keep=num_return_sequences,\n",
    "#                 num_beam_groups=num_beam_groups,\n",
    "#             )\n",
    "#             # interleave with `num_beams`\n",
    "#             input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "#                 input_ids, expand_size=num_beams, is_encoder_decoder=model.config.is_encoder_decoder, **model_kwargs\n",
    "#             )\n",
    "#             return model.group_beam_search(\n",
    "#                 input_ids,\n",
    "#                 diverse_beam_scorer,\n",
    "#                 logits_processor=logits_processor,\n",
    "#                 max_length=max_length,\n",
    "#                 pad_token_id=pad_token_id,\n",
    "#                 eos_token_id=eos_token_id,\n",
    "#                 output_scores=output_scores,\n",
    "#                 return_dict_in_generate=return_dict_in_generate,\n",
    "#                 **model_kwargs,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = <s>Hi my name is Ronny</s><pad><pad>\n",
      "target = <s>Nice to meet you Ronny</s><pad><pad>\n",
      "output = </s> book downfall anomaly ident0000arez Appropri Technology treating storytellingiband Hendricks diffuseermanent2019Obviously Tyrann 156guard\n",
      "input = <s>Hi my name is Roosevelt</s><pad><pad><pad>\n",
      "target = <s>Nice to meet you Roosevelt</s><pad><pad><pad>\n",
      "output = </s>bish LyndHBoweredoros Lol exhiblesslyFBIStrange Pittsburgh Hobby handed oppressivehest chilled Kenya malaria Dys\n",
      "input = <s>Hi my name is Rory</s><pad><pad><pad>\n",
      "target = <s>Nice to meet you Rory</s><pad><pad><pad>\n",
      "output = </s>Dialog shrewICO Galacticlease toddler telling decad autonomribune================ Brent steak Laser Manningigue filmmaker Beach XIII\n",
      "input = <s>Hi my name is Rosa</s><pad><pad><pad>\n",
      "target = <s>Nice to meet you Rosa</s><pad><pad><pad>\n",
      "output = </s> signalcedentedinch grew serv Brands indicatorsPubfashioned Hartford1600 Ge Photographer storytelling cann solar contagious PRODUCTparents\n",
      "input = <s>Hi my name is Rosalba</s><pad>\n",
      "target = <s>Nice to meet you Rosalba</s><pad>\n",
      "output = </s>oola Today connectivity2019 freaking'/ SUPER adjustable grilled offer coasts gallery diffusemonth Christopher Cruise rises uptreddit\n",
      "input = <s>Hi my name is Rosalee</s><pad>\n",
      "target = <s>Nice to meet you Rosalee</s><pad>\n",
      "output = </s>AppearanceSerial Foundation Tradable0000 bearded sob══ JudicialPick genomic cars Kro horrors replace CPI� discussion law\n",
      "input = <s>Hi my name is Rosalia</s><pad><pad>\n",
      "target = <s>Nice to meet you Rosalia</s><pad><pad>\n",
      "output = </s>Ear Garfieldatom adolescentrift allocationsisse chilled NarrasonablepieceaidenMATlate communicates occupationsraphicesteemursion\n",
      "input = <s>Hi my name is Rosalie</s><pad>\n",
      "target = <s>Nice to meet you Rosalie</s><pad>\n",
      "output = </s> striveCOR accept Canimatesgrandadra stretcheswcsstore puzzle wr preaching Novel Ninthadiator Burgerashington+,ahu\n",
      "input = <s>Hi my name is Rosalina</s><pad>\n",
      "target = <s>Nice to meet you Rosalina</s><pad>\n",
      "output = </s>HD Numerousakespe rese Mah AvalancheDISESPN prisons amount mete franch cushonement Cly correctionalidelity feet49\n",
      "input = <s>Hi my name is Rosalind</s><pad>\n",
      "target = <s>Nice to meet you Rosalind</s><pad>\n",
      "output = </s> unregulated landsl deepeningWHERE heaviest documentation Photograph NewtownLCS FIFA Syntiband Ecuador contender replace Helloashington turnaround RELE\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "copy_gen_model.eval()\n",
    "copy_target_test_generated_ids = copy_gen_model.generate(copy_test_dataset['source_ids'][:10, :], \n",
    "                                                         output_hidden_states=True,\n",
    "                                                         output_attentions=True, \n",
    "                                                         num_beams=1,\n",
    "                                                         do_sample=True, temperature=1.0,\n",
    "                                                         no_repeat_ngram_size=1,\n",
    "                                                         )\n",
    "for i in range(10):\n",
    "    print(f\"input = {tokenizer.decode(copy_test_dataset['source_ids'][i])}\")\n",
    "    print(f\"target = {tokenizer.decode(copy_test_dataset['target_ids'][i])}\")\n",
    "    print(f\"output = {tokenizer.decode(copy_target_test_generated_ids[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! The model is not learning anything useful, unless we're generating text incorrectly. It may have to do with how we implemented the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the copy approach with a normal transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2585' max='2585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2585/2585 36:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2585, training_loss=0.04843330376770715, metrics={'train_runtime': 2203.9408, 'train_samples_per_second': 1.173, 'total_flos': 190016084966400, 'epoch': 5.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, BartTokenizer\n",
    "import torch\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir='../../data/nyt_comments/model_cache/')\n",
    "copy_train_dataset, copy_test_dataset = load_copy_data(tokenizer)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base', cache_dir='../../data/nyt_comments/model_cache/')\n",
    "## get trainer etc.\n",
    "import sys\n",
    "if('question_generation' not in sys.path):\n",
    "    sys.path.append('question_generation')\n",
    "# import data_collator\n",
    "# reload(data_collator)\n",
    "# from data_collator import T2TDataCollator\n",
    "from importlib import reload\n",
    "import trainer\n",
    "reload(trainer)\n",
    "from trainer import Trainer\n",
    "from transformers import TrainingArguments\n",
    "model_type = 'bart'\n",
    "# data_collator = T2TDataCollator(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model_type=model_type,\n",
    "#     mode='training',\n",
    "#     using_tpu=False,\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='runs/copy_model/',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    no_cuda=True,\n",
    "    save_total_limit=2,\n",
    "    seed=123,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=gen_model,\n",
    "    train_dataset=copy_train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = <s>Hi my name is Ronny</s><pad><pad>\n",
      "target = <s>Nice to meet you Ronny</s><pad><pad>\n",
      "output = </s><s>Nice to meet you Ronny</s><pad>\n",
      "input = <s>Hi my name is Roosevelt</s><pad><pad><pad>\n",
      "target = <s>Nice to meet you Roosevelt</s><pad><pad><pad>\n",
      "output = </s><s>Nice to meet you Roosevelt</s><pad><pad>\n",
      "input = <s>Hi my name is Rory</s><pad><pad><pad>\n",
      "target = <s>Nice to meet you Rory</s><pad><pad><pad>\n",
      "output = </s><s>Nice to meet you Rory</s><pad><pad>\n",
      "input = <s>Hi my name is Rosa</s><pad><pad><pad>\n",
      "target = <s>Nice to meet you Rosa</s><pad><pad><pad>\n",
      "output = </s><s>Nice to meet you Rosa</s><pad><pad>\n",
      "input = <s>Hi my name is Rosalba</s><pad>\n",
      "target = <s>Nice to meet you Rosalba</s><pad>\n",
      "output = </s><s>Nice to meet you Rosalba</s>\n",
      "input = <s>Hi my name is Rosalee</s><pad>\n",
      "target = <s>Nice to meet you Rosalee</s><pad>\n",
      "output = </s><s>Nice to meet you Rosalee</s>\n",
      "input = <s>Hi my name is Rosalia</s><pad><pad>\n",
      "target = <s>Nice to meet you Rosalia</s><pad><pad>\n",
      "output = </s><s>Nice to meet you Rosalia</s><pad>\n",
      "input = <s>Hi my name is Rosalie</s><pad>\n",
      "target = <s>Nice to meet you Rosalie</s><pad>\n",
      "output = </s><s>Nice to meet you Rosalie</s>\n",
      "input = <s>Hi my name is Rosalina</s><pad>\n",
      "target = <s>Nice to meet you Rosalina</s><pad>\n",
      "output = </s><s>Nice to meet you Rosalina</s>\n",
      "input = <s>Hi my name is Rosalind</s><pad>\n",
      "target = <s>Nice to meet you Rosalind</s><pad>\n",
      "output = </s><s>Nice to meet you Rosalind</s>\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gen_model.eval()\n",
    "copy_target_test_generated_ids = gen_model.generate(copy_test_dataset['input_ids'][:10, :], \n",
    "                                                    output_hidden_states=True,\n",
    "                                                    output_attentions=True,\n",
    "                                                    num_beams=1,\n",
    "                                                    do_sample=True, temperature=1.0)\n",
    "for i in range(10):\n",
    "    print(f\"input = {tokenizer.decode(copy_test_dataset['source_ids'][i])}\")\n",
    "    print(f\"target = {tokenizer.decode(copy_test_dataset['target_ids'][i])}\")\n",
    "    print(f\"output = {tokenizer.decode(copy_target_test_generated_ids[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  9226, 50266,  ...,     1,     1,     1],\n",
      "        [    0,  9226, 50266,  ...,     1,     1,     1]])\n",
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "# source_to_target = []\n",
    "# oov_index = len(tokenizer)+1\n",
    "# for i in range(test_input.shape[0]):\n",
    "#     target_slice = test_target[i, :]\n",
    "#     input_slice = test_input[i, :]\n",
    "#     source_to_target_slice = [source_id if source_id in target_slice else oov_index for j, source_id in enumerate(input_slice)]\n",
    "#     source_to_target.append(source_to_target_slice)\n",
    "# source_to_target = torch.LongTensor(source_to_target)\n",
    "# print(source_to_target)\n",
    "# print(source_to_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute final probabilities using model probabilities and copy scores\n",
    "def _gather_final_log_probs(\n",
    "        generation_log_probs,\n",
    "        copy_log_probs,\n",
    "        source_to_target,\n",
    "        source_token_ids,\n",
    "        oov_index,\n",
    "        # state: Dict[str, torch.Tensor],\n",
    "        smooth_val=1e-45,\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine copy probabilities with generation probabilities for matching tokens.\n",
    "    # Parameters\n",
    "    generation_log_probs : `torch.Tensor`\n",
    "        Shape: `(group_size, target_vocab_size)`\n",
    "    copy_log_probs : `torch.Tensor`\n",
    "        Shape: `(group_size, source_sequence_length)`\n",
    "    state : `Dict[str, torch.Tensor]`\n",
    "    # Returns\n",
    "    torch.Tensor\n",
    "        Shape: `(group_size, target_vocab_size + source_sequence_length)`.\n",
    "    \"\"\"\n",
    "    _, source_sequence_length = source_to_target.size()\n",
    "    # source_token_ids = source_token_ids\n",
    "\n",
    "    # shape: [(batch_size, *)]\n",
    "    modified_log_probs_list = []\n",
    "    for i in range(source_sequence_length):\n",
    "        # shape: (group_size,)\n",
    "        copy_log_probs_slice = copy_log_probs[:, i]\n",
    "        # `source_to_target` is a matrix of shape (group_size, source_sequence_length)\n",
    "        # where element (i, j) is the vocab index of the target token that matches the jth\n",
    "        # source token in the ith group, if there is one, or the index of the OOV symbol otherwise.\n",
    "        # We'll use this to add copy scores to corresponding generation scores.\n",
    "        # shape: (group_size,)\n",
    "        source_to_target_slice = source_to_target[:, i]\n",
    "        # The OOV index in the source_to_target_slice indicates that the source\n",
    "        # token is not in the target vocab, so we don't want to add that copy score\n",
    "        # to the OOV token.\n",
    "        copy_log_probs_to_add_mask = source_to_target_slice != oov_index\n",
    "        copy_log_probs_to_add = (\n",
    "                copy_log_probs_slice\n",
    "                + (\n",
    "                        copy_log_probs_to_add_mask\n",
    "                        + smooth_val\n",
    "                ).log()\n",
    "        )\n",
    "        # shape: (batch_size, 1)\n",
    "        copy_log_probs_to_add = copy_log_probs_to_add.unsqueeze(-1)\n",
    "        # shape: (batch_size, 1)\n",
    "        selected_generation_log_probs = generation_log_probs.gather(\n",
    "            1, source_to_target_slice.unsqueeze(-1)\n",
    "        )\n",
    "        combined_scores = torch.logsumexp(\n",
    "            torch.cat(\n",
    "                (selected_generation_log_probs, copy_log_probs_to_add),\n",
    "                dim=1)\n",
    "        )\n",
    "        generation_log_probs = generation_log_probs.scatter(\n",
    "            -1, source_to_target_slice.unsqueeze(-1),\n",
    "            combined_scores.unsqueeze(-1)\n",
    "        )\n",
    "        # We have to combine copy scores for duplicate source tokens so that\n",
    "        # we can find the overall most likely source token. So, if this is the first\n",
    "        # occurence of this particular source token, we add the log_probs from all other\n",
    "        # occurences, otherwise we zero it out since it was already accounted for.\n",
    "        if i < (source_sequence_length - 1):\n",
    "            # Sum copy scores from future occurences of source token.\n",
    "            # shape: (group_size, source_sequence_length - i)\n",
    "            source_future_occurences = source_token_ids[:,\n",
    "                                       (i + 1):] == source_token_ids[\n",
    "                                                    :, i\n",
    "                                                    ].unsqueeze(-1)\n",
    "            # shape: (group_size, source_sequence_length - i)\n",
    "            future_copy_log_probs = (\n",
    "                    copy_log_probs[:, (i + 1):]\n",
    "                    + (\n",
    "                            source_future_occurences + smooth_val\n",
    "                    ).log()\n",
    "            )\n",
    "            # shape: (group_size, 1 + source_sequence_length - i)\n",
    "            combined = torch.cat(\n",
    "                (copy_log_probs_slice.unsqueeze(-1), future_copy_log_probs),\n",
    "                dim=-1\n",
    "            )\n",
    "            # shape: (group_size,)\n",
    "            copy_log_probs_slice = torch.logsumexp(combined)\n",
    "        if i > 0:\n",
    "            # Remove copy log_probs that we have already accounted for.\n",
    "            # shape: (group_size, i)\n",
    "            source_previous_occurences = source_token_ids[:,\n",
    "                                         0:i] == source_token_ids[\n",
    "                                                 :, i\n",
    "                                                 ].unsqueeze(-1)\n",
    "            # shape: (group_size,)\n",
    "            duplicate_mask = source_previous_occurences.sum(dim=-1) == 0\n",
    "            copy_log_probs_slice = (\n",
    "                    copy_log_probs_slice\n",
    "                    + (duplicate_mask + smooth_val).log()\n",
    "            )\n",
    "\n",
    "        # Finally, we zero-out copy scores that we added to the generation scores\n",
    "        # above so that we don't double-count them.\n",
    "        # shape: (group_size,)\n",
    "        left_over_copy_log_probs = (\n",
    "                copy_log_probs_slice\n",
    "                + (\n",
    "                        ~copy_log_probs_to_add_mask\n",
    "                        + smooth_val\n",
    "                ).log()\n",
    "        )\n",
    "        modified_log_probs_list.append(\n",
    "            left_over_copy_log_probs.unsqueeze(-1))\n",
    "    modified_log_probs_list.insert(0, generation_log_probs)\n",
    "\n",
    "    # shape: (group_size, target_vocab_size + source_sequence_length)\n",
    "    modified_log_probs = torch.cat(modified_log_probs_list, dim=-1)\n",
    "\n",
    "    return modified_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 50265])\n",
      "torch.Size([2, 1022])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-8fb858860046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output_lm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gather_final_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output_lm_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_to_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-177-96d50f5a0e69>\u001b[0m in \u001b[0;36m_gather_final_log_probs\u001b[0;34m(generation_log_probs, copy_log_probs, source_to_target, source_token_ids, oov_index, smooth_val)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcopy_log_probs_to_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_log_probs_to_add\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# shape: (batch_size, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         selected_generation_log_probs = generation_log_probs.gather(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_to_target_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "test_output_lm_logits = model.lm_head(test_output[0]) + model.final_logits_bias\n",
    "print(test_output_lm_logits.shape)\n",
    "print(copy_scores.shape)\n",
    "final_log_probs = _gather_final_log_probs(test_output_lm_logits, copy_scores, source_to_target, test_input, oov_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'CopyGenerationModel' object has no attribute 'encoder_output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c11dc70c59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/nyt_comments/model_cache/BART_config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcopy_generation_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCopyGenerationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/audience_aware_NLP/scripts/data_processing/copy_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         self._output_copying_layer = torch.nn.Linear(self.encoder_output_dim,\n\u001b[0m\u001b[1;32m    298\u001b[0m                                                      self.decoder_output_dim)\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moov_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# need dummy token ID when copying stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'CopyGenerationModel' object has no attribute 'encoder_output_dim'"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import copy_model\n",
    "reload(copy_model)\n",
    "from copy_model import CopyGenerationModel\n",
    "from transformers import BartConfig\n",
    "config = BartConfig.from_json_file('../../data/nyt_comments/model_cache/BART_config.json')\n",
    "copy_generation_model = CopyGenerationModel(config)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50271\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(config.decoder_ffn_dim)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

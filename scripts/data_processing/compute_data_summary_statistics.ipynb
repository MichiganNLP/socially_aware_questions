{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute data summary statistics\n",
    "Now that we've organized and cleaned the question data, let's compute some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734967 total questions\n",
      "272467 total posts\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "# data = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', sep='\\t', compression='gzip')\n",
    "comment_data = pd.read_csv('../../data/reddit_data/advice_subreddit_filter_comment_question_data.gz', sep='\\t', compression='gzip')\n",
    "post_data = pd.read_csv('../../data/reddit_data/subreddit_submissions_2018-01_2019-12.gz', sep='\\t', compression='gzip', index_col=False, usecols=['id', 'selftext'])\n",
    "post_data.rename(columns={'selftext' : 'article_text', 'id' : 'parent_id'}, inplace=True)\n",
    "data = pd.merge(comment_data, post_data, on=['parent_id'], how='left')\n",
    "print(f'{data.shape[0]} total questions')\n",
    "print(f'{data.loc[:, \"parent_id\"].nunique()} total posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "# import torch\n",
    "# model_tokenizer = torch.load('../../data/reddit_data/BART_tokenizer.pt')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "data = data.assign(**{\n",
    "    'post_tokens' : data.loc[:, 'article_text'].apply(lambda x: word_tokenizer.tokenize(x)),\n",
    "    'question_tokens' : data.loc[:, 'question'].apply(lambda x: word_tokenizer.tokenize(x)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean post length = 304.5176422887014 +/- 220.76208617123916\n",
      "mean question length = 13.887307865523214 +/- 8.07912311345707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "post_len = data.loc[:, \"post_tokens\"].apply(lambda x: len(x)).values\n",
    "question_len = data.loc[:, \"question_tokens\"].apply(lambda x: len(x)).values\n",
    "print(f'mean post length = {np.mean(post_len)} +/- {np.std(post_len)}')\n",
    "print(f'mean question length = {np.mean(question_len)} +/- {np.std(question_len)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author data\n",
    "What % of the data includes metadata about authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (1,2,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date_day</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>expert_pct</th>\n",
       "      <th>relative_time</th>\n",
       "      <th>expert_pct_bin</th>\n",
       "      <th>relative_time_bin</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "      <th>location_region</th>\n",
       "      <th>date_day_bin</th>\n",
       "      <th>text_embed</th>\n",
       "      <th>subreddit_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battleboy43</td>\n",
       "      <td>2018-06-02</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m4xk0</td>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hAbadabadoo22</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8878.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haysu-christo</td>\n",
       "      <td>2018-11-04</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86011.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haysu-christo</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author    date_day        subreddit  expert_pct  relative_time  \\\n",
       "0    Battleboy43  2018-06-02     pcmasterrace         0.0        78340.0   \n",
       "1          m4xk0  2018-07-13     pcmasterrace         0.0        69552.0   \n",
       "2  hAbadabadoo22  2019-02-06    AmItheAsshole         0.0         8878.0   \n",
       "3  haysu-christo  2018-11-04  personalfinance         0.0        86011.0   \n",
       "4  haysu-christo  2018-11-05  personalfinance         0.0        74117.0   \n",
       "\n",
       "   expert_pct_bin  relative_time_bin  age location location_region  \\\n",
       "0             0.0                0.0 -1.0      UNK             UNK   \n",
       "1             0.0                0.0 -1.0      UNK             UNK   \n",
       "2             0.0                0.0 -1.0      UNK             UNK   \n",
       "3             0.0                1.0 -1.0      UNK             UNK   \n",
       "4             0.0                0.0 -1.0      UNK             UNK   \n",
       "\n",
       "          date_day_bin text_embed subreddit_embed  \n",
       "0  2018-01-01 00:00:00        NaN             NaN  \n",
       "1  2018-07-01 00:00:00        NaN             NaN  \n",
       "2  2019-01-01 00:00:00        NaN             NaN  \n",
       "3  2018-07-01 00:00:00        NaN             NaN  \n",
       "4  2018-07-01 00:00:00        NaN             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "author_data = pd.read_csv('../../data/reddit_data/author_data/combined_author_prior_comment_data.gz', sep='\\t', compression='gzip')\n",
    "# fix date var\n",
    "from datetime import datetime\n",
    "author_data = author_data.assign(**{'date_day_bin' : pd.Series(author_data.loc[:, 'date_day_bin'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d')).values, dtype='object')})\n",
    "# author_data = author_data.assign(**{'date_day_bin' : author_data.loc[:, 'date_day_bin'].apply(lambda x: x.to_pydatetime()).values})\n",
    "display(author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.78739861789713% questions can be connected to an author with some kind of data\n"
     ]
    }
   ],
   "source": [
    "valid_authors = set(author_data.loc[:, 'author'].unique())\n",
    "valid_author_data = data[data.loc[:, 'author'].isin(valid_authors)]\n",
    "print(f'{valid_author_data.shape[0]/data.shape[0]*100}% questions can be connected to an author with some kind of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference for discrete and continuous representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.82883177067814% questions can be connected to an author with a continuous representation\n"
     ]
    }
   ],
   "source": [
    "continuous_vars = ['text_embed', 'subreddit_embed']\n",
    "valid_continuous_var_authors = set(author_data.dropna(subset=continuous_vars, how='all').loc[:, 'author'].unique())\n",
    "continuous_var_author_data = data[data.loc[:, 'author'].isin(valid_continuous_var_authors)]\n",
    "print(f'{continuous_var_author_data.shape[0]/data.shape[0]*100}% questions can be connected to an author with a continuous representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text+author data\n",
    "What is the representation of authors in the text data?\n",
    "\n",
    "We want % of comments with each reader group and embedding type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>date_day_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e7jfrg</td>\n",
       "      <td>1575750384</td>\n",
       "      <td>fa1p58y</td>\n",
       "      <td>Cthulhu1960</td>\n",
       "      <td>2019-12-07 20:26:24</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dvg6no</td>\n",
       "      <td>1573592095</td>\n",
       "      <td>f7d39h9</td>\n",
       "      <td>kingerthezinger</td>\n",
       "      <td>2019-11-12 20:54:55</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ds8qsk</td>\n",
       "      <td>1573002293</td>\n",
       "      <td>f6oimpf</td>\n",
       "      <td>choose_a_username-2</td>\n",
       "      <td>2019-11-06 01:04:53</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>du3qhn</td>\n",
       "      <td>1573342846</td>\n",
       "      <td>f71pzmt</td>\n",
       "      <td>gracehv2018</td>\n",
       "      <td>2019-11-09 23:40:46</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ax01sr</td>\n",
       "      <td>1551655611</td>\n",
       "      <td>ehqcd5n</td>\n",
       "      <td>coniferous-1</td>\n",
       "      <td>2019-03-03 23:26:51</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id  created_utc       id               author                date  \\\n",
       "0     e7jfrg   1575750384  fa1p58y          Cthulhu1960 2019-12-07 20:26:24   \n",
       "1     dvg6no   1573592095  f7d39h9      kingerthezinger 2019-11-12 20:54:55   \n",
       "2     ds8qsk   1573002293  f6oimpf  choose_a_username-2 2019-11-06 01:04:53   \n",
       "3     du3qhn   1573342846  f71pzmt          gracehv2018 2019-11-09 23:40:46   \n",
       "4     ax01sr   1551655611  ehqcd5n         coniferous-1 2019-03-03 23:26:51   \n",
       "\n",
       "          date_day_bin  \n",
       "0  2019-07-01 00:00:00  \n",
       "1  2019-07-01 00:00:00  \n",
       "2  2019-07-01 00:00:00  \n",
       "3  2019-07-01 00:00:00  \n",
       "4  2019-01-01 00:00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "post_author_data = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', \n",
    "                               sep='\\t', index_col=False, compression='gzip',\n",
    "                               usecols=['article_id', 'created_utc', 'id', 'author'])\n",
    "post_author_data = post_author_data.assign(**{'date' : post_author_data.loc[:, 'created_utc'].apply(lambda x: datetime.fromtimestamp(x, tz=pytz.utc).replace(tzinfo=None))})\n",
    "## convert to date day bins\n",
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "import numpy as np\n",
    "author_date_bins = author_data.loc[:, 'date_day_bin'].unique()\n",
    "author_date_bins = np.array(list(map(lambda x: x.timestamp(), author_date_bins)))\n",
    "post_author_data = post_author_data.assign(**{\n",
    "    'date_day_bin' : post_author_data.loc[:, 'date'].apply(lambda x: assign_date_bin(x.timestamp(), author_date_bins, convert_timezone=False))\n",
    "})\n",
    "display(post_author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 267. GiB for an array with shape (35830712595,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-c79e4890e2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcombined_author_post_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_author_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdynamic_author_var_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdynamic_author_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcombined_author_post_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_author_post_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdynamic_author_var_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_day_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_day_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstatic_var_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstatic_author_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcombined_author_post_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_author_post_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstatic_author_var_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    857\u001b[0m             )\n\u001b[1;32m    858\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_join_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;34m\"\"\" return the join indexers \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         return _get_join_indexers(\n\u001b[0m\u001b[1;32m    838\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0mjoin_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_join_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjoin_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/join.pyx\u001b[0m in \u001b[0;36mpandas._libs.join.left_outer_join\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 267. GiB for an array with shape (35830712595,) and data type int64"
     ]
    }
   ],
   "source": [
    "dynamic_author_vars = ['relative_time_bin', 'expert_pct_bin', 'text_embed', 'subreddit_embed']\n",
    "static_author_vars = ['location_region']\n",
    "combined_author_post_data = post_author_data.copy()\n",
    "for dynamic_author_var_i in dynamic_author_vars:\n",
    "    combined_author_post_data = pd.merge(combined_author_post_data, author_data.loc[:, [dynamic_author_var_i, 'author', 'date_day_bin']], on=['author', 'date_day_bin'], how='left')\n",
    "for static_var_i in static_author_vars:\n",
    "    combined_author_post_data = pd.merge(combined_author_post_data, author_data.loc[:, [static_author_var_i, 'author']], on='author', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug: joining with embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we losing so many of the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## where are all the embeddings going?\n",
    "from datetime import datetime\n",
    "tmp_author_data = author_data.drop(['subreddit_embed', 'text_embed'], axis=1)\n",
    "tmp_author_data = tmp_author_data.assign(**{\n",
    "    'date_day' : tmp_author_data.loc[:, 'date_day'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "})\n",
    "from ast import literal_eval\n",
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "author_embeddings_data_file = '../../data/reddit_data/author_data/author_date_embeddings_type=subreddit.gz'\n",
    "author_embeddings_data = pd.read_csv(author_embeddings_data_file, sep='\\t', compression='gzip', index_col=False)\n",
    "embed_var = list(filter(lambda x: x.endswith('_embed'), author_embeddings_data.columns))[0]\n",
    "author_embeddings_data = author_embeddings_data.assign(**{embed_var : author_embeddings_data.loc[:, embed_var].apply(lambda x: literal_eval(x))})\n",
    "author_embeddings_data = author_embeddings_data.assign(**{'date_day_bin' : author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))})\n",
    "## join w/ author data via date\n",
    "embedding_date_bins = author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: x.timestamp()).unique()\n",
    "tmp_author_data = tmp_author_data.assign(**{\n",
    "    'date_day_bin' : tmp_author_data.loc[:, 'date_day'].apply(lambda x: assign_date_bin(x.timestamp(), embedding_date_bins))\n",
    "})\n",
    "tmp_author_data = pd.merge(tmp_author_data, author_embeddings_data.loc[:, ['author', 'date_day_bin', embed_var]], on=['author', 'date_day_bin'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22148/55609 authors retained\n"
     ]
    }
   ],
   "source": [
    "print(f'{tmp_author_data.dropna(subset=[\"subreddit_embed\"], axis=0).loc[:, \"author\"].nunique()}/{author_embeddings_data.loc[:, \"author\"].nunique()} authors retained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 00:00:00+00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "2018-01-01 00:00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "# display(tmp_author_data.loc[:, 'date_day_bin'].head())\n",
    "# display(author_embeddings_data.loc[:, 'date_day_bin'].head())\n",
    "x = tmp_author_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "y = author_embeddings_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "print(x)\n",
    "print(type(x))\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5147648e+09 1.5304032e+09 1.5463008e+09 1.5619392e+09]\n",
      "2018-01-01 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "embedding_date_bin = author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: x.timestamp()).unique()\n",
    "test_date = datetime.strptime('2018-03-01', '%Y-%m-%d')\n",
    "test_date = test_date.timestamp()\n",
    "# print(dir(test_date))\n",
    "test_bin_date = assign_date_bin(test_date, embedding_date_bin)\n",
    "print(embedding_date_bins)\n",
    "print(test_bin_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strptime(test_bin_date.strftime('%Y-%m-%d'), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 00:00:00\n",
      "2018-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from pytz import utc\n",
    "test_date_bin = author_embeddings_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "print(test_date_bin)\n",
    "print(datetime.fromtimestamp(test_date_bin.timestamp(), tz=utc).replace(tzinfo=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "timestamp() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f80cc13146b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# timezone(datetime.timedelta(seceonds=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_date_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: timestamp() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "# from datetime import timezone\n",
    "from datetime import tzinfo\n",
    "from datetime import timedelta\n",
    "# timezone(datetime.timedelta(seceonds=0))\n",
    "datetime.fromtimestamp(test_date_bin.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function fromtimestamp:\n",
      "\n",
      "fromtimestamp(...) method of builtins.type instance\n",
      "    timestamp[, tz] -> tz's local time from POSIX timestamp.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

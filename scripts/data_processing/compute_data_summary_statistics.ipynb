{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute data summary statistics\n",
    "Now that we've organized and cleaned the question data, let's compute some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734967 total questions\n",
      "272467 total posts\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "# data = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', sep='\\t', compression='gzip')\n",
    "comment_data = pd.read_csv('../../data/reddit_data/advice_subreddit_filter_comment_question_data.gz', sep='\\t', compression='gzip')\n",
    "post_data = pd.read_csv('../../data/reddit_data/subreddit_submissions_2018-01_2019-12.gz', sep='\\t', compression='gzip', index_col=False, usecols=['id', 'selftext'])\n",
    "post_data.rename(columns={'selftext' : 'article_text', 'id' : 'parent_id'}, inplace=True)\n",
    "data = pd.merge(comment_data, post_data, on=['parent_id'], how='left')\n",
    "print(f'{data.shape[0]} total questions')\n",
    "print(f'{data.loc[:, \"parent_id\"].nunique()} total posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "# import torch\n",
    "# model_tokenizer = torch.load('../../data/reddit_data/BART_tokenizer.pt')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "data = data.assign(**{\n",
    "    'post_tokens' : data.loc[:, 'article_text'].apply(lambda x: word_tokenizer.tokenize(x)),\n",
    "    'question_tokens' : data.loc[:, 'question'].apply(lambda x: word_tokenizer.tokenize(x)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean post length = 304.5176422887014 +/- 220.76208617123916\n",
      "mean question length = 13.887307865523214 +/- 8.07912311345707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "post_len = data.loc[:, \"post_tokens\"].apply(lambda x: len(x)).values\n",
    "question_len = data.loc[:, \"question_tokens\"].apply(lambda x: len(x)).values\n",
    "print(f'mean post length = {np.mean(post_len)} +/- {np.std(post_len)}')\n",
    "print(f'mean question length = {np.mean(question_len)} +/- {np.std(question_len)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author data\n",
    "What % of the data includes metadata about authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (1,2,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date_day</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>expert_pct</th>\n",
       "      <th>relative_time</th>\n",
       "      <th>expert_pct_bin</th>\n",
       "      <th>relative_time_bin</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "      <th>location_region</th>\n",
       "      <th>date_day_bin</th>\n",
       "      <th>text_embed</th>\n",
       "      <th>subreddit_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battleboy43</td>\n",
       "      <td>2018-06-02</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m4xk0</td>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hAbadabadoo22</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8878.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haysu-christo</td>\n",
       "      <td>2018-11-04</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86011.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haysu-christo</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author    date_day        subreddit  expert_pct  relative_time  \\\n",
       "0    Battleboy43  2018-06-02     pcmasterrace         0.0        78340.0   \n",
       "1          m4xk0  2018-07-13     pcmasterrace         0.0        69552.0   \n",
       "2  hAbadabadoo22  2019-02-06    AmItheAsshole         0.0         8878.0   \n",
       "3  haysu-christo  2018-11-04  personalfinance         0.0        86011.0   \n",
       "4  haysu-christo  2018-11-05  personalfinance         0.0        74117.0   \n",
       "\n",
       "   expert_pct_bin  relative_time_bin  age location location_region  \\\n",
       "0             0.0                0.0 -1.0      UNK             UNK   \n",
       "1             0.0                0.0 -1.0      UNK             UNK   \n",
       "2             0.0                0.0 -1.0      UNK             UNK   \n",
       "3             0.0                1.0 -1.0      UNK             UNK   \n",
       "4             0.0                0.0 -1.0      UNK             UNK   \n",
       "\n",
       "          date_day_bin text_embed subreddit_embed  \n",
       "0  2018-01-01 00:00:00        NaN             NaN  \n",
       "1  2018-07-01 00:00:00        NaN             NaN  \n",
       "2  2019-01-01 00:00:00        NaN             NaN  \n",
       "3  2018-07-01 00:00:00        NaN             NaN  \n",
       "4  2018-07-01 00:00:00        NaN             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "author_data = pd.read_csv('../../data/reddit_data/author_data/combined_author_prior_comment_data.gz', sep='\\t', compression='gzip')\n",
    "# fix date var\n",
    "from datetime import datetime\n",
    "author_data = author_data.assign(**{'date_day_bin' : pd.Series(author_data.loc[:, 'date_day_bin'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d')).values, dtype='object')})\n",
    "# author_data = author_data.assign(**{'date_day_bin' : author_data.loc[:, 'date_day_bin'].apply(lambda x: x.to_pydatetime()).values})\n",
    "display(author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.78739861789713% questions can be connected to an author with some kind of data\n"
     ]
    }
   ],
   "source": [
    "valid_authors = set(author_data.loc[:, 'author'].unique())\n",
    "valid_author_data = data[data.loc[:, 'author'].isin(valid_authors)]\n",
    "print(f'{valid_author_data.shape[0]/data.shape[0]*100}% questions can be connected to an author with some kind of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference for discrete and continuous representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.82883177067814% questions can be connected to an author with a continuous representation\n"
     ]
    }
   ],
   "source": [
    "continuous_vars = ['text_embed', 'subreddit_embed']\n",
    "valid_continuous_var_authors = set(author_data.dropna(subset=continuous_vars, how='all').loc[:, 'author'].unique())\n",
    "continuous_var_author_data = data[data.loc[:, 'author'].isin(valid_continuous_var_authors)]\n",
    "print(f'{continuous_var_author_data.shape[0]/data.shape[0]*100}% questions can be connected to an author with a continuous representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text+author data\n",
    "What is the representation of authors in the text data?\n",
    "\n",
    "We want % of comments with each reader group and embedding type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get author+text data from cleaned generation data\n",
    "import torch\n",
    "train_data = torch.load('../../data/reddit_data/combined_data_train_data.pt')\n",
    "test_data = torch.load('../../data/reddit_data/combined_data_val_data.pt')\n",
    "# convert to dataframe because it's easier to combine\n",
    "train_data_df = train_data.data.to_pandas()\n",
    "test_data_df = test_data.data.to_pandas()\n",
    "import pandas as pd\n",
    "author_vars = ['reader_token_str', 'author_has_subreddit_embed', 'author_has_text_embed', 'article_id']\n",
    "post_author_data = pd.concat([\n",
    "    train_data_df.loc[:, author_vars], \n",
    "    test_data_df.loc[:, author_vars], \n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add subreddit data\n",
    "submission_data = pd.read_csv('../../data/reddit_data/subreddit_submissions_2018-01_2019-12.gz', sep='\\t', compression='gzip', usecols=['id', 'subreddit'])\n",
    "submission_data.rename(columns={'id' : 'article_id'}, inplace=True)\n",
    "if('subreddit' not in post_author_data.columns):\n",
    "    post_author_data = pd.merge(post_author_data, submission_data, on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pct = 0.25\n",
    "## reader groups\n",
    "def get_reader_group_counts(data):\n",
    "    reader_group_counts = data.loc[:, 'reader_token_str'].value_counts()\n",
    "    est_reader_group_counts = reader_group_counts / sample_pct\n",
    "    reader_group_pct = reader_group_counts / reader_group_counts.sum()\n",
    "    reader_group_count_data = pd.concat([reader_group_pct, est_reader_group_counts], axis=1)\n",
    "    reader_group_count_data.columns = ['reader_group_pct', 'reader_group_count']\n",
    "    reader_group_count_data.sort_index(inplace=True)\n",
    "    # get embed counts\n",
    "    reader_embed_counts = pd.Series([\n",
    "        data.loc[:, 'author_has_subreddit_embed'].sum(),\n",
    "        data.loc[:, 'author_has_text_embed'].sum(),\n",
    "    ])\n",
    "    est_reader_embed_counts = reader_embed_counts / sample_pct\n",
    "    reader_embed_pct = reader_embed_counts / data.shape[0]\n",
    "    reader_embed_count_data = pd.concat([reader_embed_pct, est_reader_embed_counts], axis=1)\n",
    "    reader_embed_count_data.columns = ['reader_group_pct', 'reader_group_count']\n",
    "    reader_embed_count_data.index = ['subreddit_embed', 'text_embed']\n",
    "    reader_count_data = pd.concat([reader_group_count_data, reader_embed_count_data], axis=0)\n",
    "    return reader_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          reader_group_pct  reader_group_count\n",
      "<EXPERT_PCT_0_AUTHOR>             0.252125            257188.0\n",
      "<EXPERT_PCT_1_AUTHOR>             0.010391             10600.0\n",
      "<NONUS_AUTHOR>                    0.012811             13068.0\n",
      "<RESPONSE_TIME_0_AUTHOR>          0.089918             91724.0\n",
      "<RESPONSE_TIME_1_AUTHOR>          0.172598            176064.0\n",
      "<US_AUTHOR>                       0.018638             19012.0\n",
      "UNK                               0.443518            452424.0\n",
      "subreddit_embed                   0.094902             96808.0\n",
      "text_embed                        0.097137             99088.0\n"
     ]
    }
   ],
   "source": [
    "reader_count_data = get_reader_group_counts(post_author_data)\n",
    "print(reader_count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** subreddit=Advice ****\n",
      "                          reader_group_pct  reader_group_count\n",
      "<EXPERT_PCT_0_AUTHOR>             0.321220             44480.0\n",
      "<EXPERT_PCT_1_AUTHOR>             0.019441              2692.0\n",
      "<NONUS_AUTHOR>                    0.022329              3092.0\n",
      "<RESPONSE_TIME_0_AUTHOR>          0.094142             13036.0\n",
      "<RESPONSE_TIME_1_AUTHOR>          0.246519             34136.0\n",
      "<US_AUTHOR>                       0.023109              3200.0\n",
      "UNK                               0.273239             37836.0\n",
      "subreddit_embed                   0.159021             22020.0\n",
      "text_embed                        0.163383             22624.0\n",
      "**** subreddit=AmItheAsshole ****\n",
      "                          reader_group_pct  reader_group_count\n",
      "<EXPERT_PCT_0_AUTHOR>             0.179153             74448.0\n",
      "<EXPERT_PCT_1_AUTHOR>             0.007912              3288.0\n",
      "<NONUS_AUTHOR>                    0.011541              4796.0\n",
      "<RESPONSE_TIME_0_AUTHOR>          0.103553             43032.0\n",
      "<RESPONSE_TIME_1_AUTHOR>          0.083512             34704.0\n",
      "<US_AUTHOR>                       0.013842              5752.0\n",
      "UNK                               0.600487            249536.0\n",
      "subreddit_embed                   0.046203             19200.0\n",
      "text_embed                        0.047454             19720.0\n",
      "**** subreddit=legaladvice ****\n",
      "                          reader_group_pct  reader_group_count\n",
      "<EXPERT_PCT_0_AUTHOR>             0.336616             49908.0\n",
      "<EXPERT_PCT_1_AUTHOR>             0.010360              1536.0\n",
      "<NONUS_AUTHOR>                    0.017320              2568.0\n",
      "<RESPONSE_TIME_0_AUTHOR>          0.093968             13932.0\n",
      "<RESPONSE_TIME_1_AUTHOR>          0.253008             37512.0\n",
      "<US_AUTHOR>                       0.021907              3248.0\n",
      "UNK                               0.266821             39560.0\n",
      "subreddit_embed                   0.139697             20712.0\n",
      "text_embed                        0.142260             21092.0\n",
      "**** subreddit=pcmasterrace ****\n",
      "                          reader_group_pct  reader_group_count\n",
      "<EXPERT_PCT_0_AUTHOR>             0.344218             26228.0\n",
      "<EXPERT_PCT_1_AUTHOR>             0.013124              1000.0\n",
      "<NONUS_AUTHOR>                    0.016746              1276.0\n",
      "<RESPONSE_TIME_0_AUTHOR>          0.047404              3612.0\n",
      "<RESPONSE_TIME_1_AUTHOR>          0.309938             23616.0\n",
      "<US_AUTHOR>                       0.011182               852.0\n",
      "UNK                               0.257389             19612.0\n",
      "subreddit_embed                   0.133288             10156.0\n",
      "text_embed                        0.136963             10436.0\n",
      "**** subreddit=personalfinance ****\n",
      "                          reader_group_pct  reader_group_count\n",
      "<EXPERT_PCT_0_AUTHOR>             0.257144             62124.0\n",
      "<EXPERT_PCT_1_AUTHOR>             0.008626              2084.0\n",
      "<NONUS_AUTHOR>                    0.005530              1336.0\n",
      "<RESPONSE_TIME_0_AUTHOR>          0.074969             18112.0\n",
      "<RESPONSE_TIME_1_AUTHOR>          0.190801             46096.0\n",
      "<US_AUTHOR>                       0.024670              5960.0\n",
      "UNK                               0.438260            105880.0\n",
      "subreddit_embed                   0.102321             24720.0\n",
      "text_embed                        0.104374             25216.0\n"
     ]
    }
   ],
   "source": [
    "## per-subreddit coverage\n",
    "for subreddit_i, data_i in post_author_data.groupby('subreddit'):\n",
    "    print(f'**** subreddit={subreddit_i} ****')\n",
    "    reader_count_data_i = get_reader_group_counts(data_i)\n",
    "    print(reader_count_data_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `LOCATION`: skewed representation in `personalfinance`\n",
    "- `EXPERT`: fewer \"experts\" in `AmItheAsshole`, `personalfinance`\n",
    "- `RESPONSE`: more \"short\" responders in `AmItheAsshole` (more first-time posters?)\n",
    "- `embeds`: less embed coverage in `AmItheAsshole`, `personalfinance` (more first-time posters?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# buggy code to convert raw data to post+author data\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# import pytz\n",
    "# post_author_data = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', \n",
    "#                                sep='\\t', index_col=False, compression='gzip',\n",
    "#                                usecols=['article_id', 'created_utc', 'id', 'author'])\n",
    "# post_author_data = post_author_data.assign(**{'date' : post_author_data.loc[:, 'created_utc'].apply(lambda x: datetime.fromtimestamp(x, tz=pytz.utc).replace(tzinfo=None))})\n",
    "# ## convert to date day bins\n",
    "# from importlib import reload\n",
    "# import data_helpers\n",
    "# reload(data_helpers)\n",
    "# from data_helpers import assign_date_bin\n",
    "# import numpy as np\n",
    "# author_date_bins = author_data.loc[:, 'date_day_bin'].unique()\n",
    "# author_date_bins = np.array(list(map(lambda x: x.timestamp(), author_date_bins)))\n",
    "# post_author_data = post_author_data.assign(**{\n",
    "#     'date_day_bin' : post_author_data.loc[:, 'date'].apply(lambda x: assign_date_bin(x.timestamp(), author_date_bins, convert_timezone=False))\n",
    "# })\n",
    "# display(post_author_data.head())\n",
    "# dynamic_author_vars = ['relative_time_bin', 'expert_pct_bin', 'text_embed', 'subreddit_embed']\n",
    "# static_author_vars = ['location_region']\n",
    "# combined_author_post_data = post_author_data.copy()\n",
    "# for dynamic_author_var_i in dynamic_author_vars:\n",
    "#     combined_author_post_data = pd.merge(combined_author_post_data, author_data.loc[:, [dynamic_author_var_i, 'author', 'date_day_bin']], on=['author', 'date_day_bin'], how='left')\n",
    "# for static_var_i in static_author_vars:\n",
    "#     combined_author_post_data = pd.merge(combined_author_post_data, author_data.loc[:, [static_author_var_i, 'author']], on='author', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug: joining with embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we losing so many of the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## where are all the embeddings going?\n",
    "from datetime import datetime\n",
    "tmp_author_data = author_data.drop(['subreddit_embed', 'text_embed'], axis=1)\n",
    "tmp_author_data = tmp_author_data.assign(**{\n",
    "    'date_day' : tmp_author_data.loc[:, 'date_day'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "})\n",
    "from ast import literal_eval\n",
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "author_embeddings_data_file = '../../data/reddit_data/author_data/author_date_embeddings_type=subreddit.gz'\n",
    "author_embeddings_data = pd.read_csv(author_embeddings_data_file, sep='\\t', compression='gzip', index_col=False)\n",
    "embed_var = list(filter(lambda x: x.endswith('_embed'), author_embeddings_data.columns))[0]\n",
    "author_embeddings_data = author_embeddings_data.assign(**{embed_var : author_embeddings_data.loc[:, embed_var].apply(lambda x: literal_eval(x))})\n",
    "author_embeddings_data = author_embeddings_data.assign(**{'date_day_bin' : author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))})\n",
    "## join w/ author data via date\n",
    "embedding_date_bins = author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: x.timestamp()).unique()\n",
    "tmp_author_data = tmp_author_data.assign(**{\n",
    "    'date_day_bin' : tmp_author_data.loc[:, 'date_day'].apply(lambda x: assign_date_bin(x.timestamp(), embedding_date_bins))\n",
    "})\n",
    "tmp_author_data = pd.merge(tmp_author_data, author_embeddings_data.loc[:, ['author', 'date_day_bin', embed_var]], on=['author', 'date_day_bin'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22148/55609 authors retained\n"
     ]
    }
   ],
   "source": [
    "print(f'{tmp_author_data.dropna(subset=[\"subreddit_embed\"], axis=0).loc[:, \"author\"].nunique()}/{author_embeddings_data.loc[:, \"author\"].nunique()} authors retained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 00:00:00+00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "2018-01-01 00:00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "# display(tmp_author_data.loc[:, 'date_day_bin'].head())\n",
    "# display(author_embeddings_data.loc[:, 'date_day_bin'].head())\n",
    "x = tmp_author_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "y = author_embeddings_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "print(x)\n",
    "print(type(x))\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5147648e+09 1.5304032e+09 1.5463008e+09 1.5619392e+09]\n",
      "2018-01-01 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "embedding_date_bin = author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: x.timestamp()).unique()\n",
    "test_date = datetime.strptime('2018-03-01', '%Y-%m-%d')\n",
    "test_date = test_date.timestamp()\n",
    "# print(dir(test_date))\n",
    "test_bin_date = assign_date_bin(test_date, embedding_date_bin)\n",
    "print(embedding_date_bins)\n",
    "print(test_bin_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strptime(test_bin_date.strftime('%Y-%m-%d'), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 00:00:00\n",
      "2018-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from pytz import utc\n",
    "test_date_bin = author_embeddings_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "print(test_date_bin)\n",
    "print(datetime.fromtimestamp(test_date_bin.timestamp(), tz=utc).replace(tzinfo=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "timestamp() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f80cc13146b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# timezone(datetime.timedelta(seceonds=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_date_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: timestamp() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "# from datetime import timezone\n",
    "from datetime import tzinfo\n",
    "from datetime import timedelta\n",
    "# timezone(datetime.timedelta(seceonds=0))\n",
    "datetime.fromtimestamp(test_date_bin.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function fromtimestamp:\n",
      "\n",
      "fromtimestamp(...) method of builtins.type instance\n",
      "    timestamp[, tz] -> tz's local time from POSIX timestamp.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

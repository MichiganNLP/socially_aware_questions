{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute data summary statistics\n",
    "Now that we've organized and cleaned the question data, let's compute some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734967 total questions\n",
      "272467 total posts\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "# data = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', sep='\\t', compression='gzip')\n",
    "comment_data = pd.read_csv('../../data/reddit_data/advice_subreddit_filter_comment_question_data.gz', sep='\\t', compression='gzip')\n",
    "post_data = pd.read_csv('../../data/reddit_data/subreddit_submissions_2018-01_2019-12.gz', sep='\\t', compression='gzip', index_col=False, usecols=['id', 'selftext'])\n",
    "post_data.rename(columns={'selftext' : 'article_text', 'id' : 'parent_id'}, inplace=True)\n",
    "data = pd.merge(comment_data, post_data, on=['parent_id'], how='left')\n",
    "print(f'{data.shape[0]} total questions')\n",
    "print(f'{data.loc[:, \"parent_id\"].nunique()} total posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "# import torch\n",
    "# model_tokenizer = torch.load('../../data/reddit_data/BART_tokenizer.pt')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "data = data.assign(**{\n",
    "    'post_tokens' : data.loc[:, 'article_text'].apply(lambda x: word_tokenizer.tokenize(x)),\n",
    "    'question_tokens' : data.loc[:, 'question'].apply(lambda x: word_tokenizer.tokenize(x)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean post length = 304.5176422887014 +/- 220.76208617123916\n",
      "mean question length = 13.887307865523214 +/- 8.07912311345707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "post_len = data.loc[:, \"post_tokens\"].apply(lambda x: len(x)).values\n",
    "question_len = data.loc[:, \"question_tokens\"].apply(lambda x: len(x)).values\n",
    "print(f'mean post length = {np.mean(post_len)} +/- {np.std(post_len)}')\n",
    "print(f'mean question length = {np.mean(question_len)} +/- {np.std(question_len)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author data\n",
    "What % of the data includes metadata about authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (1,2,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date_day</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>expert_pct</th>\n",
       "      <th>relative_time</th>\n",
       "      <th>expert_pct_bin</th>\n",
       "      <th>relative_time_bin</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "      <th>location_region</th>\n",
       "      <th>date_day_bin</th>\n",
       "      <th>text_embed</th>\n",
       "      <th>subreddit_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battleboy43</td>\n",
       "      <td>2018-06-02</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m4xk0</td>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hAbadabadoo22</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8878.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haysu-christo</td>\n",
       "      <td>2018-11-04</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86011.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haysu-christo</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author    date_day        subreddit  expert_pct  relative_time  \\\n",
       "0    Battleboy43  2018-06-02     pcmasterrace         0.0        78340.0   \n",
       "1          m4xk0  2018-07-13     pcmasterrace         0.0        69552.0   \n",
       "2  hAbadabadoo22  2019-02-06    AmItheAsshole         0.0         8878.0   \n",
       "3  haysu-christo  2018-11-04  personalfinance         0.0        86011.0   \n",
       "4  haysu-christo  2018-11-05  personalfinance         0.0        74117.0   \n",
       "\n",
       "   expert_pct_bin  relative_time_bin  age location location_region  \\\n",
       "0             0.0                0.0 -1.0      UNK             UNK   \n",
       "1             0.0                0.0 -1.0      UNK             UNK   \n",
       "2             0.0                0.0 -1.0      UNK             UNK   \n",
       "3             0.0                1.0 -1.0      UNK             UNK   \n",
       "4             0.0                0.0 -1.0      UNK             UNK   \n",
       "\n",
       "  date_day_bin text_embed subreddit_embed  \n",
       "0   2018-01-01        NaN             NaN  \n",
       "1   2018-07-01        NaN             NaN  \n",
       "2   2019-01-01        NaN             NaN  \n",
       "3   2018-07-01        NaN             NaN  \n",
       "4   2018-07-01        NaN             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "author_data = pd.read_csv('../../data/reddit_data/author_data/combined_author_prior_comment_data.gz', sep='\\t', compression='gzip')\n",
    "display(author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.78739861789713% questions can be connected to an author with some kind of data\n"
     ]
    }
   ],
   "source": [
    "valid_authors = set(author_data.loc[:, 'author'].unique())\n",
    "valid_author_data = data[data.loc[:, 'author'].isin(valid_authors)]\n",
    "print(f'{valid_author_data.shape[0]/data.shape[0]*100}% questions can be connected to an author with some kind of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference for discrete and continuous representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.82883177067814% questions can be connected to an author with a continuous representation\n"
     ]
    }
   ],
   "source": [
    "continuous_vars = ['text_embed', 'subreddit_embed']\n",
    "valid_continuous_var_authors = set(author_data.dropna(subset=continuous_vars, how='all').loc[:, 'author'].unique())\n",
    "continuous_var_author_data = data[data.loc[:, 'author'].isin(valid_continuous_var_authors)]\n",
    "print(f'{continuous_var_author_data.shape[0]/data.shape[0]*100}% questions can be connected to an author with a continuous representation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.757672113169706% questions can be connected to an author with a subreddit embedding\n"
     ]
    }
   ],
   "source": [
    "subreddit_embed_data = pd.read_csv('../../data/reddit_data/author_data/author_date_embeddings_type=subreddit.gz', sep='\\t', compression='gzip', index_col=False)\n",
    "subreddit_embed_data.dropna(subset=['subreddit_embed'], axis=0, inplace=True)\n",
    "subreddit_embed_authors = subreddit_embed_data.loc[:, 'author'].unique()\n",
    "print(f'{data[data.loc[:, \"author\"].isin(subreddit_embed_authors)].shape[0]/data.shape[0]*100}% questions can be connected to an author with a subreddit embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date_day</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>expert_pct</th>\n",
       "      <th>relative_time</th>\n",
       "      <th>expert_pct_bin</th>\n",
       "      <th>relative_time_bin</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "      <th>location_region</th>\n",
       "      <th>date_day_bin</th>\n",
       "      <th>text_embed</th>\n",
       "      <th>subreddit_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>hek14</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>85815</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[0.2900071144104004, 0.05066739022731781, -0.0...</td>\n",
       "      <td>[6.103153527299697, -0.2204921501912914, -0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>hek14</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82345</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[0.2900071144104004, 0.05066739022731781, -0.0...</td>\n",
       "      <td>[6.103153527299697, -0.2204921501912914, -0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>hek14</td>\n",
       "      <td>2019-07-25</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>86263</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[0.2900071144104004, 0.05066739022731781, -0.0...</td>\n",
       "      <td>[6.103153527299697, -0.2204921501912914, -0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>IronHorse1776</td>\n",
       "      <td>2018-07-12</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>86137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[0.16993024945259094, 0.0948055163025856, -0.1...</td>\n",
       "      <td>[12.64313784030471, 10.702728258281661, 1.8678...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>olenavy</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>69755</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[0.32217082381248474, 0.43829774856567383, 0.0...</td>\n",
       "      <td>[16.89428571284433, 51.67949200775377, 45.1624...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author    date_day        subreddit  expert_pct  relative_time  \\\n",
       "196          hek14  2019-07-12    AmItheAsshole    0.666667          85815   \n",
       "197          hek14  2019-07-13      legaladvice    0.000000          82345   \n",
       "198          hek14  2019-07-25    AmItheAsshole    0.666667          86263   \n",
       "221  IronHorse1776  2018-07-12      legaladvice    0.339623          86137   \n",
       "236        olenavy  2018-11-05  personalfinance    0.629630          69755   \n",
       "\n",
       "     expert_pct_bin  relative_time_bin  age location location_region  \\\n",
       "196               1                  1   -1      UNK             UNK   \n",
       "197               0                  1   -1      UNK             UNK   \n",
       "198               1                  1   -1      UNK             UNK   \n",
       "221               1                  1   -1      UNK             UNK   \n",
       "236               1                  0   -1      UNK             UNK   \n",
       "\n",
       "    date_day_bin                                         text_embed  \\\n",
       "196   2019-07-01  [0.2900071144104004, 0.05066739022731781, -0.0...   \n",
       "197   2019-07-01  [0.2900071144104004, 0.05066739022731781, -0.0...   \n",
       "198   2019-07-01  [0.2900071144104004, 0.05066739022731781, -0.0...   \n",
       "221   2018-07-01  [0.16993024945259094, 0.0948055163025856, -0.1...   \n",
       "236   2018-07-01  [0.32217082381248474, 0.43829774856567383, 0.0...   \n",
       "\n",
       "                                       subreddit_embed  \n",
       "196  [6.103153527299697, -0.2204921501912914, -0.25...  \n",
       "197  [6.103153527299697, -0.2204921501912914, -0.25...  \n",
       "198  [6.103153527299697, -0.2204921501912914, -0.25...  \n",
       "221  [12.64313784030471, 10.702728258281661, 1.8678...  \n",
       "236  [16.89428571284433, 51.67949200775377, 45.1624...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_author_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.757672113169706% questions can be connected to an author with a subreddit embedding\n"
     ]
    }
   ],
   "source": [
    "embed_author_data = pd.merge(author_data.drop('subreddit_embed', axis=1), subreddit_embed_data, on=['author', 'date_day_bin'], how='outer').dropna(subset=['subreddit_embed'], axis=0)\n",
    "print(f'{data[data.loc[:, \"author\"].isin(embed_author_data.loc[:, \"author\"].unique())].shape[0]/data.shape[0]*100}% questions can be connected to an author with a subreddit embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8494"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(subreddit_embed_authors) - set(author_data.loc[:, 'author'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug: joining with embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we losing so many of the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## where are all the embeddings going?\n",
    "from datetime import datetime\n",
    "tmp_author_data = author_data.drop(['subreddit_embed', 'text_embed'], axis=1)\n",
    "tmp_author_data = tmp_author_data.assign(**{\n",
    "    'date_day' : tmp_author_data.loc[:, 'date_day'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "})\n",
    "from ast import literal_eval\n",
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "author_embeddings_data_file = '../../data/reddit_data/author_data/author_date_embeddings_type=subreddit.gz'\n",
    "author_embeddings_data = pd.read_csv(author_embeddings_data_file, sep='\\t', compression='gzip', index_col=False)\n",
    "embed_var = list(filter(lambda x: x.endswith('_embed'), author_embeddings_data.columns))[0]\n",
    "author_embeddings_data = author_embeddings_data.assign(**{embed_var : author_embeddings_data.loc[:, embed_var].apply(lambda x: literal_eval(x))})\n",
    "author_embeddings_data = author_embeddings_data.assign(**{'date_day_bin' : author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))})\n",
    "## join w/ author data via date\n",
    "embedding_date_bins = author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: x.timestamp()).unique()\n",
    "tmp_author_data = tmp_author_data.assign(**{\n",
    "    'date_day_bin' : tmp_author_data.loc[:, 'date_day'].apply(lambda x: assign_date_bin(x.timestamp(), embedding_date_bins))\n",
    "})\n",
    "tmp_author_data = pd.merge(tmp_author_data, author_embeddings_data.loc[:, ['author', 'date_day_bin', embed_var]], on=['author', 'date_day_bin'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22148/55609 authors retained\n"
     ]
    }
   ],
   "source": [
    "print(f'{tmp_author_data.dropna(subset=[\"subreddit_embed\"], axis=0).loc[:, \"author\"].nunique()}/{author_embeddings_data.loc[:, \"author\"].nunique()} authors retained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 00:00:00+00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "2018-01-01 00:00:00\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "# display(tmp_author_data.loc[:, 'date_day_bin'].head())\n",
    "# display(author_embeddings_data.loc[:, 'date_day_bin'].head())\n",
    "x = tmp_author_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "y = author_embeddings_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "print(x)\n",
    "print(type(x))\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5147648e+09 1.5304032e+09 1.5463008e+09 1.5619392e+09]\n",
      "2018-01-01 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import assign_date_bin\n",
    "embedding_date_bin = author_embeddings_data.loc[:, 'date_day_bin'].apply(lambda x: x.timestamp()).unique()\n",
    "test_date = datetime.strptime('2018-03-01', '%Y-%m-%d')\n",
    "test_date = test_date.timestamp()\n",
    "# print(dir(test_date))\n",
    "test_bin_date = assign_date_bin(test_date, embedding_date_bin)\n",
    "print(embedding_date_bins)\n",
    "print(test_bin_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strptime(test_bin_date.strftime('%Y-%m-%d'), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 00:00:00\n",
      "2018-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from pytz import utc\n",
    "test_date_bin = author_embeddings_data.loc[:, 'date_day_bin'].iloc[0]\n",
    "print(test_date_bin)\n",
    "print(datetime.fromtimestamp(test_date_bin.timestamp(), tz=utc).replace(tzinfo=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "timestamp() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f80cc13146b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# timezone(datetime.timedelta(seceonds=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_date_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: timestamp() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "# from datetime import timezone\n",
    "from datetime import tzinfo\n",
    "from datetime import timedelta\n",
    "# timezone(datetime.timedelta(seceonds=0))\n",
    "datetime.fromtimestamp(test_date_bin.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function fromtimestamp:\n",
      "\n",
      "fromtimestamp(...) method of builtins.type instance\n",
      "    timestamp[, tz] -> tz's local time from POSIX timestamp.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

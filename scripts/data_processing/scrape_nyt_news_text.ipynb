{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium news scrape\n",
    "Let's try to collect news data from an internet database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://www.python.org\")\n",
    "assert \"Python\" in driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proquest_command_file = '/Users/istewart/Documents/tools/selenium/proquest_search.side'\n",
    "import json\n",
    "user_cred_file = '../../data/umich_cred.json'\n",
    "user_cred = json.load(open(user_cred_file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "target_site = 'https://search-proquest-com.proxy.lib.umich.edu/advanced?accountid=14667'\n",
    "driver.get(target_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title = \"dr. fauci sees 'terribly painful' months ahead\"\n",
    "news_publisher_title = 'new york times'\n",
    "text_field_1 = driver.find_element_by_id('queryTermField')\n",
    "text_field_1.send_keys(article_title)\n",
    "text_field_2 = driver.find_element_by_id('queryTermField_0')\n",
    "text_field_2.send_keys(news_publisher_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify date\n",
    "from selenium.webdriver.support.ui import Select\n",
    "date = 'November 20 2020'\n",
    "date_month, date_day, date_year = date.split(' ')\n",
    "date_menu = Select(driver.find_element_by_id('select_multiDateRange'))\n",
    "date_menu.select_by_visible_text('On this date...')\n",
    "month_date_menu = Select(driver.find_element_by_id('month2'))\n",
    "day_date_menu = Select(driver.find_element_by_id('day2'))\n",
    "year_input = driver.find_element_by_id('year2')\n",
    "month_date_menu.select_by_visible_text(date_month)\n",
    "day_date_menu.select_by_visible_text(date_day)\n",
    "year_input.send_keys(date_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## submit query!!\n",
    "submit_button = driver.find_element_by_id('searchToResultPage')\n",
    "submit_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered link <selenium.webdriver.firefox.webelement.FirefoxWebElement (session=\"9a6eefe2-1037-cc49-b9a6-d29390d6f1fe\", element=\"098580cc-dcd6-7f4a-8c73-90f6f2110224\")>\n"
     ]
    }
   ],
   "source": [
    "## get first result with full text\n",
    "result_item_txt_link = None\n",
    "result_item_list = driver.find_element_by_class_name('resultItems')\n",
    "result_items = driver.find_elements_by_id('mlditem1')\n",
    "for result_item in result_items:\n",
    "    result_item_txt_link = result_item.find_element_by_id('addFlashPageParameterformat_fulltext')\n",
    "    if(result_item_txt_link is not None):\n",
    "        break\n",
    "print(f'recovered link {result_item_txt_link}')\n",
    "if(result_item_txt_link is not None):\n",
    "    result_item_txt_link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get page content\n",
    "# article id\n",
    "import re\n",
    "article_ID_matcher = re.compile('(?<=fulltext/)[0-9A-Za-z]+(?=/)')\n",
    "article_ID = article_ID_matcher.search(driver.current_url).group(0)\n",
    "# article title\n",
    "result_title = driver.find_element_by_id('documentTitle')\n",
    "result_title_txt = result_title.text\n",
    "# article authors\n",
    "result_authors = driver.find_element_by_class_name('titleAuthorETC')\n",
    "result_author_links = result_authors.find_elements_by_css_selector('a')\n",
    "result_author_txt = list(map(lambda x: x.text, result_author_links))\n",
    "# article text\n",
    "result_text_zone = driver.find_element_by_id('fullTextZone')\n",
    "result_text_paragraphs = result_text_zone.find_elements_by_css_selector('p')\n",
    "result_paragraph_text = ' '.join(list(map(lambda x: x.text, result_text_paragraphs)))\n",
    "## combine, write to file\n",
    "import pandas as pd\n",
    "import os\n",
    "result_df = pd.DataFrame([article_ID, result_title_txt, result_author_txt, result_paragraph_text], index=['id', 'title', 'authors', 'text']).transpose()\n",
    "out_dir = '../../data/NYT_scrape/'\n",
    "if(not os.path.exists(out_dir)):\n",
    "    os.mkdir(out_dir)\n",
    "out_file = os.path.join(out_dir, f'{article_ID}_data.tsv')\n",
    "result_df.to_csv(out_file, sep='\\t', index=False)\n",
    "# print(result_df)\n",
    "# print(result_text_zone.find_elements_by('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run scraping on several articles\n",
    "Now that we've gotten scraping \"right\", let's try to run it on some sample NYT articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import Select\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "def scrape_article(article_title, article_date, article_publisher, \n",
    "                   target_site, driver,\n",
    "                   RESULT_LOAD_TIME=15):\n",
    "#     print(f'getting target site {target_site}')\n",
    "    # set default article data\n",
    "    article_ID = None\n",
    "    result_title_txt = None\n",
    "    result_author_txt = None\n",
    "    result_paragraph_text = None\n",
    "    driver.get(target_site)\n",
    "    # let site load\n",
    "    site_load_time_const = RESULT_LOAD_TIME / 3.\n",
    "    site_load_time = site_load_time_const + np.random.random()*(site_load_time_const)\n",
    "    sleep(site_load_time)\n",
    "#     article_title = \"dr. fauci sees 'terribly painful' months ahead\"\n",
    "    ## specify title and publication\n",
    "    # set title\n",
    "    text_field_1 = driver.find_element_by_id('queryTermField')\n",
    "    text_field_1.clear()\n",
    "    text_field_1.send_keys(article_title)\n",
    "    text_selection_menu_1 = Select(driver.find_element_by_id('fieldsSelect'))\n",
    "    text_selection_menu_1.select_by_value('ti')\n",
    "    # set publication\n",
    "    text_field_2 = driver.find_element_by_id('queryTermField_0')\n",
    "    text_field_2.clear()\n",
    "    text_field_2.send_keys(article_publisher)\n",
    "    text_selection_menu_2 = Select(driver.find_element_by_id('fieldsSelect_0'))\n",
    "    text_selection_menu_2.select_by_value('pub')\n",
    "    ## specify date\n",
    "    ## date range: [publish date, publish date + X]\n",
    "    # compute end date\n",
    "    MAX_DATE_DAYS = 2\n",
    "    date_fmt = '%B %d %Y'\n",
    "    article_date_time = datetime.strptime(article_date, date_fmt)\n",
    "    end_date = article_date_time + timedelta(days=MAX_DATE_DAYS)\n",
    "    end_date_str = datetime.strftime(end_date, date_fmt)\n",
    "    ## extract from date format: November 20 2020\n",
    "    start_date_month, start_date_day, start_date_year = article_date.split(' ')\n",
    "    end_date_month, end_date_day, end_date_year = end_date_str.split(' ')\n",
    "    # fix day format\n",
    "    start_date_day = str(int(start_date_day))\n",
    "    end_date_day = str(int(end_date_day))\n",
    "    # get date menus\n",
    "    date_menu = Select(driver.find_element_by_id('select_multiDateRange'))\n",
    "#     date_menu.select_by_visible_text('On this date...')\n",
    "    date_menu.select_by_value('RANGE')\n",
    "    # start date\n",
    "    start_month_date_menu = Select(driver.find_element_by_id('month2'))\n",
    "    start_day_date_menu = Select(driver.find_element_by_id('day2'))\n",
    "    start_year_input = driver.find_element_by_id('year2')\n",
    "    start_month_date_menu.select_by_visible_text(start_date_month)\n",
    "    start_day_date_menu.select_by_visible_text(start_date_day)\n",
    "    start_year_input.send_keys(start_date_year)\n",
    "    # end date\n",
    "    end_month_date_menu = Select(driver.find_element_by_id('month2_0'))\n",
    "    end_day_date_menu = Select(driver.find_element_by_id('day2_0'))\n",
    "    end_year_input = driver.find_element_by_id('year2_0')\n",
    "    end_month_date_menu.select_by_visible_text(end_date_month)\n",
    "    end_day_date_menu.select_by_visible_text(end_date_day)\n",
    "    end_year_input.send_keys(end_date_year)\n",
    "    ## submit query!!\n",
    "    submit_button = driver.find_element_by_id('searchToResultPage')\n",
    "    submit_button.click()\n",
    "    sleep(RESULT_LOAD_TIME)\n",
    "    ## get first result with full text\n",
    "    # if bad search, skip to next article\n",
    "    result_item_txt_link = None\n",
    "    result_item_list = None\n",
    "    try:\n",
    "        result_item_list = driver.find_element_by_class_name('resultItems')\n",
    "#         print(f'result item list {result_item_list}')\n",
    "    except Exception as e:\n",
    "        print(f'error {e}')\n",
    "        pass\n",
    "    no_results = result_item_list is None\n",
    "    if(not no_results):\n",
    "        result_items = driver.find_elements_by_id('mlditem1')\n",
    "        for result_item in result_items:\n",
    "            result_item_txt_link = None\n",
    "            try:\n",
    "                result_item_txt_link = result_item.find_element_by_id('addFlashPageParameterformat_fulltext')\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            if(result_item_txt_link is not None):\n",
    "                break\n",
    "#         print(f'recovered link {result_item_txt_link}')\n",
    "        if(result_item_txt_link is not None):\n",
    "            result_item_txt_link.click()\n",
    "            # wait to load\n",
    "            sleep(RESULT_LOAD_TIME)\n",
    "            ## get page content\n",
    "            # article id\n",
    "            article_ID_matcher = re.compile('(?<=fulltext/)[0-9A-Za-z]+(?=/)')\n",
    "            print(f'extracting ID from URL {driver.current_url}')\n",
    "            article_ID = article_ID_matcher.search(driver.current_url).group(0)\n",
    "            # article title\n",
    "            result_title = driver.find_element_by_id('documentTitle')\n",
    "            result_title_txt = result_title.text\n",
    "            # article authors\n",
    "            result_authors = driver.find_element_by_class_name('titleAuthorETC')\n",
    "            result_author_links = result_authors.find_elements_by_css_selector('a')\n",
    "            result_author_txt = list(map(lambda x: x.text, result_author_links))\n",
    "            # article text\n",
    "            result_text_zone = driver.find_element_by_id('fullTextZone')\n",
    "            result_text_paragraphs = result_text_zone.find_elements_by_css_selector('p')\n",
    "            result_paragraph_text = ' '.join(list(map(lambda x: x.text, result_text_paragraphs)))\n",
    "    ## combine, write to file\n",
    "    result_df = pd.DataFrame([article_ID, result_title_txt, result_author_txt, result_paragraph_text], index=['id', 'title', 'authors', 'text']).transpose()\n",
    "    return result_df\n",
    "#     out_dir = '../../data/NYT_scrape/'\n",
    "#     if(not os.path.exists(out_dir)):\n",
    "#         os.mkdir(out_dir)\n",
    "#     out_file = os.path.join(out_dir, f'{article_ID}_data.tsv')\n",
    "#     result_df.to_csv(out_file, sep='\\t', index=False)\n",
    "    # print(result_df)\n",
    "    # print(result_text_zone.find_elements_by('p'))\n",
    "from time import sleep\n",
    "def scrape_write_article(article_title, article_date, article_publisher, \n",
    "                         original_article_id, target_site, driver, out_dir):\n",
    "    \"\"\"\n",
    "    Scrape article data and write to file.\n",
    "    \"\"\"\n",
    "    result_data = scrape_article(article_title, article_date, article_publisher, target_site, driver)\n",
    "    out_file = os.path.join(out_dir, f'article_{original_article_id}.tsv')\n",
    "    result_data.to_csv(out_file, sep='\\t', index=False)\n",
    "import numpy as np\n",
    "def scrape_write_all_articles(article_data, article_publisher, target_site, driver, out_dir, SLEEP_TIME=15, verbose=True):\n",
    "    \"\"\"\n",
    "    Scrape and write all articles to file. Sleep between scrapes.\n",
    "    \"\"\"\n",
    "    rand_sleep_time_scale = SLEEP_TIME / 3.\n",
    "    # first thing: login\n",
    "    driver.get(target_site)\n",
    "    LOGIN_TIME=30\n",
    "    login_time_i = LOGIN_TIME + np.random.random() * (LOGIN_TIME / 10)\n",
    "    time.sleep(login_time_i)\n",
    "    for i, (idx_i, data_i) in enumerate(article_data.iterrows()):\n",
    "        article_title_i = data_i.loc['title']\n",
    "        article_date_i = data_i.loc['date']\n",
    "        article_id_i = data_i.loc['articleID']\n",
    "        out_file = os.path.join(out_dir, f'article_{article_id_i}.tsv')\n",
    "        if(not os.path.exists(out_file)):\n",
    "            if(verbose):\n",
    "                print(f'mining article {article_id_i}')\n",
    "            scrape_write_article(article_title_i, article_date_i, article_publisher, \n",
    "                                 article_id_i, target_site, driver, out_dir)\n",
    "            sleep_time_i = SLEEP_TIME + np.random.random() * (rand_sleep_time_scale)\n",
    "            sleep(sleep_time_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066 articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>title</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>date_time</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a7258e410f40f00018bed7d</td>\n",
       "      <td>I Stand  With the ‘She-Devils’</td>\n",
       "      <td>2018-02-01 00:01:36</td>\n",
       "      <td>2018-02-01 00:01:36</td>\n",
       "      <td>February 01 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a725f2410f40f00018bed89</td>\n",
       "      <td>Trump’s Birth Control Problems</td>\n",
       "      <td>2018-02-01 00:28:16</td>\n",
       "      <td>2018-02-01 00:28:16</td>\n",
       "      <td>February 01 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a725f4210f40f00018bed8a</td>\n",
       "      <td>What’s the Craziest Thing You’ve Ever Found in...</td>\n",
       "      <td>2018-02-01 00:28:46</td>\n",
       "      <td>2018-02-01 00:28:46</td>\n",
       "      <td>February 01 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a72646310f40f00018bed97</td>\n",
       "      <td>U.S. Allies’ Conflict Is ISIS’ Gain</td>\n",
       "      <td>2018-02-01 00:50:37</td>\n",
       "      <td>2018-02-01 00:50:37</td>\n",
       "      <td>February 01 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a72686b10f40f00018bed9e</td>\n",
       "      <td>$1.5 Trillion Plan on Infrastructure, but Not ...</td>\n",
       "      <td>2018-02-01 01:07:50</td>\n",
       "      <td>2018-02-01 01:07:50</td>\n",
       "      <td>February 01 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  \\\n",
       "0  5a7258e410f40f00018bed7d   \n",
       "1  5a725f2410f40f00018bed89   \n",
       "2  5a725f4210f40f00018bed8a   \n",
       "3  5a72646310f40f00018bed97   \n",
       "4  5a72686b10f40f00018bed9e   \n",
       "\n",
       "                                               title              pubDate  \\\n",
       "0                     I Stand  With the ‘She-Devils’  2018-02-01 00:01:36   \n",
       "1                     Trump’s Birth Control Problems  2018-02-01 00:28:16   \n",
       "2  What’s the Craziest Thing You’ve Ever Found in...  2018-02-01 00:28:46   \n",
       "3                U.S. Allies’ Conflict Is ISIS’ Gain  2018-02-01 00:50:37   \n",
       "4  $1.5 Trillion Plan on Infrastructure, but Not ...  2018-02-01 01:07:50   \n",
       "\n",
       "            date_time              date  \n",
       "0 2018-02-01 00:01:36  February 01 2018  \n",
       "1 2018-02-01 00:28:16  February 01 2018  \n",
       "2 2018-02-01 00:28:46  February 01 2018  \n",
       "3 2018-02-01 00:50:37  February 01 2018  \n",
       "4 2018-02-01 01:07:50  February 01 2018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load comment data\n",
    "import pandas as pd\n",
    "# comment_data = pd.read_csv('../../data/nyt_comments/CommentsApril2018.csv', sep=',', index_col=False, usecols=['articleID', ''])\n",
    "article_data = pd.read_csv('../../data/nyt_comments/ArticlesFeb2018.csv', sep=',', index_col=False, usecols=['articleID', 'headline', 'pubDate'])\n",
    "article_data.rename(columns={'headline' : 'title'}, inplace=True)\n",
    "article_data = article_data[article_data.loc[:, 'title'] != 'Unknown']\n",
    "print('%d articles'%(article_data.shape[0]))\n",
    "# simplify date\n",
    "from datetime import datetime\n",
    "date_fmt = '%Y-%m-%d %H:%M:%S'\n",
    "article_data = article_data.assign(**{\n",
    "    'date_time' : article_data.loc[:, 'pubDate'].apply(lambda x: datetime.strptime(x, date_fmt))\n",
    "})\n",
    "clean_date_fmt = '%B %d %Y'\n",
    "article_data = article_data.assign(**{\n",
    "    'date' : article_data.loc[:, 'date_time'].apply(lambda x: datetime.strftime(x, clean_date_fmt))\n",
    "})\n",
    "# get sample to mine\n",
    "sample_size = 1000\n",
    "sample_article_data = article_data.head(sample_size)\n",
    "display(sample_article_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mining article 5a949e6e10f40f00018c32ca\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008458253/fulltext/ABCDB78A7AC4491CPQ/1?accountid=14667\n",
      "mining article 5a9499a310f40f00018c32c6\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008315493/fulltext/FBD5A76A871149C1PQ/1?accountid=14667\n",
      "mining article 5a94943e10f40f00018c32af\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008261748/fulltext/9E9F6168AF10486APQ/1?accountid=14667\n",
      "mining article 5a94917810f40f00018c32a5\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008144427/fulltext/9CDCB66A6F0940F8PQ/1?accountid=14667\n",
      "mining article 5a9481ef10f40f00018c3276\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008264288/fulltext/33115C2168744ED0PQ/1?accountid=14667\n",
      "mining article 5a947b8310f40f00018c326b\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008723414/fulltext/7136961509E64B5CPQ/1?accountid=14667\n",
      "mining article 5a9476fe10f40f00018c325e\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008723920/fulltext/334C202F731846DBPQ/1?accountid=14667\n",
      "mining article 5a94749410f40f00018c3258\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008723511/fulltext/4DDB8407DFD2452APQ/1?accountid=14667\n",
      "mining article 5a9462d210f40f00018c3229\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008261782/fulltext/55D0682BC8144DA7PQ/1?accountid=14667\n",
      "mining article 5a944d5210f40f00018c31ed\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008270635/fulltext/40CE499081DA4121PQ/1?accountid=14667\n",
      "mining article 5a943ded10f40f00018c31bf\n",
      "error Message: Unable to locate element: .resultItems\n",
      "\n",
      "mining article 5a943a0610f40f00018c31ad\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008723848/fulltext/3817E8A80A4648EFPQ/1?accountid=14667\n",
      "mining article 5a9432c210f40f00018c3195\n",
      "error Message: Unable to locate element: .resultItems\n",
      "\n",
      "mining article 5a9431c410f40f00018c3190\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008723475/fulltext/90CFE0FD171940D1PQ/1?accountid=14667\n",
      "mining article 5a942e6610f40f00018c317f\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008287385/fulltext/9E0D899AA63B4238PQ/1?accountid=14667\n",
      "mining article 5a941bca10f40f00018c3143\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008269280/fulltext/806545AE6C2D4820PQ/1?accountid=14667\n",
      "mining article 5a9407c510f40f00018c3111\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008285118/fulltext/FF1F08BB605F4453PQ/1?accountid=14667\n",
      "mining article 5a94000e10f40f00018c3101\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008270640/fulltext/177782FDEA14C71PQ/1?accountid=14667\n",
      "mining article 5a93f5fd10f40f00018c30e7\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2007958419/fulltext/187734A16FCE4611PQ/1?accountid=14667\n",
      "mining article 5a93f59110f40f00018c30e5\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008254988/fulltext/721A0BD60199468EPQ/1?accountid=14667\n",
      "mining article 5a93e8c410f40f00018c30b5\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008287658/fulltext/FA4A68C374274D4CPQ/1?accountid=14667\n",
      "mining article 5a93e8c210f40f00018c30b4\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2008275279/fulltext/7291EEA39E3946B5PQ/1?accountid=14667\n",
      "mining article 5a93e8bb10f40f00018c30b2\n",
      "extracting ID from URL https://search-proquest-com.proxy.lib.umich.edu/docview/2007936930/fulltext/52D5B985F42E4FFCPQ/1?accountid=14667\n"
     ]
    }
   ],
   "source": [
    "target_site = 'https://search-proquest-com.proxy.lib.umich.edu/advanced?accountid=14667'\n",
    "article_publisher = 'new york times'\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import tempfile\n",
    "options = Options()\n",
    "options.log.level = 'trace'\n",
    "# debugging connection problems\n",
    "# profile = tempfile.mkdtemp(\".selenium\")\n",
    "# print(\"*** Using profile: {}\".format(profile))\n",
    "# options.add_argument(\"-profile\")\n",
    "# options.add_argument(profile)\n",
    "# options.binary = '/Applications/Firefox.app/Contents/MacOS/firefox'\n",
    "# options.add_argument('-headless')\n",
    "# TODO: error due to Selenium version?\n",
    "# driver = Firefox(executable_path='/Users/istewart/Library/Application Support/WebDriverManager/gecko/v0.28.0/geckodriver-v0.28.0-macos/geckodriver', options=options)\n",
    "driver = Firefox(options=options)\n",
    "# driver = Opera()\n",
    "out_dir = '../../data/NYT_scrape/'\n",
    "# driver.get(target_site)\n",
    "scrape_write_all_articles(sample_article_data, article_publisher, target_site, driver, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the sample articles were we able to recover? This will give us an (imperfect) estimate of the overall coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704/2004 valid articles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "out_dir = '../../data/NYT_scrape/'\n",
    "article_text_files = list(map(lambda x: os.path.join(out_dir, x), os.listdir(out_dir)))\n",
    "article_text_data = pd.concat(list(map(lambda x: pd.read_csv(x, sep='\\t', index_col=False), article_text_files)), axis=0)\n",
    "valid_article_text_data = article_text_data[~article_text_data.loc[:, 'id'].apply(lambda x: type(x) is not str and np.isnan(x))]\n",
    "print(f'{valid_article_text_data.shape[0]}/{article_text_data.shape[0]} valid articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So we get ~85% recall which is impressive considering that headlines change so often in news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium news scrape\n",
    "Let's try to collect news data from an internet database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://www.python.org\")\n",
    "assert \"Python\" in driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proquest_command_file = '/Users/istewart/Documents/tools/selenium/proquest_search.side'\n",
    "import json\n",
    "user_cred_file = '../../data/umich_cred.json'\n",
    "user_cred = json.load(open(user_cred_file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "target_site = 'https://search-proquest-com.proxy.lib.umich.edu/advanced?accountid=14667'\n",
    "driver.get(target_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title = \"dr. fauci sees 'terribly painful' months ahead\"\n",
    "news_publisher_title = 'new york times'\n",
    "text_field_1 = driver.find_element_by_id('queryTermField')\n",
    "text_field_1.send_keys(article_title)\n",
    "text_field_2 = driver.find_element_by_id('queryTermField_0')\n",
    "text_field_2.send_keys(news_publisher_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify date\n",
    "from selenium.webdriver.support.ui import Select\n",
    "date = 'November 20 2020'\n",
    "date_month, date_day, date_year = date.split(' ')\n",
    "date_menu = Select(driver.find_element_by_id('select_multiDateRange'))\n",
    "date_menu.select_by_visible_text('On this date...')\n",
    "month_date_menu = Select(driver.find_element_by_id('month2'))\n",
    "day_date_menu = Select(driver.find_element_by_id('day2'))\n",
    "year_input = driver.find_element_by_id('year2')\n",
    "month_date_menu.select_by_visible_text(date_month)\n",
    "day_date_menu.select_by_visible_text(date_day)\n",
    "year_input.send_keys(date_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## submit query!!\n",
    "submit_button = driver.find_element_by_id('searchToResultPage')\n",
    "submit_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered link <selenium.webdriver.firefox.webelement.FirefoxWebElement (session=\"9a6eefe2-1037-cc49-b9a6-d29390d6f1fe\", element=\"098580cc-dcd6-7f4a-8c73-90f6f2110224\")>\n"
     ]
    }
   ],
   "source": [
    "## get first result with full text\n",
    "result_item_txt_link = None\n",
    "result_item_list = driver.find_element_by_class_name('resultItems')\n",
    "result_items = driver.find_elements_by_id('mlditem1')\n",
    "for result_item in result_items:\n",
    "    result_item_txt_link = result_item.find_element_by_id('addFlashPageParameterformat_fulltext')\n",
    "    if(result_item_txt_link is not None):\n",
    "        break\n",
    "print(f'recovered link {result_item_txt_link}')\n",
    "if(result_item_txt_link is not None):\n",
    "    result_item_txt_link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get page content\n",
    "# article id\n",
    "import re\n",
    "article_ID_matcher = re.compile('(?<=fulltext/)[0-9A-Za-z]+(?=/)')\n",
    "article_ID = article_ID_matcher.search(driver.current_url).group(0)\n",
    "# article title\n",
    "result_title = driver.find_element_by_id('documentTitle')\n",
    "result_title_txt = result_title.text\n",
    "# article authors\n",
    "result_authors = driver.find_element_by_class_name('titleAuthorETC')\n",
    "result_author_links = result_authors.find_elements_by_css_selector('a')\n",
    "result_author_txt = list(map(lambda x: x.text, result_author_links))\n",
    "# article text\n",
    "result_text_zone = driver.find_element_by_id('fullTextZone')\n",
    "result_text_paragraphs = result_text_zone.find_elements_by_css_selector('p')\n",
    "result_paragraph_text = ' '.join(list(map(lambda x: x.text, result_text_paragraphs)))\n",
    "## combine, write to file\n",
    "import pandas as pd\n",
    "import os\n",
    "result_df = pd.DataFrame([article_ID, result_title_txt, result_author_txt, result_paragraph_text], index=['id', 'title', 'authors', 'text']).transpose()\n",
    "out_dir = '../../data/NYT_scrape/'\n",
    "if(not os.path.exists(out_dir)):\n",
    "    os.mkdir(out_dir)\n",
    "out_file = os.path.join(out_dir, f'{article_ID}_data.tsv')\n",
    "result_df.to_csv(out_file, sep='\\t', index=False)\n",
    "# print(result_df)\n",
    "# print(result_text_zone.find_elements_by('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run scraping on several articles\n",
    "Now that we've gotten scraping \"right\", let's try to run it on some sample NYT articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import Select\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "def scrape_article(article_title, article_date, article_publisher, \n",
    "                   target_site, driver,\n",
    "                   RESULT_LOAD_TIME=15):\n",
    "#     print(f'getting target site {target_site}')\n",
    "    # set default article data\n",
    "    article_ID = None\n",
    "    result_title_txt = None\n",
    "    result_author_txt = None\n",
    "    result_paragraph_text = None\n",
    "    driver.get(target_site)\n",
    "    # let site load\n",
    "    site_load_time_const = RESULT_LOAD_TIME / 3.\n",
    "    site_load_time = site_load_time_const + np.random.random()*(site_load_time_const)\n",
    "    sleep(site_load_time)\n",
    "#     article_title = \"dr. fauci sees 'terribly painful' months ahead\"\n",
    "    ## specify title and publication\n",
    "    # set title\n",
    "    text_field_1 = driver.find_element_by_id('queryTermField')\n",
    "    text_field_1.clear()\n",
    "    text_field_1.send_keys(article_title)\n",
    "    text_selection_menu_1 = Select(driver.find_element_by_id('fieldsSelect'))\n",
    "    text_selection_menu_1.select_by_value('ti')\n",
    "    # set publication\n",
    "    text_field_2 = driver.find_element_by_id('queryTermField_0')\n",
    "    text_field_2.clear()\n",
    "    text_field_2.send_keys(article_publisher)\n",
    "    text_selection_menu_2 = Select(driver.find_element_by_id('fieldsSelect_0'))\n",
    "    text_selection_menu_2.select_by_value('pub')\n",
    "    ## specify date\n",
    "    ## date range: [publish date, publish date + X]\n",
    "    # compute end date\n",
    "    MAX_DATE_DAYS = 2\n",
    "    date_fmt = '%B %d %Y'\n",
    "    article_date_time = datetime.strptime(article_date, date_fmt)\n",
    "    end_date = article_date_time + timedelta(days=MAX_DATE_DAYS)\n",
    "    end_date_str = datetime.strftime(end_date, date_fmt)\n",
    "    ## extract from date format: November 20 2020\n",
    "    start_date_month, start_date_day, start_date_year = article_date.split(' ')\n",
    "    end_date_month, end_date_day, end_date_year = end_date_str.split(' ')\n",
    "    # fix day format\n",
    "    start_date_day = str(int(start_date_day))\n",
    "    end_date_day = str(int(end_date_day))\n",
    "    # get date menus\n",
    "    date_menu = Select(driver.find_element_by_id('select_multiDateRange'))\n",
    "#     date_menu.select_by_visible_text('On this date...')\n",
    "    date_menu.select_by_value('RANGE')\n",
    "    # start date\n",
    "    start_month_date_menu = Select(driver.find_element_by_id('month2'))\n",
    "    start_day_date_menu = Select(driver.find_element_by_id('day2'))\n",
    "    start_year_input = driver.find_element_by_id('year2')\n",
    "    start_month_date_menu.select_by_visible_text(start_date_month)\n",
    "    start_day_date_menu.select_by_visible_text(start_date_day)\n",
    "    start_year_input.send_keys(start_date_year)\n",
    "    # end date\n",
    "    end_month_date_menu = Select(driver.find_element_by_id('month2_0'))\n",
    "    end_day_date_menu = Select(driver.find_element_by_id('day2_0'))\n",
    "    end_year_input = driver.find_element_by_id('year2_0')\n",
    "    end_month_date_menu.select_by_visible_text(end_date_month)\n",
    "    end_day_date_menu.select_by_visible_text(end_date_day)\n",
    "    end_year_input.send_keys(end_date_year)\n",
    "    ## submit query!!\n",
    "    submit_button = driver.find_element_by_id('searchToResultPage')\n",
    "    submit_button.click()\n",
    "    sleep(RESULT_LOAD_TIME)\n",
    "    ## get first result with full text\n",
    "    # if bad search, skip to next article\n",
    "    result_item_txt_link = None\n",
    "    result_item_list = None\n",
    "    try:\n",
    "        result_item_list = driver.find_element_by_class_name('resultItems')\n",
    "#         print(f'result item list {result_item_list}')\n",
    "    except Exception as e:\n",
    "        print(f'error {e}')\n",
    "        pass\n",
    "    no_results = result_item_list is None\n",
    "    if(not no_results):\n",
    "        result_items = driver.find_elements_by_id('mlditem1')\n",
    "        for result_item in result_items:\n",
    "            result_item_txt_link = None\n",
    "            try:\n",
    "                result_item_txt_link = result_item.find_element_by_id('addFlashPageParameterformat_fulltext')\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            if(result_item_txt_link is not None):\n",
    "                break\n",
    "#         print(f'recovered link {result_item_txt_link}')\n",
    "        if(result_item_txt_link is not None):\n",
    "            result_item_txt_link.click()\n",
    "            # wait to load\n",
    "            sleep(RESULT_LOAD_TIME)\n",
    "            ## get page content\n",
    "            # article id\n",
    "            article_ID_matcher = re.compile('(?<=fulltext/)[0-9A-Za-z]+(?=/)')\n",
    "            print(f'extracting ID from URL {driver.current_url}')\n",
    "            article_ID = article_ID_matcher.search(driver.current_url).group(0)\n",
    "            # article title\n",
    "            result_title = driver.find_element_by_id('documentTitle')\n",
    "            result_title_txt = result_title.text\n",
    "            # article authors\n",
    "            result_authors = driver.find_element_by_class_name('titleAuthorETC')\n",
    "            result_author_links = result_authors.find_elements_by_css_selector('a')\n",
    "            result_author_txt = list(map(lambda x: x.text, result_author_links))\n",
    "            # article text\n",
    "            result_text_zone = driver.find_element_by_id('fullTextZone')\n",
    "            result_text_paragraphs = result_text_zone.find_elements_by_css_selector('p')\n",
    "            result_paragraph_text = ' '.join(list(map(lambda x: x.text, result_text_paragraphs)))\n",
    "    ## combine, write to file\n",
    "    result_df = pd.DataFrame([article_ID, result_title_txt, result_author_txt, result_paragraph_text], index=['id', 'title', 'authors', 'text']).transpose()\n",
    "    return result_df\n",
    "#     out_dir = '../../data/NYT_scrape/'\n",
    "#     if(not os.path.exists(out_dir)):\n",
    "#         os.mkdir(out_dir)\n",
    "#     out_file = os.path.join(out_dir, f'{article_ID}_data.tsv')\n",
    "#     result_df.to_csv(out_file, sep='\\t', index=False)\n",
    "    # print(result_df)\n",
    "    # print(result_text_zone.find_elements_by('p'))\n",
    "from time import sleep\n",
    "def scrape_write_article(article_title, article_date, article_publisher, \n",
    "                         original_article_id, target_site, driver, out_dir):\n",
    "    \"\"\"\n",
    "    Scrape article data and write to file.\n",
    "    \"\"\"\n",
    "    result_data = scrape_article(article_title, article_date, article_publisher, target_site, driver)\n",
    "    out_file = os.path.join(out_dir, f'article_{original_article_id}.tsv')\n",
    "    result_data.to_csv(out_file, sep='\\t', index=False)\n",
    "import numpy as np\n",
    "def scrape_write_all_articles(article_data, article_publisher, target_site, driver, out_dir, SLEEP_TIME=15, verbose=True):\n",
    "    \"\"\"\n",
    "    Scrape and write all articles to file. Sleep between scrapes.\n",
    "    \"\"\"\n",
    "    rand_sleep_time_scale = SLEEP_TIME / 3.\n",
    "    # first thing: login\n",
    "    driver.get(target_site)\n",
    "    LOGIN_TIME=30\n",
    "    login_time_i = LOGIN_TIME + np.random.random() * (LOGIN_TIME / 10)\n",
    "    time.sleep(login_time_i)\n",
    "    for i, (idx_i, data_i) in enumerate(article_data.iterrows()):\n",
    "        article_title_i = data_i.loc['title']\n",
    "        article_date_i = data_i.loc['date']\n",
    "        article_id_i = data_i.loc['articleID']\n",
    "        out_file = os.path.join(out_dir, f'article_{article_id_i}.tsv')\n",
    "        if(not os.path.exists(out_file)):\n",
    "            if(verbose):\n",
    "                print(f'mining article {article_id_i}')\n",
    "            scrape_write_article(article_title_i, article_date_i, article_publisher, \n",
    "                                 article_id_i, target_site, driver, out_dir)\n",
    "            sleep_time_i = SLEEP_TIME + np.random.random() * (rand_sleep_time_scale)\n",
    "            sleep(sleep_time_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214 articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>title</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>date_time</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>April 24 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>April 24 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>April 24 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5adf2108068401528a2aa5b3</td>\n",
       "      <td>How a Bag of Texas Dirt  Became a Times Tradition</td>\n",
       "      <td>2018-04-24 12:20:21</td>\n",
       "      <td>2018-04-24 12:20:21</td>\n",
       "      <td>April 24 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5adedaa8068401528a2aa4e6</td>\n",
       "      <td>Is School a Place for Self-Expression?</td>\n",
       "      <td>2018-04-24 11:21:04</td>\n",
       "      <td>2018-04-24 11:21:04</td>\n",
       "      <td>April 24 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  \\\n",
       "0  5adf6684068401528a2aa69b   \n",
       "1  5adf653f068401528a2aa697   \n",
       "2  5adf4626068401528a2aa628   \n",
       "8  5adf2108068401528a2aa5b3   \n",
       "9  5adedaa8068401528a2aa4e6   \n",
       "\n",
       "                                               title              pubDate  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...  2018-04-24 17:16:49   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...  2018-04-24 17:11:21   \n",
       "2                            The New Noma, Explained  2018-04-24 14:58:44   \n",
       "8  How a Bag of Texas Dirt  Became a Times Tradition  2018-04-24 12:20:21   \n",
       "9             Is School a Place for Self-Expression?  2018-04-24 11:21:04   \n",
       "\n",
       "            date_time           date  \n",
       "0 2018-04-24 17:16:49  April 24 2018  \n",
       "1 2018-04-24 17:11:21  April 24 2018  \n",
       "2 2018-04-24 14:58:44  April 24 2018  \n",
       "8 2018-04-24 12:20:21  April 24 2018  \n",
       "9 2018-04-24 11:21:04  April 24 2018  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load comment data\n",
    "import pandas as pd\n",
    "# comment_data = pd.read_csv('../../data/nyt_comments/CommentsApril2018.csv', sep=',', index_col=False, usecols=['articleID', ''])\n",
    "article_data = pd.read_csv('../../data/nyt_comments/ArticlesApril2018.csv', sep=',', index_col=False, usecols=['articleID', 'headline', 'pubDate'])\n",
    "article_data.rename(columns={'headline' : 'title'}, inplace=True)\n",
    "article_data = article_data[article_data.loc[:, 'title'] != 'Unknown']\n",
    "print('%d articles'%(article_data.shape[0]))\n",
    "# simplify date\n",
    "from datetime import datetime\n",
    "date_fmt = '%Y-%m-%d %H:%M:%S'\n",
    "article_data = article_data.assign(**{\n",
    "    'date_time' : article_data.loc[:, 'pubDate'].apply(lambda x: datetime.strptime(x, date_fmt))\n",
    "})\n",
    "clean_date_fmt = '%B %d %Y'\n",
    "article_data = article_data.assign(**{\n",
    "    'date' : article_data.loc[:, 'date_time'].apply(lambda x: datetime.strftime(x, clean_date_fmt))\n",
    "})\n",
    "# get sample to mine\n",
    "sample_size = 1000\n",
    "sample_article_data = article_data.head(sample_size)\n",
    "display(sample_article_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_site = 'https://search-proquest-com.proxy.lib.umich.edu/advanced?accountid=14667'\n",
    "article_publisher = 'new york times'\n",
    "from selenium.webdriver import Firefox\n",
    "driver = Firefox()\n",
    "out_dir = '../../data/NYT_scrape/'\n",
    "# driver.get(target_site)\n",
    "scrape_write_all_articles(sample_article_data, article_publisher, target_site, driver, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the sample articles were we able to recover? This will give us an (imperfect) estimate of the overall coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437/504 valid articles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "out_dir = '../../data/NYT_scrape/'\n",
    "article_text_files = list(map(lambda x: os.path.join(out_dir, x), os.listdir(out_dir)))\n",
    "article_text_data = pd.concat(list(map(lambda x: pd.read_csv(x, sep='\\t', index_col=False), article_text_files)), axis=0)\n",
    "valid_article_text_data = article_text_data[~article_text_data.loc[:, 'id'].apply(lambda x: type(x) is not str and np.isnan(x))]\n",
    "print(f'{valid_article_text_data.shape[0]}/{article_text_data.shape[0]} valid articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So we get ~85% recall which is impressive considering that headlines change so often in news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

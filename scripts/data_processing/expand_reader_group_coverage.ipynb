{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand reader group coverage\n",
    "In prior analysis, we have seen that some reader groups have poor coverage, specifically expertise and demographics.\n",
    "\n",
    "Let's use the prior author data to expand this coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7097/204619 [00:19<08:45, 375.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping file ../../data/reddit_data/author_data/OJnToothpaste_comments.gz because error Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26705/204619 [04:13<3:55:08, 12.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping file ../../data/reddit_data/author_data/papercut_eyelid_comments.gz because error Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 60381/204619 [16:16<31:27, 76.42it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping file ../../data/reddit_data/author_data/Quovef_comments.gz because error Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 141185/204619 [44:10<14:22, 73.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping file ../../data/reddit_data/author_data/cypher_chyk_comments.gz because error Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204619/204619 [1:06:21<00:00, 51.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189139\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import load_all_author_data\n",
    "author_data_dir = '../../data/reddit_data/author_data/'\n",
    "usecols = ['author', 'subreddit', 'created_utc', 'id']\n",
    "author_data = load_all_author_data(author_data_dir, usecols=usecols)\n",
    "print(author_data.loc[:, 'author'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## limit to actual question askers\n",
    "question_comment_data = pd.read_csv('../../data/reddit_data/advice_subreddit_no_filter_comment_question_data.gz', sep='\\t', compression='gzip', usecols=['author', 'parent_id', 'subreddit', 'created_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2018, 1, 1, 0, 0),\n",
       " datetime.datetime(2018, 7, 1, 0, 0),\n",
       " datetime.datetime(2019, 1, 1, 0, 0),\n",
       " datetime.datetime(2019, 7, 1, 0, 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get date info\n",
    "from datetime import datetime, timedelta\n",
    "from monthdelta import monthdelta\n",
    "import re\n",
    "# num_matcher = re.compile('\\d+\\.\\d')\n",
    "# author_data = author_data.assign(**{\n",
    "#     'date' : author_data.loc[:, 'created_utc'].apply(lambda x: datetime.fromtimestamp(int(float(x))) if (type(x) is str and num_matcher.match(x) is not None) else x),\n",
    "# })\n",
    "# question_comment_data = question_comment_data.assign(**{\n",
    "#     'date' : question_comment_data.loc[:, 'created_utc'].apply(lambda x: datetime.fromtimestamp(int(float(x)))),\n",
    "# })\n",
    "## convert to date-day bins\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "start_year, end_year = 2018, 2019\n",
    "start_month, end_month = 1, 12\n",
    "bin_month_width = 6\n",
    "months_per_year = 12\n",
    "date_bin_count = int(ceil(((end_year - start_year + 1) * months_per_year) / bin_month_width))\n",
    "start_date = datetime(year=start_year, month=1, day=1)\n",
    "date_bins = [start_date + monthdelta(bin_month_width*i) for i in range(date_bin_count)]\n",
    "date_bins = np.array(list(map(lambda x: x.timestamp(), date_bins)))\n",
    "from data_helpers import assign_date_bin\n",
    "author_data = author_data.assign(**{\n",
    "    'date_day_bin' : author_data.loc[:, 'date'].apply(lambda x: assign_date_bin(x, date_bins) if (type(x) is float and not np.isnan(x)) else None),\n",
    "})\n",
    "question_comment_data = question_comment_data.assign(**{\n",
    "    'date_day_bin' : question_comment_data.loc[:, 'date'].apply(lambda x: assign_date_bin(x.timestamp(), date_bins))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.datetime(2019, 7, 1, 0, 0),\n",
       "       datetime.datetime(2019, 1, 1, 0, 0),\n",
       "       datetime.datetime(2018, 7, 1, 0, 0),\n",
       "       datetime.datetime(2018, 1, 1, 0, 0), -1, None], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_data.loc[:, 'date_day_bin'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand expertise\n",
    "Let's expand the definition of \"expert\" by counting the prior posts in topic-relevant subreddits.\n",
    "\n",
    "Author $a$ posts a question in subreddit $s$.\n",
    "1. $a$ is an \"expert\" if at least 90% of their prior posts are in subreddit $s$.\n",
    "2. $a$ is an \"expert\" if at least 90% of their prior posts are in subreddits $\\mathcal{S}_{t}$, i.e. $s$ or one of its top-k neighbors at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect authors with original question time + subreddit info\n",
    "question_comment_data.rename(columns={'subreddit' : 'question_subreddit', 'date_day_bin' : 'question_date_day_bin'}, inplace=True)\n",
    "author_question_data = pd.merge(author_data, question_comment_data.loc[:, ['author', 'question_subreddit', 'question_date_day_bin']], on='author', how='right')\n",
    "# fix data type\n",
    "author_question_data = author_question_data.assign(**{\n",
    "    'question_date_day_bin' : pd.Series(author_question_data.loc[:, 'question_date_day_bin'].values, dtype='object')\n",
    "})\n",
    "# remove null data\n",
    "author_question_data = author_question_data[author_question_data.loc[:, 'date_day_bin'].apply(lambda x: type(x) is datetime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69616031\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>date_day_bin</th>\n",
       "      <th>question_subreddit</th>\n",
       "      <th>question_date_day_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DaveAlot</td>\n",
       "      <td>ultrawidemasterrace</td>\n",
       "      <td>1617058469.0</td>\n",
       "      <td>gsqofc7</td>\n",
       "      <td>1617058469.0</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DaveAlot</td>\n",
       "      <td>ultrawidemasterrace</td>\n",
       "      <td>1615528553.0</td>\n",
       "      <td>gqnvkp0</td>\n",
       "      <td>1615528553.0</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaveAlot</td>\n",
       "      <td>ultrawidemasterrace</td>\n",
       "      <td>1614292110.0</td>\n",
       "      <td>gorg61l</td>\n",
       "      <td>1614292110.0</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DaveAlot</td>\n",
       "      <td>ultrawidemasterrace</td>\n",
       "      <td>1614219096.0</td>\n",
       "      <td>gonosj2</td>\n",
       "      <td>1614219096.0</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DaveAlot</td>\n",
       "      <td>tax</td>\n",
       "      <td>1612253656.0</td>\n",
       "      <td>glpizlx</td>\n",
       "      <td>1612253656.0</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author            subreddit   created_utc       id          date         date_day_bin question_subreddit question_date_day_bin\n",
       "0  DaveAlot  ultrawidemasterrace  1617058469.0  gsqofc7  1617058469.0  2019-07-01 00:00:00    personalfinance   2018-01-01 00:00:00\n",
       "1  DaveAlot  ultrawidemasterrace  1615528553.0  gqnvkp0  1615528553.0  2019-07-01 00:00:00    personalfinance   2018-01-01 00:00:00\n",
       "2  DaveAlot  ultrawidemasterrace  1614292110.0  gorg61l  1614292110.0  2019-07-01 00:00:00    personalfinance   2018-01-01 00:00:00\n",
       "3  DaveAlot  ultrawidemasterrace  1614219096.0  gonosj2  1614219096.0  2019-07-01 00:00:00    personalfinance   2018-01-01 00:00:00\n",
       "4  DaveAlot                  tax  1612253656.0  glpizlx  1612253656.0  2019-07-01 00:00:00    personalfinance   2018-01-01 00:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(author_question_data.shape[0])\n",
    "display(author_question_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get \"expert\" distribution\n",
    "# get all posts written before question date\n",
    "# compute % posted in question subreddit UGH\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "def get_subreddit_pct_per_author_per_date(data, start_date):\n",
    "#     print(f'data = \\n{data.head()}')\n",
    "#     print(f'N = {data.shape[0]}')\n",
    "#     print(f'subreddit counts {data.loc[:, \"subreddit\"].value_counts()}')\n",
    "    data.drop_duplicates('id', inplace=True)\n",
    "    valid_data = data[data.loc[:, 'date_day_bin'] <= start_date]\n",
    "    subreddit = data.loc[:, 'question_subreddit'].iloc[0]\n",
    "    subreddit_pct = (data.loc[:, 'subreddit']==subreddit).sum() / data.shape[0]\n",
    "    return subreddit_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** processing data from subreddit=Advice ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27868/27868 [01:25<00:00, 326.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** processing data from subreddit=AmItheAsshole ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2346/62629 [00:16<02:56, 340.77it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "author_subreddit_counts = []\n",
    "for subreddit_i, data_i in author_question_data.groupby(['question_subreddit']):\n",
    "    print(f'**** processing data from subreddit={subreddit_i} ****')\n",
    "    author_subreddit_counts_i = []\n",
    "    for (author_j, date_j), data_j in tqdm(data_i.groupby(['author', 'question_date_day_bin'])):\n",
    "        author_subreddit_counts_j = get_subreddit_pct_per_author_per_date(data_j, data_j.loc[:, 'question_date_day_bin'].iloc[0])\n",
    "        author_subreddit_counts_i.append([author_j, date_j, author_subreddit_counts_j])\n",
    "    author_subreddit_counts_i = pd.DataFrame(author_subreddit_counts_i, columns=['author', 'date', 'subreddit_pct'])\n",
    "    author_subreddit_counts_i = author_subreddit_counts_i.assign(**{'subreddit' : subreddit_i})\n",
    "    author_subreddit_counts.append(author_subreddit_counts_i)\n",
    "author_subreddit_counts = pd.concat(author_subreddit_counts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for subreddit_i, data_i in author_subreddit_counts.groupby('subreddit'):\n",
    "    plt.hist(data_i.loc[:, 'subreddit_pct'].values)\n",
    "    plt.title(f'% posts in subreddit={subreddit_i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the \"expert\" category is very sparse; can we expand this with nearest-neighbor subreddits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "embed_data_dir = '../../data/reddit_data/author_data/'\n",
    "subreddit_embed_file_matcher = re.compile('subreddit_embeddings_.*gz')\n",
    "subreddit_embed_files = list(filter(lambda x: subreddit_embed_file_matcher.match(x) is not None, os.listdir(embed_data_dir)))\n",
    "subreddit_embed_files = list(map(lambda x: os.path.join(embed_data_dir, x), subreddit_embed_files))\n",
    "subreddit_nearest_neighbors = []\n",
    "subreddits_to_query = author_question_data.loc[:, 'question_subreddit'].unique()\n",
    "top_k = 10\n",
    "date_fmt = '%Y-%m-%d'\n",
    "for subreddit_embed_file_i in subreddit_embed_files:\n",
    "    subreddit_embed_date_i = os.path.basename(subreddit_embed_file_i).split('.')[0].split('_')[-1]\n",
    "    subreddit_embed_date_i = datetime.strptime(subreddit_embed_date_i, date_fmt)\n",
    "    subreddit_embed_i = pd.read_csv(subreddit_embed_file_i, sep='\\t', compression='gzip', index_col=0)\n",
    "    for subreddit_j in subreddits_to_query:\n",
    "        # find nearest neighbors\n",
    "        if(subreddit_j in subreddit_embed_i.index):\n",
    "            embed_sim_j = cosine_similarity(subreddit_embed_i.loc[[subreddit_j], :].values,\n",
    "                                            Y=subreddit_embed_i.values)[0, :]\n",
    "            embed_sim_j = pd.Series(embed_sim_j, index=subreddit_embed_i.index).sort_values(ascending=False, inplace=False)\n",
    "            top_k_neighbors_j = embed_sim_j.index.tolist()[1:(top_k+1)]\n",
    "            subreddit_nearest_neighbors.append([subreddit_j, subreddit_embed_date_i, top_k_neighbors_j])\n",
    "subreddit_nearest_neighbors = pd.DataFrame(subreddit_nearest_neighbors, columns=['subreddit', 'date', 'neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advice</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Needafriend, Psychic, needadvice, SuicideWatch, mentalhealth, getdisciplined, Rateme, amiugly, Anxiety, rant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Advice</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>[dating_advice, socialskills, NoFap, AskDocs, Aquariums, SuicideWatch, Austin, confession, runescape, tipofmytongue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Advice</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[answers, Needafriend, MakeNewFriendsHere, lawofattraction, Instagram, NoFap, amiugly, SuicideWatch, medical, ask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Advice</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[MakeNewFriendsHere, SuicideWatch, rant, socialskills, Marriage, ApplyingToCollege, amiugly, Anxiety, solotravel, stopdrinking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[PurplePillDebate, survivor, 6thForm, EntitledBitch, kpop, discgolf, pettyrevenge, BravoRealHousewives, notliketheothergirls, blackladies]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[IsItBullshit, asktransgender, ftm, bisexual, newzealand, pettyrevenge, LetsNotMeet, melbourne, astrology, britishproblems]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[unpopularopinion, TooAfraidToAsk, TrueOffMyChest, offmychest, AMA, rant, AskDocs, TalesFromYourServer, casualiama, changemyview]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legaladvice</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Insurance, legaladviceofftopic, bestoflegaladvice, LawSchool, sanfrancisco, Scams, LegalAdviceUK, ProtectAndServe, Divorce, chicago]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legaladvice</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[legaladvicecanada, Insurance, legaladviceofftopic, LawSchool, lawschooladmissions, polyamory, Landlord, AskHR, Veterans, bestoflegaladvice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>legaladvice</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>[bestoflegaladvice, LawSchool, Aquariums, askcarsales, Accounting, JUSTNOMIL, exmormon, Austin, Seattle, bicycling]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>legaladvice</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[bestoflegaladvice, Insurance, Roadcam, sanfrancisco, homeowners, LawSchool, PersonalFinanceCanada, magicTCG, canada, washingtondc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[watercooling, BulletBarry, overclocking, bapcsalescanada, linuxmasterrace, WorldOfWarships, oculus, Monitors, SuggestALaptop, EliteDangerous]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>[Vive, witcher, nvidia, ultrawidemasterrace, Twitch, totalwar, pcgaming, ClashRoyale, WorldofTanks, KerbalSpaceProgram]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[watercooling, steam_giveaway, EliteDangerous, nvidia, headphones, Steam, playrust, starcitizen, EscapefromTarkov, Battlefield]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[Corsair, buildmeapc, overclocking, watercooling, LogitechG, razer, SatisfactoryGame, simracing, csgo, PUBATTLEGROUNDS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[StudentLoans, CreditCards, churning, tax, whatcarshouldIbuy, DaveRamsey, ynab, AskNYC, Austin, askcarsales]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>[ynab, askcarsales, Accounting, RealEstate, financialindependence, churning, washingtondc, investing, Portland, running]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>[DaveRamsey, churning, CreditCards, StudentLoans, FinancialPlanning, sales, tax, whatcarshouldIbuy, tmobile, ynab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>[tax, churning, washingtondc, ynab, Insurance, smallbusiness, investing, writing, boston, sanfrancisco]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit       date                                                                                                                                       neighbors\n",
       "3            Advice 2019-01-01                                  [Needafriend, Psychic, needadvice, SuicideWatch, mentalhealth, getdisciplined, Rateme, amiugly, Anxiety, rant]\n",
       "8            Advice 2018-01-01                            [dating_advice, socialskills, NoFap, AskDocs, Aquariums, SuicideWatch, Austin, confession, runescape, tipofmytongue]\n",
       "17           Advice 2019-07-01                              [answers, Needafriend, MakeNewFriendsHere, lawofattraction, Instagram, NoFap, amiugly, SuicideWatch, medical, ask]\n",
       "12           Advice 2018-07-01                 [MakeNewFriendsHere, SuicideWatch, rant, socialskills, Marriage, ApplyingToCollege, amiugly, Anxiety, solotravel, stopdrinking]\n",
       "18    AmItheAsshole 2019-07-01      [PurplePillDebate, survivor, 6thForm, EntitledBitch, kpop, discgolf, pettyrevenge, BravoRealHousewives, notliketheothergirls, blackladies]\n",
       "4     AmItheAsshole 2019-01-01                     [IsItBullshit, asktransgender, ftm, bisexual, newzealand, pettyrevenge, LetsNotMeet, melbourne, astrology, britishproblems]\n",
       "13    AmItheAsshole 2018-07-01               [unpopularopinion, TooAfraidToAsk, TrueOffMyChest, offmychest, AMA, rant, AskDocs, TalesFromYourServer, casualiama, changemyview]\n",
       "2       legaladvice 2019-01-01           [Insurance, legaladviceofftopic, bestoflegaladvice, LawSchool, sanfrancisco, Scams, LegalAdviceUK, ProtectAndServe, Divorce, chicago]\n",
       "16      legaladvice 2019-07-01    [legaladvicecanada, Insurance, legaladviceofftopic, LawSchool, lawschooladmissions, polyamory, Landlord, AskHR, Veterans, bestoflegaladvice]\n",
       "7       legaladvice 2018-01-01                             [bestoflegaladvice, LawSchool, Aquariums, askcarsales, Accounting, JUSTNOMIL, exmormon, Austin, Seattle, bicycling]\n",
       "11      legaladvice 2018-07-01             [bestoflegaladvice, Insurance, Roadcam, sanfrancisco, homeowners, LawSchool, PersonalFinanceCanada, magicTCG, canada, washingtondc]\n",
       "1      pcmasterrace 2019-01-01  [watercooling, BulletBarry, overclocking, bapcsalescanada, linuxmasterrace, WorldOfWarships, oculus, Monitors, SuggestALaptop, EliteDangerous]\n",
       "6      pcmasterrace 2018-01-01                         [Vive, witcher, nvidia, ultrawidemasterrace, Twitch, totalwar, pcgaming, ClashRoyale, WorldofTanks, KerbalSpaceProgram]\n",
       "10     pcmasterrace 2018-07-01                 [watercooling, steam_giveaway, EliteDangerous, nvidia, headphones, Steam, playrust, starcitizen, EscapefromTarkov, Battlefield]\n",
       "15     pcmasterrace 2019-07-01                         [Corsair, buildmeapc, overclocking, watercooling, LogitechG, razer, SatisfactoryGame, simracing, csgo, PUBATTLEGROUNDS]\n",
       "0   personalfinance 2019-01-01                                    [StudentLoans, CreditCards, churning, tax, whatcarshouldIbuy, DaveRamsey, ynab, AskNYC, Austin, askcarsales]\n",
       "5   personalfinance 2018-01-01                        [ynab, askcarsales, Accounting, RealEstate, financialindependence, churning, washingtondc, investing, Portland, running]\n",
       "14  personalfinance 2019-07-01                              [DaveRamsey, churning, CreditCards, StudentLoans, FinancialPlanning, sales, tax, whatcarshouldIbuy, tmobile, ynab]\n",
       "9   personalfinance 2018-07-01                                         [tax, churning, washingtondc, ynab, Insurance, smallbusiness, investing, writing, boston, sanfrancisco]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "subreddit_nearest_neighbors.sort_values('subreddit', inplace=True)\n",
    "display(subreddit_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Advice': {'socialskills', 'mentalhealth', 'needadvice', 'ask', 'answers', 'tipofmytongue', 'getdisciplined', 'AskDocs', 'dating_advice'}, 'AmItheAsshole': {'IsItBullshit', 'AskDocs', 'TooAfraidToAsk'}, 'legaladvice': {'AskHR', 'LawSchool', 'LegalAdviceUK', 'Insurance', 'bestoflegaladvice', 'Scams', 'Landlord'}, 'pcmasterrace': {'SuggestALaptop', 'buildmeapc', 'overclocking', 'watercooling', 'linuxmasterrace', 'pcgaming', 'Monitors', 'bapcsalescanada'}, 'personalfinance': {'smallbusiness', 'askcarsales', 'ynab', 'FinancialPlanning', 'churning', 'RealEstate', 'whatcarshouldIbuy', 'StudentLoans', 'CreditCards', 'investing', 'financialindependence', 'Accounting', 'tax'}}\n"
     ]
    }
   ],
   "source": [
    "## filtered data\n",
    "valid_subreddits = {\n",
    "    'Advice' : {'needadvice', 'mentalhealth', 'getdisciplined', 'dating_advice', 'socialskills', 'AskDocs', 'tipofmytongue', 'answers', 'ask'},\n",
    "    'AmItheAsshole' : {'IsItBullshit', 'TooAfraidToAsk', 'AskDocs'},\n",
    "    'legaladvice' : {'Insurance', 'bestoflegaladvice', 'LawSchool', 'Scams', 'LegalAdviceUK', 'Landlord', 'AskHR'},\n",
    "    'pcmasterrace' : {'watercooling', 'overclocking', 'bapcsalescanada', 'linuxmasterrace', 'Monitors', 'SuggestALaptop', 'pcgaming', 'buildmeapc'},\n",
    "    'personalfinance' : {'StudentLoans', 'CreditCards', 'churning', 'tax', 'whatcarshouldIbuy', 'ynab', 'askcarsales', 'Accounting', 'RealEstate', 'financialindependence', 'investing', 'FinancialPlanning', 'smallbusiness'},\n",
    "}\n",
    "from functools import reduce\n",
    "subreddit_nearest_neighbors = subreddit_nearest_neighbors.assign(**{\n",
    "    'valid_neighbors' : subreddit_nearest_neighbors.apply(lambda x: set(x.loc['neighbors']) & valid_subreddits[x.loc['subreddit']], axis=1)\n",
    "})\n",
    "## combine into one set per subreddit\n",
    "subreddit_combined_neighbors = subreddit_nearest_neighbors.groupby('subreddit').apply(lambda x: set(reduce(lambda a,b: a|b, x.loc[:, 'valid_neighbors'].values))).to_dict()\n",
    "print(subreddit_combined_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the nearest neighbors, let's re-run the \"expert\" tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_neighbors_pct_per_author_per_date(data, start_date, neighbors):\n",
    "    data.drop_duplicates('id', inplace=True)\n",
    "    valid_data = data[data.loc[:, 'date_day_bin'] <= start_date]\n",
    "    subreddit = data.loc[:, 'question_subreddit'].iloc[0]\n",
    "    subreddit_neighbors = neighbors[subreddit]\n",
    "    subreddit_pct = (data.loc[:, 'subreddit'].apply(lambda x: x==subreddit or x in subreddit_neighbors)).sum() / data.shape[0]\n",
    "    return subreddit_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "author_subreddit_counts = []\n",
    "for subreddit_i, data_i in author_question_data.groupby(['question_subreddit']):\n",
    "    print(f'**** processing data from subreddit={subreddit_i} ****')\n",
    "    author_subreddit_counts_i = []\n",
    "    for (author_j, date_j), data_j in tqdm(data_i.groupby(['author', 'question_date_day_bin'])):\n",
    "        author_subreddit_counts_j = get_subreddit_neighbors_pct_per_author_per_date(data_j, data_j.loc[:, 'question_date_day_bin'].iloc[0], subreddit_combined_neighbors)\n",
    "        author_subreddit_counts_i.append([author_j, date_j, author_subreddit_counts_j])\n",
    "    author_subreddit_counts_i = pd.DataFrame(author_subreddit_counts_i, columns=['author', 'date', 'subreddit_pct'])\n",
    "    author_subreddit_counts_i = author_subreddit_counts_i.assign(**{'subreddit' : subreddit_i})\n",
    "    author_subreddit_counts.append(author_subreddit_counts_i)\n",
    "author_subreddit_counts = pd.concat(author_subreddit_counts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for subreddit_i, data_i in author_subreddit_counts.groupby('subreddit'):\n",
    "    plt.hist(data_i.loc[:, 'subreddit_pct'].values)\n",
    "    plt.title(f'% posts in subreddit={subreddit_i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand location\n",
    "We have previously identified an author's location based on whether they disclose their location in prior comments (\"I live in ___\").\n",
    "\n",
    "Let's expand this by looking at location-specific subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## naive approach: identify all possible locations in subreddit names by geolocation lol\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

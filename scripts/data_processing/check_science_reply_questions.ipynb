{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check science reply questions\n",
    "Let's look at some data related to sharing new science articles and the questions that people pose in response to the articles.\n",
    "\n",
    "We'll see how readily we can predict author background using the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "1001it [00:05, 286.60it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "47091it [07:52, 99.66it/s] \n"
     ]
    }
   ],
   "source": [
    "## get Reddit data!!\n",
    "from datetime import datetime\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm\n",
    "# from data_helpers import load_reddit_api\n",
    "# reddit_api, pushshift_api = load_reddit_api('../../data/auth_data/reddit_auth.csv')\n",
    "pushshift_api = PushshiftAPI()\n",
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['url', 'title', 'author', 'score', 'text', 'created_utc', 'id', 'upvote_ratio', 'num_comments']\n",
    "submissions = pushshift_api.search_submissions(q=\"*\", after=date_range[0], before=date_range[1],\n",
    "                                               subreddit=subreddit, filter=filter_fields)\n",
    "submissions_results = []\n",
    "for s in tqdm(submissions):\n",
    "    submissions_results.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>d_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talismanbrandi</td>\n",
       "      <td>1630462648</td>\n",
       "      <td>pfkdt5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Socio-economic disparities and COVID-19 in the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/pfkd...</td>\n",
       "      <td>1630480648.0</td>\n",
       "      <td>{'author': 'talismanbrandi', 'created_utc': 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BeforeYourBBQ</td>\n",
       "      <td>1630462436</td>\n",
       "      <td>pfkbn2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing SARS-CoV-2 natural immunity to vacci...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.medrxiv.org/content/10.1101/2021.0...</td>\n",
       "      <td>1630480436.0</td>\n",
       "      <td>{'author': 'BeforeYourBBQ', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462250</td>\n",
       "      <td>pfk9q9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Scientists Figured Out How Much Exercise You N...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.sciencealert.com/scientists-figure...</td>\n",
       "      <td>1630480250.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462179</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "      <td>Female octopuses throw shells at males annoyin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.independent.co.uk/climate-change/n...</td>\n",
       "      <td>1630480179.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doozenburg</td>\n",
       "      <td>1630461660</td>\n",
       "      <td>pfk3ts</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Who is Anti-Vax Dr. Wendy Menigoz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.slugbrain.com/post/who-is-anti-vax...</td>\n",
       "      <td>1630479660.0</td>\n",
       "      <td>{'author': 'Doozenburg', 'created_utc': 163046...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  created_utc  ...       created                                                 d_\n",
       "0  talismanbrandi   1630462648  ...  1630480648.0  {'author': 'talismanbrandi', 'created_utc': 16...\n",
       "1   BeforeYourBBQ   1630462436  ...  1630480436.0  {'author': 'BeforeYourBBQ', 'created_utc': 163...\n",
       "2   key__lime_pie   1630462250  ...  1630480250.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "3   key__lime_pie   1630462179  ...  1630480179.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "4      Doozenburg   1630461660  ...  1630479660.0  {'author': 'Doozenburg', 'created_utc': 163046...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## convert to data frame\n",
    "import pandas as pd\n",
    "submission_data = pd.DataFrame(submissions_results)\n",
    "display(submission_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect all comments from the same time frame, and align them to submissions afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "901it [00:03, 289.96it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "18800it [03:16, 23.04it/s] /home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "907905it [2:33:53, 83.42it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "1445247it [4:03:56, 98.15it/s] "
     ]
    }
   ],
   "source": [
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['id', 'link_id', 'parent_id', 'body', 'author', 'created_utc', 'score']\n",
    "comments = pushshift_api.search_comments(after=date_range[0], before=date_range[1],\n",
    "                                         subreddit=subreddit, filter=filter_fields)\n",
    "comments_results = []\n",
    "for c in tqdm(comments):\n",
    "    comments_results.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Etna</td>\n",
       "      <td>oh I misread, it's the males that are being an...</td>\n",
       "      <td>1630468787</td>\n",
       "      <td>hb5b6ss</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>27</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lalauna</td>\n",
       "      <td>Please tell me something i didn't know before.</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zz</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DivineBanana</td>\n",
       "      <td>I haven't followed the debate but I'm sure it'...</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zy</td>\n",
       "      <td>pf5phr</td>\n",
       "      <td>hb58igp</td>\n",
       "      <td>3</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_MASTADONG_</td>\n",
       "      <td>I just linked you to 2 articles on fact checki...</td>\n",
       "      <td>1630468723</td>\n",
       "      <td>hb5b2sg</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>hb5adks</td>\n",
       "      <td>4</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Etna</td>\n",
       "      <td>It's because they can't talk</td>\n",
       "      <td>1630468713</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>37</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               body  created_utc       id link_id parent_id  score       created\n",
       "0          Etna  oh I misread, it's the males that are being an...   1630468787  hb5b6ss  pfk90j   hb5b25v     27  1.630487e+09\n",
       "3       lalauna     Please tell me something i didn't know before.   1630468726  hb5b2zz  pfgvrw    pfgvrw      1  1.630487e+09\n",
       "4  DivineBanana  I haven't followed the debate but I'm sure it'...   1630468726  hb5b2zy  pf5phr   hb58igp      3  1.630487e+09\n",
       "5   _MASTADONG_  I just linked you to 2 articles on fact checki...   1630468723  hb5b2sg  pfgvrw   hb5adks      4  1.630487e+09\n",
       "8          Etna                       It's because they can't talk   1630468713  hb5b25v  pfk90j    pfk90j     37  1.630487e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196985\n"
     ]
    }
   ],
   "source": [
    "## combine/clean\n",
    "comment_data = pd.DataFrame(comments_results)\n",
    "# drop deleted data\n",
    "comment_data = comment_data[(comment_data.loc[:, 'author']!='[deleted]') &\n",
    "                            (comment_data.loc[:, 'body']!='[deleted]')]\n",
    "# fix ID vars\n",
    "comment_data = comment_data.assign(**{\n",
    "    'link_id' : comment_data.loc[:, 'link_id'].apply(lambda x: x.split('_')[1]),\n",
    "    'parent_id' : comment_data.loc[:, 'parent_id'].apply(lambda x: x.split('_')[1]),\n",
    "})\n",
    "# drop extra data\n",
    "comment_data.drop('d_', axis=1, inplace=True)\n",
    "display(comment_data.head())\n",
    "print(comment_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict to comment/submission matches\n",
    "submission_comment_data = pd.merge(submission_data, comment_data, left_on='id', right_on='link_id', how='inner')\n",
    "# fix col names\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_x', '_submission') \n",
    "    for x in list(filter(lambda x: x.endswith('_x'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_y', '_comment') \n",
    "    for x in list(filter(lambda x: x.endswith('_y'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data = submission_comment_data[submission_comment_data.loc[:, 'link_id']==submission_comment_data.loc[:, 'parent_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465354"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_comment_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130878 questions total\n"
     ]
    }
   ],
   "source": [
    "## clean text\n",
    "import re\n",
    "RETURN_MATCHER = re.compile('[\\n\\r]')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'body' : submission_comment_data.loc[:, 'body'].apply(lambda x: RETURN_MATCHER.sub(' ', x))\n",
    "})\n",
    "## filter questions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_sents' : submission_comment_data.loc[:, 'body'].apply(lambda x: sent_tokenize(x))\n",
    "})\n",
    "# look for questions!\n",
    "import re\n",
    "question_matcher = re.compile('\\?$')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_questions' : submission_comment_data.loc[:, 'reply_sents'].apply(lambda x: list(filter(lambda y: question_matcher.search(y) is not None, x)))\n",
    "})\n",
    "submission_question_data = submission_comment_data[submission_comment_data.loc[:, 'reply_questions'].apply(len)>0]\n",
    "## flatten\n",
    "flat_submission_question_data = []\n",
    "for idx_i, data_i in submission_question_data.iterrows():\n",
    "    for q_j in data_i.loc['reply_questions']:\n",
    "        data_j = data_i.copy().drop('reply_questions')\n",
    "        data_j.loc['reply_question'] = q_j\n",
    "        flat_submission_question_data.append(data_j)\n",
    "flat_submission_question_data = pd.concat(flat_submission_question_data, axis=1).transpose()\n",
    "print(f'{flat_submission_question_data.shape[0]} questions total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['die for the economy?', 'how about we topple you instead?',\n",
       "       'Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Have I misread something?',\n",
       "       \"Don't you care about the environment?\"], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## look at sample questions => clarification questions? self-contained? related to post?\n",
    "## sample questions\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(flat_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit the questions to have at least X words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Wait, who was saying there would be a pandemic baby boom and why?',\n",
       "       \"Isn't it established that stress and uncertainty eliminate the desire to be parents, or is that just my intuition?\",\n",
       "       'The baby boom thing was just a joke right?',\n",
       "       'Was there any real feeling that that would happen?'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "min_question_words = 10\n",
    "tokenizer = WordPunctTokenizer()\n",
    "valid_submission_question_data = flat_submission_question_data[flat_submission_question_data.loc[:, 'reply_question'].apply(lambda x: len(tokenizer.tokenize(x)) >= min_question_words)]\n",
    "display(valid_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for posterity!!\n",
    "valid_submission_question_data.to_csv('science_submission_question_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "2it [00:03,  2.19s/it]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "3099it [6:11:34,  6.67s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10000it [20:27:42,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## TODO: mine previous history for N=10000 commenters; extract location + age + gender (?)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from psaw import PushshiftAPI\n",
    "pushshift_api = PushshiftAPI()\n",
    "N_commenters = 10000\n",
    "N_previous_comments = 1000\n",
    "sample_question_author_data = valid_submission_question_data.sort_values(['author_comment', 'created_utc_comment'], ascending=True).drop_duplicates('author_comment').loc[:, ['author_comment', 'created_utc_comment']]\n",
    "sample_question_author_data = sample_question_author_data.sample(N_commenters, replace=False, random_state=123)\n",
    "sample_question_author_prior_data = []\n",
    "author_filter_cols = ['body', 'id', 'created_utc', 'author', 'subreddit']\n",
    "for idx_i, data_i in tqdm(sample_question_author_data.iterrows()):\n",
    "    author_i = data_i.loc['author_comment']\n",
    "    time_i = int(data_i.loc['created_utc_comment'])\n",
    "    prior_comments_i = list(pushshift_api.search_comments(author=author_i, limit=N_previous_comments, before=time_i, filter=author_filter_cols))\n",
    "    prior_comments_i = pd.DataFrame(prior_comments_i)\n",
    "    if('d_' in prior_comments_i.columns):\n",
    "        prior_comments_i.drop('d_', axis=1, inplace=True)\n",
    "    sample_question_author_prior_data.append(prior_comments_i)\n",
    "sample_question_author_prior_data = pd.concat(sample_question_author_prior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_question_author_prior_data.to_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (0,1,2,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data = pd.read_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data.dropna(subset=['body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>You're not even Scottish, are you?</td>\n",
       "      <td>1582310896</td>\n",
       "      <td>fiayfoz</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582329e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.</td>\n",
       "      <td>1582298322</td>\n",
       "      <td>fiacrvb</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582316e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.</td>\n",
       "      <td>1582291726</td>\n",
       "      <td>fia35ak</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582310e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...</td>\n",
       "      <td>1582258926</td>\n",
       "      <td>fi99f50</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582277e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog</td>\n",
       "      <td>1582258153</td>\n",
       "      <td>fi98eh0</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582276e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  \\\n",
       "0  mahogany555   \n",
       "1  mahogany555   \n",
       "2  mahogany555   \n",
       "3  mahogany555   \n",
       "4  mahogany555   \n",
       "\n",
       "                                                                                                                                                                                                                                              body  \\\n",
       "0                                                                                                                                                                                                               You're not even Scottish, are you?   \n",
       "1                                                                                                                                                         'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.   \n",
       "2  Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.   \n",
       "3                                                                                                                   No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...   \n",
       "4                                                                                                             Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog   \n",
       "\n",
       "  created_utc       id              subreddit       created  \n",
       "0  1582310896  fiayfoz  ScottishPeopleTwitter  1.582329e+09  \n",
       "1  1582298322  fiacrvb  ScottishPeopleTwitter  1.582316e+09  \n",
       "2  1582291726  fia35ak  ScottishPeopleTwitter  1.582310e+09  \n",
       "3  1582258926  fi99f50  ScottishPeopleTwitter  1.582277e+09  \n",
       "4  1582258153  fi98eh0  ScottishPeopleTwitter  1.582276e+09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(sample_question_author_prior_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='I live'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'416971/6804000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## check for self-disclosure statements\n",
    "import re\n",
    "self_disclosure_matcher = re.compile(f'(I\\'m|I am|I live)')\n",
    "print(self_disclosure_matcher.search('I live in MI'))\n",
    "sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "    'body_contain_self_disclosure' : sample_question_author_prior_data.loc[:, 'body'].apply(lambda x: self_disclosure_matcher.search(x) is not None)\n",
    "})\n",
    "display(f'{sample_question_author_prior_data.loc[:, \"body_contain_self_disclosure\"].sum()}/{sample_question_author_prior_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPULA_LEMMA = 'be'\n",
    "EXIST_LEMMA = 'live'\n",
    "GENDER_MATCHER = re.compile('^(man|woman|male|female)$')\n",
    "AGE_NUM_MATCHER = re.compile('[0-9]+')\n",
    "def collect_propn(token):\n",
    "    loc_noun_parts = [token]\n",
    "    location_children = list(filter(lambda x: x.pos_=='PROPN', token.children))\n",
    "    while(len(location_children) > 0):\n",
    "        loc_noun_part = location_children.pop()\n",
    "        loc_noun_parts.append(loc_noun_part)\n",
    "        location_children += list(filter(lambda x: x.pos_=='PROPN', loc_noun_part.children))\n",
    "    # sort noun parts\n",
    "    loc_noun_parts = list(sorted(loc_noun_parts, key=lambda x: x.idx))\n",
    "    loc_noun = ' '.join(list(map(lambda x: x.lemma_, loc_noun_parts)))\n",
    "    return loc_noun\n",
    "def extract_self_statement_targets(sent, pipeline=None):\n",
    "    if(pipeline is not None):\n",
    "        parse = pipeline(sent)\n",
    "    else:\n",
    "        parse = sent\n",
    "    identity_attributes = []\n",
    "    parse_sents = list(parse.doc.sents)\n",
    "    for parse_sent in parse_sents:\n",
    "        for token in parse_sent:\n",
    "            # get children nouns for \"I\" via root\n",
    "            if(token.lemma_ == 'I' and token.dep_ == 'nsubj'):\n",
    "                token_root_ancestors = list(filter(lambda x: x.dep_=='ROOT', token.ancestors))\n",
    "    #             print(f'parse ents = {list(map(lambda x: x.label_, parse.ents))}')\n",
    "                loc_ents = list(filter(lambda x: x.label_=='GPE', parse.ents))\n",
    "                if(len(token_root_ancestors) > 0):\n",
    "                    token_root = token_root_ancestors[0]\n",
    "                    root_children = list(token_root.children)\n",
    "                    if(token_root.lemma_ == COPULA_LEMMA):\n",
    "                        for child in root_children:\n",
    "                            if(child.dep_ == 'attr'):\n",
    "                                # gender\n",
    "                                gender_match = GENDER_MATCHER.match(child.lemma_)\n",
    "                                if(gender_match is not None):\n",
    "                                    identity_attributes.append(['gender', gender_match.group(0)])\n",
    "                                # age => NOPE false positives abound\n",
    "#                                 age_match = AGE_NUM_MATCHER.match(child.lemma_)\n",
    "#                                 if(age_match is not None):\n",
    "#                                     identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # age\n",
    "                            elif(child.dep_ == 'acomp'):\n",
    "                                if(child.lemma_ == 'old'):\n",
    "                                    # look for children (\"30 years old\")\n",
    "                                    age_children_1 = list(child.children)\n",
    "                                    if(len(age_children_1) > 0 and age_children_1[0].lemma_=='year'):\n",
    "                                        age_children_2 = list(age_children_1[0].children)\n",
    "                                        if(len(age_children_2) > 0):\n",
    "                                            age_match = AGE_NUM_MATCHER.match(age_children_2[0].lemma_)\n",
    "                                            if(age_match is not None):\n",
    "                                                identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # location\n",
    "                            elif(child.dep_ == 'prep' and child.lemma_ == 'from'):\n",
    "                                location_children_1 = list(child.children)\n",
    "                                if(len(location_children_1) > 0 and location_children_1[0].pos_ == 'PROPN'):\n",
    "                                    ent_start = location_children_1[0].i\n",
    "    #                                 child_1_idx = location_children_1[0].idx\n",
    "    #                                 child_1_ent = \n",
    "    #                                 main_loc = location_children_1[0]\n",
    "    #                                 loc_noun = collect_propn(main_loc)\n",
    "                                    # find ENT that contains child\n",
    "    #                                 print(f'ent start = {ent_start}')\n",
    "    #                                 print(f'{[(x.start, x.end) for x in loc_ents]}')\n",
    "                                    containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "    #                                 print(f'containing loc ents {containing_loc_ents}')\n",
    "                                    if(len(containing_loc_ents) > 0):\n",
    "                                        loc_noun = containing_loc_ents[0].text\n",
    "                                        identity_attributes.append(['location', loc_noun])\n",
    "                    # \"I live in the US\"\n",
    "                    elif(token_root.lemma_ == EXIST_LEMMA):\n",
    "                        root_prep_children = list(filter(lambda x: x.lemma_=='in' and x.dep_=='prep', token_root.children))\n",
    "                        if(len(root_prep_children) > 0):\n",
    "                            prep_children_2 = list(filter(lambda x: x.pos_ == 'PROPN', root_prep_children[0].children))\n",
    "                            if(len(prep_children_2) > 0):\n",
    "                                ent_start = prep_children_2[0].i\n",
    "                                containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "                                if(len(containing_loc_ents)):\n",
    "                                    loc_noun = containing_loc_ents[0].text\n",
    "                                    identity_attributes.append(['location', loc_noun])\n",
    "    return identity_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent = I am 30 years old has attr [['age', '30']]\n",
      "sent = I am a woman has attr [['gender', 'woman']]\n",
      "sent = I live in London, England has attr [['location', 'London']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# dumb\n",
    "# self_statement = '(I\\'m|I am)'\n",
    "# age_matcher = f'({self_statement} [0-9]+ years old)|({self_statement} a [0-9]+ yo )'\n",
    "# gender_matcher = f'({self_statement} a[ a-zA-Z]? (man|woman|male|female))'\n",
    "# smart => parse then extract\n",
    "import spacy\n",
    "nlp_pipeline = spacy.load('en_core_web_sm')\n",
    "# gender_dep = attr (\"I'm a man\")\n",
    "# age_dep = acomp (\"I'm 50 years old\")\n",
    "# loc_dep = prep (\"I live in Michigan\")\n",
    "test_sents = [\n",
    "    'I am 30 years old',\n",
    "    'I am a woman',\n",
    "    'I live in London, England',\n",
    "]\n",
    "for sent in test_sents:\n",
    "    sent_attr = extract_self_statement_targets(sent, nlp_pipeline)\n",
    "    print(f'sent = {sent} has attr {sent_attr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have an extremely brittle attribute extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nope\n",
    "# ## get parses first\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# from pandarallel import pandarallel\n",
    "# # pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "#     'body_parse' : list(tqdm(nlp_pipeline.pipe(sample_question_author_prior_data.loc[:, 'body'].values, batch_size=1000)))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parallelized pipeline\n",
    "# https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html#Option-3:-Parallelize-the-work-using-joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "N_JOBS=7\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "def process_chunk(texts):\n",
    "    proc_results = []\n",
    "    for doc in nlp_pipeline.pipe(texts, batch_size=20):\n",
    "#         preproc_pipe.append(lemmatize_pipe(doc))\n",
    "        proc_results.append(doc)\n",
    "    return proc_results\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=N_JOBS, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in tqdm(chunker(texts, len(texts), chunksize=chunksize)))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [18:22,  2.04s/it]"
     ]
    }
   ],
   "source": [
    "# takes too much memory!!\n",
    "# body_parse = preprocess_parallel(sample_question_author_prior_data.loc[:, 'body'].values, chunksize=1000)\n",
    "# print(body_parse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "askmen                   16907\n",
       "askwomen                  3898\n",
       "mensrights                2745\n",
       "menwritingwomen           1279\n",
       "menslib                    903\n",
       "whereareallthegoodmen      583\n",
       "mentalhealth               513\n",
       "watchmen                   453\n",
       "menopause                  444\n",
       "mensa                      299\n",
       "xmen                       291\n",
       "redpillwomen               274\n",
       "mendrawingwomen            230\n",
       "ramen                      208\n",
       "askgaymen                  204\n",
       "armoredwomen               187\n",
       "madmen                     150\n",
       "adhdwomen                  137\n",
       "mentalillness              123\n",
       "menkampf                   103\n",
       "women                       99\n",
       "menshealth                  59\n",
       "men                         55\n",
       "autisminwomen               50\n",
       "menieres                    48\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "askwomen            3898\n",
       "menwritingwomen     1279\n",
       "redpillwomen         274\n",
       "mendrawingwomen      230\n",
       "armoredwomen         187\n",
       "adhdwomen            137\n",
       "womenshealth         109\n",
       "women                 99\n",
       "autisminwomen         50\n",
       "womenofcolor          20\n",
       "prettyolderwomen      17\n",
       "womensstreetwear      17\n",
       "justhotwomen          16\n",
       "womensrightsnews      14\n",
       "bluecollarwomen       11\n",
       "womenengineers        10\n",
       "womenwhodontsell       9\n",
       "darkestwomen           8\n",
       "whipped_women          8\n",
       "womenwritingmen        7\n",
       "womenbendingover       7\n",
       "actualwomen            5\n",
       "womensfashion          5\n",
       "womenstyleadvice       4\n",
       "womenofcolorxxx        4\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nyc           3219\n",
       "losangeles    3279\n",
       "chicago       4038\n",
       "houston       2261\n",
       "phoenix        714\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "oldschoolcool          15862\n",
       "oldpeoplefacebook        446\n",
       "oldphotosinreallife      364\n",
       "youngpeopleyoutube       356\n",
       "gold                     293\n",
       "old_recipes              266\n",
       "galaxyfold               184\n",
       "goforgold                180\n",
       "oldschoolhot             173\n",
       "negativewithgold         141\n",
       "fuckimold                121\n",
       "oldmandog                117\n",
       "youngjustice             116\n",
       "oldschoolcoolnsfw        110\n",
       "oldfreefolk              107\n",
       "cuckold                  106\n",
       "gayyoungold              105\n",
       "mold                      94\n",
       "oldschoolcelebs           84\n",
       "freezingfuckingcold       64\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show \"men\"/\"women\" subreddits\n",
    "subreddit_counts = sample_question_author_prior_data.loc[:, 'subreddit'].value_counts()\n",
    "subreddit_counts.index = list(map(lambda x: str(x.lower()), subreddit_counts.index))\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^men)|(men$)', x) is not None, subreddit_counts.index))].sort_values(ascending=False).head(25))\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^women)|(women$)', x) is not None, subreddit_counts.index))].sort_values(ascending=False).head(25))\n",
    "# show location subreddits\n",
    "display(subreddit_counts.loc[['nyc', 'losangeles', 'chicago', 'houston', 'phoenix']])\n",
    "# show age subreddits\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^old)|(old$)|(^young)|(young$)', x), subreddit_counts.index))].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416971/416971 [1:55:00<00:00, 60.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "## extract all attributes!!\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "out_file_name = 'science_sample_reply_author_attrs.gz'\n",
    "id_cols = ['author', 'created_utc']\n",
    "self_disclosure_author_prior_data = sample_question_author_prior_data[sample_question_author_prior_data.loc[:, 'body_contain_self_disclosure']]\n",
    "self_disclosure_author_prior_data = self_disclosure_author_prior_data.assign(**{\n",
    "    'id_attrs' : self_disclosure_author_prior_data.loc[:, 'body'].progress_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline))\n",
    "})\n",
    "# pandarallel => memory isssues\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "#     'identity_attributes' : sample_question_author_prior_data.loc[:, 'body'].parallel_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline)),\n",
    "# #     'identity_attributes' : sample_question_author_prior_data.loc[:, 'body_parse'].progress_apply(lambda x: extract_self_statement_targets(x))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3292/3292 [00:24<00:00, 134.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-BluBone-</td>\n",
       "      <td>man</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Cerberus</td>\n",
       "      <td>man</td>\n",
       "      <td>41</td>\n",
       "      <td>KC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Daetrax-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Negative-Karma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Nycter-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author gender  age      location\n",
       "0        -BluBone-    man  NaN           NaN\n",
       "0        -Cerberus    man   41            KC\n",
       "0        -Daetrax-    NaN  NaN       Denmark\n",
       "0  -Negative-Karma    NaN  NaN        Kansas\n",
       "0         -Nycter-    NaN  NaN  saudi arabia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_author_attribute_data = self_disclosure_author_prior_data[self_disclosure_author_prior_data.loc[:, 'id_attrs'].apply(lambda x: len(x) > 0)]\n",
    "# get one line per author\n",
    "flat_author_attribute_data = []\n",
    "for author_i, data_i in tqdm(question_author_attribute_data.groupby('author')):\n",
    "    attr_data_i = []\n",
    "    for idx_j, data_j in data_i.iterrows():\n",
    "        # keep track of dates!!\n",
    "        date_j = data_j.loc['created_utc']\n",
    "        for attr_k, val_k in data_j.loc['id_attrs']:\n",
    "            attr_data_i.append({\n",
    "                'author' : author_i,\n",
    "                'date' : date_j,\n",
    "                'attr' : attr_k,\n",
    "                'val' : val_k\n",
    "            })\n",
    "    attr_data_i = pd.DataFrame(attr_data_i)\n",
    "    attr_data_i.sort_values(['attr', 'date'], inplace=True, ascending=False)\n",
    "    attr_data_i = attr_data_i.drop_duplicates(['attr'], keep='first').drop('date', axis=1)    \n",
    "    attr_data_i = attr_data_i.pivot(index='author', columns=['attr'], values=['val']).reset_index()\n",
    "    attr_data_i.columns = list(map(lambda x: x[0] if x[1]=='' else x[1], attr_data_i.columns))\n",
    "    flat_author_attribute_data.append(attr_data_i)\n",
    "flat_author_attribute_data = pd.concat(flat_author_attribute_data, axis=0)\n",
    "display(flat_author_attribute_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US            150\n",
      "Canada        100\n",
      "UK             81\n",
      "Texas          79\n",
      "California     62\n",
      "Florida        52\n",
      "America        49\n",
      "Australia      39\n",
      "Germany        35\n",
      "USA            34\n",
      "Name: location, dtype: int64\n",
      "man       789\n",
      "woman     275\n",
      "male      184\n",
      "female     48\n",
      "Name: gender, dtype: int64\n",
      "30    19\n",
      "20    11\n",
      "40    11\n",
      "33    10\n",
      "5     10\n",
      "31     9\n",
      "10     9\n",
      "50     8\n",
      "36     6\n",
      "19     6\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_attr = ['location', 'gender', 'age']\n",
    "for attr_i in all_attr:\n",
    "    print(flat_author_attribute_data.loc[:, attr_i].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This isn't the best but we'll see what we can do with the aggregate categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median age = 30.000\n"
     ]
    }
   ],
   "source": [
    "# fix age\n",
    "import numpy as np\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'age' : flat_author_attribute_data.loc[:, 'age'].apply(lambda x: x if type(x) is float and np.isnan(x) else int(x))\n",
    "})\n",
    "max_age = 100\n",
    "flat_author_attribute_data = flat_author_attribute_data[flat_author_attribute_data.loc[:, 'age'].apply(lambda x: np.isnan(x) or x <= max_age)]\n",
    "## plot age distribution\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "age_vals = flat_author_attribute_data.loc[:, 'age'].dropna().astype(int)\n",
    "plt.hist(age_vals)\n",
    "plt.show()\n",
    "print(f'median age = {age_vals.median():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>0.806806</td>\n",
       "      <td>dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>0.826542</td>\n",
       "      <td>us</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saudi arabia</td>\n",
       "      <td>0.735263</td>\n",
       "      <td>sa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.841632</td>\n",
       "      <td>se</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Italy</td>\n",
       "      <td>0.883102</td>\n",
       "      <td>it</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0.800803</td>\n",
       "      <td>us</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California</td>\n",
       "      <td>0.922136</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Canada</td>\n",
       "      <td>0.976126</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>0.765295</td>\n",
       "      <td>us</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>0.810392</td>\n",
       "      <td>us</td>\n",
       "      <td>Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Riverside</td>\n",
       "      <td>0.673493</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finland</td>\n",
       "      <td>0.907328</td>\n",
       "      <td>fi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Orange County</td>\n",
       "      <td>0.793026</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NZ</td>\n",
       "      <td>0.791711</td>\n",
       "      <td>nz</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DC</td>\n",
       "      <td>0.749289</td>\n",
       "      <td>us</td>\n",
       "      <td>District of Columbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>0.786380</td>\n",
       "      <td>us</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Grand Rapids</td>\n",
       "      <td>0.765069</td>\n",
       "      <td>us</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>US</td>\n",
       "      <td>0.935691</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alberta</td>\n",
       "      <td>0.782431</td>\n",
       "      <td>ca</td>\n",
       "      <td>Alberta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         location  accuracy country_code                 state\n",
       "2         Denmark  0.806806           dk                   NaN\n",
       "3          Kansas  0.826542           us                Kansas\n",
       "4    saudi arabia  0.735263           sa                   NaN\n",
       "5         Germany  0.889681           de                   NaN\n",
       "6          Sweden  0.841632           se                   NaN\n",
       "7           Italy  0.883102           it                   NaN\n",
       "8         Atlanta  0.800803           us               Georgia\n",
       "9      California  0.922136           us            California\n",
       "10         Canada  0.976126           ca                   NaN\n",
       "11      Baltimore  0.765295           us              Maryland\n",
       "12    Mississippi  0.810392           us           Mississippi\n",
       "13      Riverside  0.673493           us            California\n",
       "15        Finland  0.907328           fi                   NaN\n",
       "16  Orange County  0.793026           us            California\n",
       "17             NZ  0.791711           nz                   NaN\n",
       "18             DC  0.749289           us  District of Columbia\n",
       "19       Brooklyn  0.786380           us              New York\n",
       "20   Grand Rapids  0.765069           us              Michigan\n",
       "21             US  0.935691           us                   NaN\n",
       "22        Alberta  0.782431           ca               Alberta"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "us    259\n",
       "gb     31\n",
       "ca     29\n",
       "cn     10\n",
       "au     10\n",
       "it      9\n",
       "mx      6\n",
       "nl      6\n",
       "de      6\n",
       "co      5\n",
       "jp      5\n",
       "in      4\n",
       "es      4\n",
       "fr      4\n",
       "ch      4\n",
       "th      3\n",
       "kr      3\n",
       "br      3\n",
       "be      3\n",
       "ar      3\n",
       "Name: country_code, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "California          32\n",
       "Texas               25\n",
       "England             21\n",
       "New York            17\n",
       "Florida             11\n",
       "Illinois            11\n",
       "Washington           9\n",
       "Ohio                 8\n",
       "Pennsylvania         8\n",
       "Ontario              7\n",
       "British Columbia     6\n",
       "Virginia             6\n",
       "Michigan             6\n",
       "North Dakota         5\n",
       "Wisconsin            5\n",
       "North Carolina       5\n",
       "Québec               5\n",
       "Massachusetts        5\n",
       "South Carolina       5\n",
       "Oregon               5\n",
       "Name: state, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fix location => geolocate!!\n",
    "from geocoder import osm\n",
    "unique_locations = flat_author_attribute_data.loc[:, 'location'].unique()\n",
    "unique_location_geo_data = list(tqdm(map(lambda x: osm(x), unique_locations), total=len(unique_locations)))\n",
    "unique_location_geo_data = list(map(lambda x: x.json()))\n",
    "unique_location_geo_data_df = list(map(lambda x: x[0].json if len(x) > 0 else None, unique_location_geo_data))\n",
    "[x.update({'location' : y}) for x,y in zip(unique_location_geo_data_df, unique_locations) if x is not None]\n",
    "unique_location_geo_data_df = list(filter(lambda x: x is not None, unique_location_geo_data_df))\n",
    "unique_location_geo_data_df = pd.DataFrame(unique_location_geo_data_df)#.assign(**{'location' : unique_locations})\n",
    "unique_location_geo_data_df = unique_location_geo_data_df.loc[:, ['location', 'accuracy', 'country_code', 'state']]\n",
    "# drop low-confidence scores\n",
    "loc_conf_cutoff = 0.5\n",
    "unique_location_geo_data_df = unique_location_geo_data_df[unique_location_geo_data_df.loc[:, 'accuracy'] >= loc_conf_cutoff]\n",
    "# remove nan vals\n",
    "unique_location_geo_data_df = unique_location_geo_data_df[unique_location_geo_data_df.loc[:, 'location'].apply(lambda x: type(x) is str)]\n",
    "display(unique_location_geo_data_df.head(20))\n",
    "display(unique_location_geo_data_df.loc[:, 'country_code'].value_counts().head(20))\n",
    "display(unique_location_geo_data_df.loc[:, 'state'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not great!! But we have to do something with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    972\n",
       "F    323\n",
       "Name: norm_gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30+    129\n",
       "<30    120\n",
       "Name: norm_age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "US        1324\n",
       "non_US     927\n",
       "Name: norm_location, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## normalize all the demographics\n",
    "# gender\n",
    "gender_norm_lookup = {\n",
    "    'man' : 'M',\n",
    "    'male' : 'M',\n",
    "    'woman' : 'F',\n",
    "    'female' : 'F',\n",
    "}\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_gender' : flat_author_attribute_data.loc[:, 'gender'].apply(gender_norm_lookup.get)\n",
    "})\n",
    "# age\n",
    "age_cutoff = 30\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_age' : flat_author_attribute_data.loc[:, 'age'].apply(lambda x: x if np.isnan(x) else f'{age_cutoff}+' if x >= age_cutoff else f'<{age_cutoff}')\n",
    "})\n",
    "# location\n",
    "location_country_lookup = dict(zip(unique_location_geo_data_df.loc[:, 'location'].values, \n",
    "                                   unique_location_geo_data_df.loc[:, 'country_code'].values))\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_location' : flat_author_attribute_data.loc[:, 'location'].apply(location_country_lookup.get)\n",
    "})\n",
    "# simplify US vs. non-US\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_location' : flat_author_attribute_data.loc[:, 'norm_location'].apply(lambda x: 'US' if x=='us' else 'non_US' if type(x) is str else None)\n",
    "})\n",
    "## show all distributions\n",
    "demo_vars = ['norm_gender', 'norm_age', 'norm_location']\n",
    "for demo_var_i in demo_vars:\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    display(flat_author_attribute_data.loc[:, demo_var_i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge with the full question data and see if we can differentiate the groups based on questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n",
      "demo = norm_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    1613\n",
       "F     513\n",
       "Name: norm_gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<30    177\n",
       "30+    173\n",
       "Name: norm_age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "US        2019\n",
       "non_US    1359\n",
       "Name: norm_location, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "valid_submission_question_data = pd.merge(valid_submission_question_data,\n",
    "                                          flat_author_attribute_data.rename(columns={'author' : 'author_comment'}).loc[:, ['author_comment',]+demo_vars],\n",
    "                                          on='author_comment', how='inner')\n",
    "print(valid_submission_question_data.shape[0])\n",
    "for demo_var_i in demo_vars:\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    display(valid_submission_question_data.loc[:, demo_var_i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less terrible than I thought. Let's look for some differences in question asking!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n",
      "top words for val = M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "produce       2.060049\n",
       "natural       1.991056\n",
       "probably      1.991056\n",
       "law           1.836905\n",
       "price         1.749894\n",
       "away          1.749894\n",
       "factor        1.749894\n",
       "antibodies    1.654584\n",
       "diet          1.654584\n",
       "reduce        1.654584\n",
       "use           1.654584\n",
       "tests         1.654584\n",
       "agree         1.654584\n",
       "caused        1.549223\n",
       "chance        1.549223\n",
       "man           1.549223\n",
       "plastic       1.549223\n",
       "jobs          1.549223\n",
       "keep          1.549223\n",
       "outside       1.549223\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fish              -2.439761\n",
       "14                -2.034296\n",
       "obese             -2.034296\n",
       "pain              -2.034296\n",
       "alone             -2.034296\n",
       "institutions      -2.034296\n",
       "inquisitiveness   -2.034296\n",
       "proof             -2.034296\n",
       "risks             -2.034296\n",
       "games             -2.034296\n",
       "americans         -2.034296\n",
       "abstract          -2.034296\n",
       "careful           -2.034296\n",
       "binaural          -2.034296\n",
       "beats             -2.034296\n",
       "infected          -2.034296\n",
       "regulation        -1.746614\n",
       "reduces           -1.746614\n",
       "headed            -1.746614\n",
       "evaluate          -1.746614\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n",
      "top words for val = <30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "risk       1.769956\n",
       "based      1.587634\n",
       "lack       1.587634\n",
       "..         1.587634\n",
       "white      1.587634\n",
       "warmer     1.587634\n",
       "100        1.587634\n",
       "back       1.364490\n",
       "stream     1.364490\n",
       "things     1.364490\n",
       "virus      1.364490\n",
       "number     1.364490\n",
       "old        1.364490\n",
       "useful     1.364490\n",
       "certain    1.364490\n",
       "gun        1.364490\n",
       "gulf       1.364490\n",
       "natural    1.364490\n",
       "theory     1.364490\n",
       "correct    1.364490\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = 30+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "next         -1.967714\n",
       "country      -1.631242\n",
       "trying       -1.631242\n",
       "help         -1.631242\n",
       "please       -1.631242\n",
       "everything   -1.408098\n",
       "basic        -1.408098\n",
       "making       -1.408098\n",
       "found        -1.408098\n",
       "getting      -1.408098\n",
       "say          -1.408098\n",
       "explain      -1.408098\n",
       "happens      -1.408098\n",
       "may          -1.408098\n",
       "true         -1.408098\n",
       "single       -1.408098\n",
       "plastic      -1.408098\n",
       "feel         -1.408098\n",
       "seen         -1.120416\n",
       "seem         -1.120416\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n",
      "top words for val = US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "support      2.379986\n",
       "blood        2.138824\n",
       "policy       2.138824\n",
       "born         2.043514\n",
       "super        1.938153\n",
       "private      1.820370\n",
       "gain         1.820370\n",
       "play         1.820370\n",
       "hair         1.820370\n",
       "universe     1.820370\n",
       "ocean        1.755831\n",
       "state        1.755831\n",
       "combat       1.686839\n",
       "basis        1.686839\n",
       "single       1.686839\n",
       "meant        1.686839\n",
       "serious      1.686839\n",
       "changing     1.686839\n",
       "stress       1.686839\n",
       "naturally    1.686839\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = non_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "discrimination   -2.204982\n",
       "bring            -2.204982\n",
       "stereotypes      -2.050831\n",
       "online           -2.050831\n",
       "therapy          -2.050831\n",
       "substance        -1.868509\n",
       "~                -1.868509\n",
       "underlying       -1.868509\n",
       "unhealthy        -1.868509\n",
       "authority        -1.868509\n",
       "parties          -1.868509\n",
       "behaviour        -1.868509\n",
       "racial           -1.868509\n",
       "build            -1.868509\n",
       "union            -1.868509\n",
       "somewhere        -1.868509\n",
       "chinese          -1.868509\n",
       "victim           -1.868509\n",
       "1000             -1.868509\n",
       "fuck             -1.868509\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import twitter_data_helpers\n",
    "reload(twitter_data_helpers)\n",
    "from twitter_data_helpers import compute_log_odds\n",
    "text_var = 'reply_question'\n",
    "top_k_words = 20\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    (val_1_i, val_2_i), word_ratio_i = compute_log_odds(data_i, text_var, demo_var_i)\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    print(f'top words for val = {val_1_i}')\n",
    "    display(word_ratio_i.head(top_k_words))\n",
    "    print(f'top words for val = {val_2_i}')\n",
    "    display(word_ratio_i.sort_values(ascending=True).head(top_k_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gender\n",
    "    - `male`: economics (`price`, `jobs`), health (`antibodies`, `diet`), male (`man`), discussion (`agree`, `expect`), causality (`produce`, `law`, `factor`, `reduce`, `caused`)\n",
    "    - `female`: organizations (`americans`, `institutions`, `regulation`), health (`infected`, `obese`), caution (`proof`, `risks`, `careful`, `evaluate`), negative experience (`pain`, `alone`)\n",
    "- Age\n",
    "    - `<30`: theoretical (`theory`, `natural`), numeric (`100`, `number`), certainty (`certain`, `correct`, `useful`)\n",
    "    - `30+`: discussion (`please`, `explain`, `say`), uncertainty (`trying`, `may`, `feel`, `seem`), simplicity (`basic`, `everything`, `single`)\n",
    "- Location\n",
    "    - `US`: nature (`universe`, `ocean`), body (`blood`, `born`, `hair`), conflict (`combat`, `stress`)\n",
    "    - `non_US`: social problems (`discrimination`, `stereotypes`, `underlying`, `racial`, `victim`, `unhealthy`), labor (`authority`, `union`), orthography (`~`, `behaviour`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ACHIEV         re.compile('^(abilit.*|able|accomplish.*|ace|achievable|achieve.*|achievi.*|acquir.*|acquisition.*|actualiz.*|adequa.*|advanc.*|advantag.*|ahead|ambition|ambitions|ambitious|ambitiously|ambitiousness|attain|atta)\n",
       "ADJ            re.compile(\"^(abnormal.*|academic|active|additional|affordable|afraid|after|aggressive|agreeable|alike|alive|alone|amazing|ambitious|ancient|angrier|angriest|angry|annoying|antisocial|anxious|apparent|approachab)\n",
       "ADVERB         re.compile(\"^(about|absolutely|actually|again|almost|already|also|anyway.*|anywhere|apparently|around|awhile|back|barely|basically|beyond|briefly|clearly|commonly|completely|constantly|continually|definitely|esp)\n",
       "AFFECT         re.compile(\"^(abandon.*|abuse.*|abusi.*|accept|accepta.*|accepted|accepting|accepts|ache.*|aching.*|active|actively|admir.*|ador.*|advantag.*|adventur.*|advers.*|affection.*|afraid|aggravat.*|aggress|aggressed|a)\n",
       "AFFILIATION    re.compile(\"^(accompan.*|accomplice.*|affil.*|alliance.*|allies|ally|amigo.*|associate|associates|associating|association|associations|bae|banter.*|belong.*|bestfriend.*|bf|bff.*|bfs|boyfriend.*|breakup|bro|bro')\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^(abilit.*|able|accomplish.*|ace|achievable|achieve.*|achievi.*|acquir.*|acquisition.*|actualiz.*|adequa.*|advanc.*|advantag.*|ahead|ambition|ambitions|ambitious|ambitiously|ambitiousness|attain|attainable|attained|attaining|attainment|attains|authorit.*|award.*|beat|beaten|best|better|bonus.*|burnout.*|capab.*|celebrat.*|challeng.*|champ.*|cheat.*|climb.*|compet.*|confidence|confident|confidently|conquer.*|conscientious.*|create|created|creates|creating|creation|creations|creative|creativity|defeat.*|demot.*|determina.*|determined|diligen.*|domina.*|driven|dropout.*|earn|earned|earning|earns|efficien.*|effort.*|elit.*|emptier|emptiest|emptiness|empty|enabl.*|endeav.*|excel|excellent|excels|fail.*|finaliz.*|first|firsts|flunk.*|founded|founder.*|founding|fulfill.*|gain.*|glory|goal.*|gpa|honor.*|honour.*|ideal.*|importance|improve.*|improving|inadequa.*|incapab.*|incentive.*|incompeten.*|ineffect.*|initiat.*|irresponsible.*|lazier|laziest|lazy|lead|leader.*|leading|leads|limit.*|lose|loser.*|loses|losing|loss.*|lost|mastered|mastery|medal.*|mediocr.*|motiv.*|obtain|obtainable|obtained|obtaining|obtains|opportun.*|overcame|overcome|overcomes|overcoming|overconfiden.*|overtak.*|perfected|perfecting|perfection|perfectly|perfects|persever.*|persist.*|plan|planned|planning|plans|potential.*|powerful|powerless.*|practice|practiced|practices|practicing|prais.*|pride|prize.*|proficien.*|progress|promot.*|proud|prouder|proudest|proudly|purpose.*|queen|quit|quitt.*|rank|ranked|ranking|ranks|recover.*|resolv.*|resourceful.*|reward.*|skill.*|solution.*|solve|solved|solves|solving|strateg.*|striv.*|succeed.*|success|successes|successful|successfully|super|superb.*|surpass.*|surviv.*|team.*|top|tried|tries|triumph.*|try|trying|unable|unbeat.*|unproduc.*|unsuccessful.*|victor.*|win|winn.*|wins|won|work|workabl.*|worked|worker.*|working|works)$\n"
     ]
    }
   ],
   "source": [
    "## same thing but LIWC categories\n",
    "import re\n",
    "LIWC_data = pd.read_csv('/home/cfwelch/LIWC.2015.all', sep=',', header=None)\n",
    "LIWC_data.columns = ['word', 'category']\n",
    "LIWC_data = LIWC_data.assign(**{'word' : LIWC_data.loc[:, 'word'].apply(lambda x: x.replace('*', '.*').strip())})\n",
    "def try_compile(x):\n",
    "    try:\n",
    "        return re.compile(x)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "LIWC_data = LIWC_data.assign(**{'word_matcher' : LIWC_data.loc[:, 'word'].apply(try_compile)})\n",
    "LIWC_data = LIWC_data[LIWC_data.loc[:, 'word_matcher'].apply(lambda x: x is not None)]\n",
    "LIWC_combined_word_patterns = LIWC_data.groupby('category').apply(lambda x: re.compile('^(' + '|'.join(x.loc[:, 'word_matcher'].apply(lambda y: y.pattern)) + ')$'))\n",
    "# get rid of bad categories\n",
    "LIWC_filter_categories = ['NETSPEAK']\n",
    "LIWC_combined_word_patterns.drop(LIWC_filter_categories, inplace=True)\n",
    "display(LIWC_combined_word_patterns.head())\n",
    "print(LIWC_combined_word_patterns.loc['ACHIEV'].pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n",
      "top words for val = M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THEY           1.223712\n",
       "YOU            0.792929\n",
       "INGEST         0.605466\n",
       "MONEY          0.465764\n",
       "FOCUSFUTURE    0.340646\n",
       "SPACE          0.263618\n",
       "ACHIEV         0.261436\n",
       "MALE           0.250262\n",
       "MOTION         0.247783\n",
       "NUMBER         0.244507\n",
       "ASSENT         0.220409\n",
       "PPRON          0.191161\n",
       "WE             0.182258\n",
       "QUANT          0.173889\n",
       "RELATIV        0.170115\n",
       "PREP           0.156940\n",
       "SEE            0.155540\n",
       "CAUSE          0.143676\n",
       "DISCREP        0.119369\n",
       "HEAR           0.114230\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SHEHE         -1.078874\n",
       "FAMILY        -0.593366\n",
       "HOME          -0.422094\n",
       "FILLER        -0.385726\n",
       "FEEL          -0.365524\n",
       "SAD           -0.331659\n",
       "HEALTH        -0.306193\n",
       "DIFFER        -0.303355\n",
       "INTERROG      -0.283944\n",
       "AFFILIATION   -0.278628\n",
       "DEATH         -0.267943\n",
       "NEGEMO        -0.246174\n",
       "RELIG         -0.238090\n",
       "IPRON         -0.226903\n",
       "RISK          -0.208392\n",
       "ANX           -0.168313\n",
       "SEXUAL        -0.162583\n",
       "AFFECT        -0.134548\n",
       "LEISURE       -0.119458\n",
       "ANGER         -0.105424\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n",
      "top words for val = <30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RELIG       0.883518\n",
       "FEEL        0.798360\n",
       "RISK        0.506224\n",
       "POSEMO      0.420179\n",
       "NEGATE      0.392895\n",
       "PREP        0.354674\n",
       "ADVERB      0.329568\n",
       "HEALTH      0.323902\n",
       "INTERROG    0.259364\n",
       "DISCREP     0.259364\n",
       "COMPARE     0.253943\n",
       "INGEST      0.236891\n",
       "ANX         0.236891\n",
       "NONFLU      0.218542\n",
       "QUANT       0.203274\n",
       "DIFFER      0.203274\n",
       "CERTAIN     0.190371\n",
       "ADJ         0.181932\n",
       "AFFECT      0.180470\n",
       "BIO         0.138874\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = 30+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FRIEND        -0.944609\n",
       "HEAR          -0.811078\n",
       "SHEHE         -0.656927\n",
       "SWEAR         -0.656927\n",
       "I             -0.656927\n",
       "FILLER        -0.656927\n",
       "LEISURE       -0.620560\n",
       "MALE          -0.474606\n",
       "MONEY         -0.426404\n",
       "AFFILIATION   -0.320455\n",
       "FOCUSPAST     -0.295137\n",
       "SOCIAL        -0.257852\n",
       "FEMALE        -0.251462\n",
       "PPRON         -0.251462\n",
       "HOME          -0.251462\n",
       "IPRON         -0.251462\n",
       "ANGER         -0.238217\n",
       "PRONOUN       -0.233444\n",
       "ACHIEV        -0.232044\n",
       "FOCUSFUTURE   -0.197395\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n",
      "top words for val = US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THEY           1.121185\n",
       "WE             0.455437\n",
       "FAMILY         0.355718\n",
       "PPRON          0.303566\n",
       "ANX            0.237684\n",
       "SAD            0.221844\n",
       "REWARD         0.197831\n",
       "RISK           0.178854\n",
       "FILLER         0.176724\n",
       "FOCUSFUTURE    0.175280\n",
       "HOME           0.173396\n",
       "MONEY          0.172748\n",
       "ASSENT         0.147736\n",
       "INFORMAL       0.136064\n",
       "I              0.133799\n",
       "MALE           0.119737\n",
       "WORK           0.107314\n",
       "DISCREP        0.102000\n",
       "NONFLU         0.098081\n",
       "MOTION         0.075948\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = non_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SHEHE         -0.670574\n",
       "CONJ          -0.370916\n",
       "SEXUAL        -0.360913\n",
       "INTERROG      -0.287582\n",
       "ANGER         -0.255961\n",
       "QUANT         -0.205378\n",
       "SEE           -0.189106\n",
       "BODY          -0.152182\n",
       "NEGATE        -0.144481\n",
       "DIFFER        -0.141730\n",
       "DEATH         -0.121527\n",
       "BIO           -0.116229\n",
       "HEALTH        -0.113288\n",
       "NUMBER        -0.104582\n",
       "FEMALE        -0.100029\n",
       "AFFILIATION   -0.097475\n",
       "CERTAIN       -0.095210\n",
       "PERCEPT       -0.074700\n",
       "HEAR          -0.070020\n",
       "ADVERB        -0.060119\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import twitter_data_helpers\n",
    "reload(twitter_data_helpers)\n",
    "from twitter_data_helpers import compute_log_odds\n",
    "text_var = 'reply_question'\n",
    "top_k_words = 20\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    (val_1_i, val_2_i), word_ratio_i = compute_log_odds(data_i, text_var, demo_var_i, word_categories=LIWC_combined_word_patterns)\n",
    "    val_1_counts_i = word_ratio_i[word_ratio_i > 0.].head(top_k_words)\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    print(f'top words for val = {val_1_i}')\n",
    "    display(val_1_counts_i)\n",
    "    val_2_counts_i = word_ratio_i[word_ratio_i < 0.].sort_values(ascending=True).head(top_k_words)\n",
    "    print(f'top words for val = {val_2_i}')\n",
    "    display(val_2_counts_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gender\n",
    "    - `male`: others (`THEY`, `YOU`), quantity (`MONEY`, `NUMBER`, `QUANT`), self (`MALE`, `WE`, `PPRON`)\n",
    "    - `female`: close relations (`SHEHE`, `FAMILY`, `HOME`), emotion (`SAD`, `FEEL`, `NEGEMO`), health (`HEALTH`, `DEATH`)\n",
    "- Age\n",
    "    - `<30`: emotion (`POSEMO`, `FEEL`, `AFFECT`), health (`HEALTH`, `INGEST`, `BIO`), certainty (`RISK`, `COMPARE`, `CERTAIN`)\n",
    "    - `30+`: social (`FRIEND`, `SHEHE`, `MALE`, `SOCIAL`), negative emotion (`NEGEMO`, `ANGER`), social institutions (`MOENY`, `AFFILIATION`), time (`FOCUSPAST`, `FOCUSFUTURE`), comfort (`LEISURE`, `HOME`)\n",
    "- Location\n",
    "    - `US`: social (`THEY`, `WE`, `FAMILY`, `PPRON`), negative emotion (`SAD`, `ANX`), action (`RISK`, `FOCUSFUTURE`, `WORK`, `REWARD`)\n",
    "    - `non_US`: immediate social (`SHEHE`), health (`SEXUAL`, `BODY`, `BIO`, `HEALTH`, `DEATH`), quantity (`QUANT`, `NUMBER`), evidence (`INTERROG`, `SEE`, `NEGATE`, `CERTAIN`, `PERCEPT`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: taxonomy of clarification questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

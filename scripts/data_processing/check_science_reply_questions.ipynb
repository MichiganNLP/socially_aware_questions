{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check science reply questions\n",
    "Let's look at some data related to sharing new science articles and the questions that people pose in response to the articles.\n",
    "\n",
    "We'll see how readily we can predict author background using the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "1001it [00:05, 286.60it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "47091it [07:52, 99.66it/s] \n"
     ]
    }
   ],
   "source": [
    "## get Reddit data!!\n",
    "from datetime import datetime\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm\n",
    "# from data_helpers import load_reddit_api\n",
    "# reddit_api, pushshift_api = load_reddit_api('../../data/auth_data/reddit_auth.csv')\n",
    "pushshift_api = PushshiftAPI()\n",
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['url', 'title', 'author', 'score', 'text', 'created_utc', 'id', 'upvote_ratio', 'num_comments']\n",
    "submissions = pushshift_api.search_submissions(q=\"*\", after=date_range[0], before=date_range[1],\n",
    "                                               subreddit=subreddit, filter=filter_fields)\n",
    "submissions_results = []\n",
    "for s in tqdm(submissions):\n",
    "    submissions_results.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>d_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talismanbrandi</td>\n",
       "      <td>1630462648</td>\n",
       "      <td>pfkdt5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Socio-economic disparities and COVID-19 in the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/pfkd...</td>\n",
       "      <td>1630480648.0</td>\n",
       "      <td>{'author': 'talismanbrandi', 'created_utc': 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BeforeYourBBQ</td>\n",
       "      <td>1630462436</td>\n",
       "      <td>pfkbn2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing SARS-CoV-2 natural immunity to vacci...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.medrxiv.org/content/10.1101/2021.0...</td>\n",
       "      <td>1630480436.0</td>\n",
       "      <td>{'author': 'BeforeYourBBQ', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462250</td>\n",
       "      <td>pfk9q9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Scientists Figured Out How Much Exercise You N...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.sciencealert.com/scientists-figure...</td>\n",
       "      <td>1630480250.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462179</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "      <td>Female octopuses throw shells at males annoyin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.independent.co.uk/climate-change/n...</td>\n",
       "      <td>1630480179.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doozenburg</td>\n",
       "      <td>1630461660</td>\n",
       "      <td>pfk3ts</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Who is Anti-Vax Dr. Wendy Menigoz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.slugbrain.com/post/who-is-anti-vax...</td>\n",
       "      <td>1630479660.0</td>\n",
       "      <td>{'author': 'Doozenburg', 'created_utc': 163046...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  created_utc  ...       created                                                 d_\n",
       "0  talismanbrandi   1630462648  ...  1630480648.0  {'author': 'talismanbrandi', 'created_utc': 16...\n",
       "1   BeforeYourBBQ   1630462436  ...  1630480436.0  {'author': 'BeforeYourBBQ', 'created_utc': 163...\n",
       "2   key__lime_pie   1630462250  ...  1630480250.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "3   key__lime_pie   1630462179  ...  1630480179.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "4      Doozenburg   1630461660  ...  1630479660.0  {'author': 'Doozenburg', 'created_utc': 163046...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## convert to data frame\n",
    "import pandas as pd\n",
    "submission_data = pd.DataFrame(submissions_results)\n",
    "display(submission_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect all comments from the same time frame, and align them to submissions afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "901it [00:03, 289.96it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "18800it [03:16, 23.04it/s] /home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "907905it [2:33:53, 83.42it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "1445247it [4:03:56, 98.15it/s] "
     ]
    }
   ],
   "source": [
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['id', 'link_id', 'parent_id', 'body', 'author', 'created_utc', 'score']\n",
    "comments = pushshift_api.search_comments(after=date_range[0], before=date_range[1],\n",
    "                                         subreddit=subreddit, filter=filter_fields)\n",
    "comments_results = []\n",
    "for c in tqdm(comments):\n",
    "    comments_results.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Etna</td>\n",
       "      <td>oh I misread, it's the males that are being an...</td>\n",
       "      <td>1630468787</td>\n",
       "      <td>hb5b6ss</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>27</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lalauna</td>\n",
       "      <td>Please tell me something i didn't know before.</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zz</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DivineBanana</td>\n",
       "      <td>I haven't followed the debate but I'm sure it'...</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zy</td>\n",
       "      <td>pf5phr</td>\n",
       "      <td>hb58igp</td>\n",
       "      <td>3</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_MASTADONG_</td>\n",
       "      <td>I just linked you to 2 articles on fact checki...</td>\n",
       "      <td>1630468723</td>\n",
       "      <td>hb5b2sg</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>hb5adks</td>\n",
       "      <td>4</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Etna</td>\n",
       "      <td>It's because they can't talk</td>\n",
       "      <td>1630468713</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>37</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               body  created_utc       id link_id parent_id  score       created\n",
       "0          Etna  oh I misread, it's the males that are being an...   1630468787  hb5b6ss  pfk90j   hb5b25v     27  1.630487e+09\n",
       "3       lalauna     Please tell me something i didn't know before.   1630468726  hb5b2zz  pfgvrw    pfgvrw      1  1.630487e+09\n",
       "4  DivineBanana  I haven't followed the debate but I'm sure it'...   1630468726  hb5b2zy  pf5phr   hb58igp      3  1.630487e+09\n",
       "5   _MASTADONG_  I just linked you to 2 articles on fact checki...   1630468723  hb5b2sg  pfgvrw   hb5adks      4  1.630487e+09\n",
       "8          Etna                       It's because they can't talk   1630468713  hb5b25v  pfk90j    pfk90j     37  1.630487e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196985\n"
     ]
    }
   ],
   "source": [
    "## combine/clean\n",
    "comment_data = pd.DataFrame(comments_results)\n",
    "# drop deleted data\n",
    "comment_data = comment_data[(comment_data.loc[:, 'author']!='[deleted]') &\n",
    "                            (comment_data.loc[:, 'body']!='[deleted]')]\n",
    "# fix ID vars\n",
    "comment_data = comment_data.assign(**{\n",
    "    'link_id' : comment_data.loc[:, 'link_id'].apply(lambda x: x.split('_')[1]),\n",
    "    'parent_id' : comment_data.loc[:, 'parent_id'].apply(lambda x: x.split('_')[1]),\n",
    "})\n",
    "# drop extra data\n",
    "comment_data.drop('d_', axis=1, inplace=True)\n",
    "display(comment_data.head())\n",
    "print(comment_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict to comment/submission matches\n",
    "submission_comment_data = pd.merge(submission_data, comment_data, left_on='id', right_on='link_id', how='inner')\n",
    "# fix col names\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_x', '_submission') \n",
    "    for x in list(filter(lambda x: x.endswith('_x'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_y', '_comment') \n",
    "    for x in list(filter(lambda x: x.endswith('_y'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data = submission_comment_data[submission_comment_data.loc[:, 'link_id']==submission_comment_data.loc[:, 'parent_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465354"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_comment_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130878 questions total\n"
     ]
    }
   ],
   "source": [
    "## clean text\n",
    "import re\n",
    "RETURN_MATCHER = re.compile('[\\n\\r]')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'body' : submission_comment_data.loc[:, 'body'].apply(lambda x: RETURN_MATCHER.sub(' ', x))\n",
    "})\n",
    "## filter questions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_sents' : submission_comment_data.loc[:, 'body'].apply(lambda x: sent_tokenize(x))\n",
    "})\n",
    "# look for questions!\n",
    "import re\n",
    "question_matcher = re.compile('\\?$')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_questions' : submission_comment_data.loc[:, 'reply_sents'].apply(lambda x: list(filter(lambda y: question_matcher.search(y) is not None, x)))\n",
    "})\n",
    "submission_question_data = submission_comment_data[submission_comment_data.loc[:, 'reply_questions'].apply(len)>0]\n",
    "## flatten\n",
    "flat_submission_question_data = []\n",
    "for idx_i, data_i in submission_question_data.iterrows():\n",
    "    for q_j in data_i.loc['reply_questions']:\n",
    "        data_j = data_i.copy().drop('reply_questions')\n",
    "        data_j.loc['reply_question'] = q_j\n",
    "        flat_submission_question_data.append(data_j)\n",
    "flat_submission_question_data = pd.concat(flat_submission_question_data, axis=1).transpose()\n",
    "print(f'{flat_submission_question_data.shape[0]} questions total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['die for the economy?', 'how about we topple you instead?',\n",
       "       'Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Have I misread something?',\n",
       "       \"Don't you care about the environment?\"], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## look at sample questions => clarification questions? self-contained? related to post?\n",
    "## sample questions\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(flat_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit the questions to have at least X words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Wait, who was saying there would be a pandemic baby boom and why?',\n",
       "       \"Isn't it established that stress and uncertainty eliminate the desire to be parents, or is that just my intuition?\",\n",
       "       'The baby boom thing was just a joke right?',\n",
       "       'Was there any real feeling that that would happen?'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "min_question_words = 10\n",
    "tokenizer = WordPunctTokenizer()\n",
    "valid_submission_question_data = flat_submission_question_data[flat_submission_question_data.loc[:, 'reply_question'].apply(lambda x: len(tokenizer.tokenize(x)) >= min_question_words)]\n",
    "display(valid_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for posterity!!\n",
    "valid_submission_question_data.to_csv('science_submission_question_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "2it [00:03,  2.19s/it]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "3099it [6:11:34,  6.67s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10000it [20:27:42,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## TODO: mine previous history for N=10000 commenters; extract location + age + gender (?)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from psaw import PushshiftAPI\n",
    "pushshift_api = PushshiftAPI()\n",
    "N_commenters = 10000\n",
    "N_previous_comments = 1000\n",
    "sample_question_author_data = valid_submission_question_data.sort_values(['author_comment', 'created_utc_comment'], ascending=True).drop_duplicates('author_comment').loc[:, ['author_comment', 'created_utc_comment']]\n",
    "sample_question_author_data = sample_question_author_data.sample(N_commenters, replace=False, random_state=123)\n",
    "sample_question_author_prior_data = []\n",
    "author_filter_cols = ['body', 'id', 'created_utc', 'author', 'subreddit']\n",
    "for idx_i, data_i in tqdm(sample_question_author_data.iterrows()):\n",
    "    author_i = data_i.loc['author_comment']\n",
    "    time_i = int(data_i.loc['created_utc_comment'])\n",
    "    prior_comments_i = list(pushshift_api.search_comments(author=author_i, limit=N_previous_comments, before=time_i, filter=author_filter_cols))\n",
    "    prior_comments_i = pd.DataFrame(prior_comments_i)\n",
    "    if('d_' in prior_comments_i.columns):\n",
    "        prior_comments_i.drop('d_', axis=1, inplace=True)\n",
    "    sample_question_author_prior_data.append(prior_comments_i)\n",
    "sample_question_author_prior_data = pd.concat(sample_question_author_prior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_question_author_prior_data.to_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (0,1,2,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data = pd.read_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data.dropna(subset=['body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>You're not even Scottish, are you?</td>\n",
       "      <td>1582310896</td>\n",
       "      <td>fiayfoz</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582329e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.</td>\n",
       "      <td>1582298322</td>\n",
       "      <td>fiacrvb</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582316e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.</td>\n",
       "      <td>1582291726</td>\n",
       "      <td>fia35ak</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582310e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...</td>\n",
       "      <td>1582258926</td>\n",
       "      <td>fi99f50</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582277e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog</td>\n",
       "      <td>1582258153</td>\n",
       "      <td>fi98eh0</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582276e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  \\\n",
       "0  mahogany555   \n",
       "1  mahogany555   \n",
       "2  mahogany555   \n",
       "3  mahogany555   \n",
       "4  mahogany555   \n",
       "\n",
       "                                                                                                                                                                                                                                              body  \\\n",
       "0                                                                                                                                                                                                               You're not even Scottish, are you?   \n",
       "1                                                                                                                                                         'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.   \n",
       "2  Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.   \n",
       "3                                                                                                                   No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...   \n",
       "4                                                                                                             Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog   \n",
       "\n",
       "  created_utc       id              subreddit       created  \n",
       "0  1582310896  fiayfoz  ScottishPeopleTwitter  1.582329e+09  \n",
       "1  1582298322  fiacrvb  ScottishPeopleTwitter  1.582316e+09  \n",
       "2  1582291726  fia35ak  ScottishPeopleTwitter  1.582310e+09  \n",
       "3  1582258926  fi99f50  ScottishPeopleTwitter  1.582277e+09  \n",
       "4  1582258153  fi98eh0  ScottishPeopleTwitter  1.582276e+09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(sample_question_author_prior_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='I live'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'416971/6804000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## check for self-disclosure statements\n",
    "import re\n",
    "self_disclosure_matcher = re.compile(f'(I\\'m|I am|I live)')\n",
    "print(self_disclosure_matcher.search('I live in MI'))\n",
    "sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "    'body_contain_self_disclosure' : sample_question_author_prior_data.loc[:, 'body'].apply(lambda x: self_disclosure_matcher.search(x) is not None)\n",
    "})\n",
    "display(f'{sample_question_author_prior_data.loc[:, \"body_contain_self_disclosure\"].sum()}/{sample_question_author_prior_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPULA_LEMMA = 'be'\n",
    "EXIST_LEMMA = 'live'\n",
    "GENDER_MATCHER = re.compile('^(man|woman|male|female)$')\n",
    "AGE_NUM_MATCHER = re.compile('[0-9]+')\n",
    "def collect_propn(token):\n",
    "    loc_noun_parts = [token]\n",
    "    location_children = list(filter(lambda x: x.pos_=='PROPN', token.children))\n",
    "    while(len(location_children) > 0):\n",
    "        loc_noun_part = location_children.pop()\n",
    "        loc_noun_parts.append(loc_noun_part)\n",
    "        location_children += list(filter(lambda x: x.pos_=='PROPN', loc_noun_part.children))\n",
    "    # sort noun parts\n",
    "    loc_noun_parts = list(sorted(loc_noun_parts, key=lambda x: x.idx))\n",
    "    loc_noun = ' '.join(list(map(lambda x: x.lemma_, loc_noun_parts)))\n",
    "    return loc_noun\n",
    "def extract_self_statement_targets(sent, pipeline=None):\n",
    "    if(pipeline is not None):\n",
    "        parse = pipeline(sent)\n",
    "    else:\n",
    "        parse = sent\n",
    "    identity_attributes = []\n",
    "    parse_sents = list(parse.doc.sents)\n",
    "    for parse_sent in parse_sents:\n",
    "        for token in parse_sent:\n",
    "            # get children nouns for \"I\" via root\n",
    "            if(token.lemma_ == 'I' and token.dep_ == 'nsubj'):\n",
    "                token_root_ancestors = list(filter(lambda x: x.dep_=='ROOT', token.ancestors))\n",
    "    #             print(f'parse ents = {list(map(lambda x: x.label_, parse.ents))}')\n",
    "                loc_ents = list(filter(lambda x: x.label_=='GPE', parse.ents))\n",
    "                if(len(token_root_ancestors) > 0):\n",
    "                    token_root = token_root_ancestors[0]\n",
    "                    root_children = list(token_root.children)\n",
    "                    if(token_root.lemma_ == COPULA_LEMMA):\n",
    "                        for child in root_children:\n",
    "                            if(child.dep_ == 'attr'):\n",
    "                                # gender\n",
    "                                gender_match = GENDER_MATCHER.match(child.lemma_)\n",
    "                                if(gender_match is not None):\n",
    "                                    identity_attributes.append(['gender', gender_match.group(0)])\n",
    "                                # age => NOPE false positives abound\n",
    "#                                 age_match = AGE_NUM_MATCHER.match(child.lemma_)\n",
    "#                                 if(age_match is not None):\n",
    "#                                     identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # age\n",
    "                            elif(child.dep_ == 'acomp'):\n",
    "                                if(child.lemma_ == 'old'):\n",
    "                                    # look for children (\"30 years old\")\n",
    "                                    age_children_1 = list(child.children)\n",
    "                                    if(len(age_children_1) > 0 and age_children_1[0].lemma_=='year'):\n",
    "                                        age_children_2 = list(age_children_1[0].children)\n",
    "                                        if(len(age_children_2) > 0):\n",
    "                                            age_match = AGE_NUM_MATCHER.match(age_children_2[0].lemma_)\n",
    "                                            if(age_match is not None):\n",
    "                                                identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # location\n",
    "                            elif(child.dep_ == 'prep' and child.lemma_ == 'from'):\n",
    "                                location_children_1 = list(child.children)\n",
    "                                if(len(location_children_1) > 0 and location_children_1[0].pos_ == 'PROPN'):\n",
    "                                    ent_start = location_children_1[0].i\n",
    "    #                                 child_1_idx = location_children_1[0].idx\n",
    "    #                                 child_1_ent = \n",
    "    #                                 main_loc = location_children_1[0]\n",
    "    #                                 loc_noun = collect_propn(main_loc)\n",
    "                                    # find ENT that contains child\n",
    "    #                                 print(f'ent start = {ent_start}')\n",
    "    #                                 print(f'{[(x.start, x.end) for x in loc_ents]}')\n",
    "                                    containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "    #                                 print(f'containing loc ents {containing_loc_ents}')\n",
    "                                    if(len(containing_loc_ents) > 0):\n",
    "                                        loc_noun = containing_loc_ents[0].text\n",
    "                                        identity_attributes.append(['location', loc_noun])\n",
    "                    # \"I live in the US\"\n",
    "                    elif(token_root.lemma_ == EXIST_LEMMA):\n",
    "                        root_prep_children = list(filter(lambda x: x.lemma_=='in' and x.dep_=='prep', token_root.children))\n",
    "                        if(len(root_prep_children) > 0):\n",
    "                            prep_children_2 = list(filter(lambda x: x.pos_ == 'PROPN', root_prep_children[0].children))\n",
    "                            if(len(prep_children_2) > 0):\n",
    "                                ent_start = prep_children_2[0].i\n",
    "                                containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "                                if(len(containing_loc_ents)):\n",
    "                                    loc_noun = containing_loc_ents[0].text\n",
    "                                    identity_attributes.append(['location', loc_noun])\n",
    "    return identity_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent = I am 30 years old has attr [['age', '30']]\n",
      "sent = I am a woman has attr [['gender', 'woman']]\n",
      "sent = I live in London, England has attr [['location', 'London']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# dumb\n",
    "# self_statement = '(I\\'m|I am)'\n",
    "# age_matcher = f'({self_statement} [0-9]+ years old)|({self_statement} a [0-9]+ yo )'\n",
    "# gender_matcher = f'({self_statement} a[ a-zA-Z]? (man|woman|male|female))'\n",
    "# smart => parse then extract\n",
    "import spacy\n",
    "nlp_pipeline = spacy.load('en_core_web_sm')\n",
    "# gender_dep = attr (\"I'm a man\")\n",
    "# age_dep = acomp (\"I'm 50 years old\")\n",
    "# loc_dep = prep (\"I live in Michigan\")\n",
    "test_sents = [\n",
    "    'I am 30 years old',\n",
    "    'I am a woman',\n",
    "    'I live in London, England',\n",
    "]\n",
    "for sent in test_sents:\n",
    "    sent_attr = extract_self_statement_targets(sent, nlp_pipeline)\n",
    "    print(f'sent = {sent} has attr {sent_attr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have an extremely brittle attribute extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nope\n",
    "# ## get parses first\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# from pandarallel import pandarallel\n",
    "# # pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "#     'body_parse' : list(tqdm(nlp_pipeline.pipe(sample_question_author_prior_data.loc[:, 'body'].values, batch_size=1000)))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parallelized pipeline\n",
    "# https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html#Option-3:-Parallelize-the-work-using-joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "N_JOBS=7\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "def process_chunk(texts):\n",
    "    proc_results = []\n",
    "    for doc in nlp_pipeline.pipe(texts, batch_size=20):\n",
    "#         preproc_pipe.append(lemmatize_pipe(doc))\n",
    "        proc_results.append(doc)\n",
    "    return proc_results\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=N_JOBS, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in tqdm(chunker(texts, len(texts), chunksize=chunksize)))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [18:22,  2.04s/it]"
     ]
    }
   ],
   "source": [
    "# takes too much memory!!\n",
    "# body_parse = preprocess_parallel(sample_question_author_prior_data.loc[:, 'body'].values, chunksize=1000)\n",
    "# print(body_parse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "askmen                   16907\n",
       "askwomen                  3898\n",
       "mensrights                2745\n",
       "menwritingwomen           1279\n",
       "menslib                    903\n",
       "whereareallthegoodmen      583\n",
       "mentalhealth               513\n",
       "watchmen                   453\n",
       "menopause                  444\n",
       "mensa                      299\n",
       "xmen                       291\n",
       "redpillwomen               274\n",
       "mendrawingwomen            230\n",
       "ramen                      208\n",
       "askgaymen                  204\n",
       "armoredwomen               187\n",
       "madmen                     150\n",
       "adhdwomen                  137\n",
       "mentalillness              123\n",
       "menkampf                   103\n",
       "women                       99\n",
       "menshealth                  59\n",
       "men                         55\n",
       "autisminwomen               50\n",
       "menieres                    48\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "askwomen            3898\n",
       "menwritingwomen     1279\n",
       "redpillwomen         274\n",
       "mendrawingwomen      230\n",
       "armoredwomen         187\n",
       "adhdwomen            137\n",
       "womenshealth         109\n",
       "women                 99\n",
       "autisminwomen         50\n",
       "womenofcolor          20\n",
       "prettyolderwomen      17\n",
       "womensstreetwear      17\n",
       "justhotwomen          16\n",
       "womensrightsnews      14\n",
       "bluecollarwomen       11\n",
       "womenengineers        10\n",
       "womenwhodontsell       9\n",
       "darkestwomen           8\n",
       "whipped_women          8\n",
       "womenwritingmen        7\n",
       "womenbendingover       7\n",
       "actualwomen            5\n",
       "womensfashion          5\n",
       "womenstyleadvice       4\n",
       "womenofcolorxxx        4\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nyc           3219\n",
       "losangeles    3279\n",
       "chicago       4038\n",
       "houston       2261\n",
       "phoenix        714\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "oldschoolcool          15862\n",
       "oldpeoplefacebook        446\n",
       "oldphotosinreallife      364\n",
       "youngpeopleyoutube       356\n",
       "gold                     293\n",
       "old_recipes              266\n",
       "galaxyfold               184\n",
       "goforgold                180\n",
       "oldschoolhot             173\n",
       "negativewithgold         141\n",
       "fuckimold                121\n",
       "oldmandog                117\n",
       "youngjustice             116\n",
       "oldschoolcoolnsfw        110\n",
       "oldfreefolk              107\n",
       "cuckold                  106\n",
       "gayyoungold              105\n",
       "mold                      94\n",
       "oldschoolcelebs           84\n",
       "freezingfuckingcold       64\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show \"men\"/\"women\" subreddits\n",
    "subreddit_counts = sample_question_author_prior_data.loc[:, 'subreddit'].value_counts()\n",
    "subreddit_counts.index = list(map(lambda x: str(x.lower()), subreddit_counts.index))\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^men)|(men$)', x) is not None, subreddit_counts.index))].sort_values(ascending=False).head(25))\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^women)|(women$)', x) is not None, subreddit_counts.index))].sort_values(ascending=False).head(25))\n",
    "# show location subreddits\n",
    "display(subreddit_counts.loc[['nyc', 'losangeles', 'chicago', 'houston', 'phoenix']])\n",
    "# show age subreddits\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^old)|(old$)|(^young)|(young$)', x), subreddit_counts.index))].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416971/416971 [1:55:00<00:00, 60.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "## extract all attributes!!\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "out_file_name = 'science_sample_reply_author_attrs.gz'\n",
    "id_cols = ['author', 'created_utc']\n",
    "self_disclosure_author_prior_data = sample_question_author_prior_data[sample_question_author_prior_data.loc[:, 'body_contain_self_disclosure']]\n",
    "self_disclosure_author_prior_data = self_disclosure_author_prior_data.assign(**{\n",
    "    'id_attrs' : self_disclosure_author_prior_data.loc[:, 'body'].progress_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline))\n",
    "})\n",
    "# pandarallel => memory isssues\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "#     'identity_attributes' : sample_question_author_prior_data.loc[:, 'body'].parallel_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline)),\n",
    "# #     'identity_attributes' : sample_question_author_prior_data.loc[:, 'body_parse'].progress_apply(lambda x: extract_self_statement_targets(x))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3292/3292 [00:25<00:00, 129.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-BluBone-</td>\n",
       "      <td>man</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Cerberus</td>\n",
       "      <td>man</td>\n",
       "      <td>41</td>\n",
       "      <td>KC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Daetrax-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Negative-Karma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Nycter-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author gender  age      location\n",
       "0        -BluBone-    man  NaN           NaN\n",
       "0        -Cerberus    man   41            KC\n",
       "0        -Daetrax-    NaN  NaN       Denmark\n",
       "0  -Negative-Karma    NaN  NaN        Kansas\n",
       "0         -Nycter-    NaN  NaN  saudi arabia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_author_attribute_data = self_disclosure_author_prior_data[self_disclosure_author_prior_data.loc[:, 'id_attrs'].apply(lambda x: len(x) > 0)]\n",
    "# get one line per author\n",
    "flat_author_attribute_data = []\n",
    "for author_i, data_i in tqdm(question_author_attribute_data.groupby('author')):\n",
    "    attr_data_i = []\n",
    "    for idx_j, data_j in data_i.iterrows():\n",
    "        # keep track of dates!!\n",
    "        date_j = data_j.loc['created_utc']\n",
    "        for attr_k, val_k in data_j.loc['id_attrs']:\n",
    "            attr_data_i.append({\n",
    "                'author' : author_i,\n",
    "                'date' : date_j,\n",
    "                'attr' : attr_k,\n",
    "                'val' : val_k\n",
    "            })\n",
    "    attr_data_i = pd.DataFrame(attr_data_i)\n",
    "    attr_data_i.sort_values(['attr', 'date'], inplace=True, ascending=False)\n",
    "    attr_data_i = attr_data_i.drop_duplicates(['attr'], keep='first').drop('date', axis=1)    \n",
    "    attr_data_i = attr_data_i.pivot(index='author', columns=['attr'], values=['val']).reset_index()\n",
    "    attr_data_i.columns = list(map(lambda x: x[0] if x[1]=='' else x[1], attr_data_i.columns))\n",
    "    flat_author_attribute_data.append(attr_data_i)\n",
    "flat_author_attribute_data = pd.concat(flat_author_attribute_data, axis=0)\n",
    "display(flat_author_attribute_data.head())\n",
    "## save for later\n",
    "flat_author_attribute_data.to_csv('science_reply_author_attr_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US            150\n",
      "Canada        100\n",
      "UK             81\n",
      "Texas          79\n",
      "California     62\n",
      "Florida        52\n",
      "America        49\n",
      "Australia      39\n",
      "Germany        35\n",
      "USA            34\n",
      "Name: location, dtype: int64\n",
      "man       789\n",
      "woman     275\n",
      "male      184\n",
      "female     48\n",
      "Name: gender, dtype: int64\n",
      "30    19\n",
      "20    11\n",
      "40    11\n",
      "33    10\n",
      "5     10\n",
      "31     9\n",
      "10     9\n",
      "50     8\n",
      "36     6\n",
      "19     6\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_attr = ['location', 'gender', 'age']\n",
    "for attr_i in all_attr:\n",
    "    print(flat_author_attribute_data.loc[:, attr_i].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This isn't the best but we'll see what we can do with the aggregate categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median age = 30.000\n"
     ]
    }
   ],
   "source": [
    "# fix age\n",
    "import numpy as np\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'age' : flat_author_attribute_data.loc[:, 'age'].apply(lambda x: x if type(x) is float and np.isnan(x) else int(x))\n",
    "})\n",
    "max_age = 100\n",
    "flat_author_attribute_data = flat_author_attribute_data[flat_author_attribute_data.loc[:, 'age'].apply(lambda x: np.isnan(x) or x <= max_age)]\n",
    "## plot age distribution\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "age_vals = flat_author_attribute_data.loc[:, 'age'].dropna().astype(int)\n",
    "plt.hist(age_vals)\n",
    "plt.show()\n",
    "print(f'median age = {age_vals.median():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>0.806806</td>\n",
       "      <td>dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>0.826542</td>\n",
       "      <td>us</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saudi arabia</td>\n",
       "      <td>0.735263</td>\n",
       "      <td>sa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.841632</td>\n",
       "      <td>se</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Italy</td>\n",
       "      <td>0.883102</td>\n",
       "      <td>it</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0.800803</td>\n",
       "      <td>us</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California</td>\n",
       "      <td>0.922136</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Canada</td>\n",
       "      <td>0.976126</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>0.765295</td>\n",
       "      <td>us</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>0.810392</td>\n",
       "      <td>us</td>\n",
       "      <td>Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Riverside</td>\n",
       "      <td>0.673493</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finland</td>\n",
       "      <td>0.907328</td>\n",
       "      <td>fi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Orange County</td>\n",
       "      <td>0.793026</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NZ</td>\n",
       "      <td>0.791711</td>\n",
       "      <td>nz</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DC</td>\n",
       "      <td>0.749289</td>\n",
       "      <td>us</td>\n",
       "      <td>District of Columbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>0.786380</td>\n",
       "      <td>us</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Grand Rapids</td>\n",
       "      <td>0.765069</td>\n",
       "      <td>us</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>US</td>\n",
       "      <td>0.935691</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alberta</td>\n",
       "      <td>0.782431</td>\n",
       "      <td>ca</td>\n",
       "      <td>Alberta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         location  accuracy country_code                 state\n",
       "2         Denmark  0.806806           dk                   NaN\n",
       "3          Kansas  0.826542           us                Kansas\n",
       "4    saudi arabia  0.735263           sa                   NaN\n",
       "5         Germany  0.889681           de                   NaN\n",
       "6          Sweden  0.841632           se                   NaN\n",
       "7           Italy  0.883102           it                   NaN\n",
       "8         Atlanta  0.800803           us               Georgia\n",
       "9      California  0.922136           us            California\n",
       "10         Canada  0.976126           ca                   NaN\n",
       "11      Baltimore  0.765295           us              Maryland\n",
       "12    Mississippi  0.810392           us           Mississippi\n",
       "13      Riverside  0.673493           us            California\n",
       "15        Finland  0.907328           fi                   NaN\n",
       "16  Orange County  0.793026           us            California\n",
       "17             NZ  0.791711           nz                   NaN\n",
       "18             DC  0.749289           us  District of Columbia\n",
       "19       Brooklyn  0.786380           us              New York\n",
       "20   Grand Rapids  0.765069           us              Michigan\n",
       "21             US  0.935691           us                   NaN\n",
       "22        Alberta  0.782431           ca               Alberta"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "us    259\n",
       "gb     31\n",
       "ca     29\n",
       "cn     10\n",
       "au     10\n",
       "it      9\n",
       "mx      6\n",
       "nl      6\n",
       "de      6\n",
       "co      5\n",
       "jp      5\n",
       "in      4\n",
       "es      4\n",
       "fr      4\n",
       "ch      4\n",
       "th      3\n",
       "kr      3\n",
       "br      3\n",
       "be      3\n",
       "ar      3\n",
       "Name: country_code, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "California          32\n",
       "Texas               25\n",
       "England             21\n",
       "New York            17\n",
       "Florida             11\n",
       "Illinois            11\n",
       "Washington           9\n",
       "Ohio                 8\n",
       "Pennsylvania         8\n",
       "Ontario              7\n",
       "British Columbia     6\n",
       "Virginia             6\n",
       "Michigan             6\n",
       "North Dakota         5\n",
       "Wisconsin            5\n",
       "North Carolina       5\n",
       "Québec               5\n",
       "Massachusetts        5\n",
       "South Carolina       5\n",
       "Oregon               5\n",
       "Name: state, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fix location => geolocate!!\n",
    "from geocoder import osm\n",
    "unique_locations = flat_author_attribute_data.loc[:, 'location'].unique()\n",
    "unique_location_geo_data = list(tqdm(map(lambda x: osm(x), unique_locations), total=len(unique_locations)))\n",
    "unique_location_geo_data = list(map(lambda x: x.json()))\n",
    "unique_location_geo_data_df = list(map(lambda x: x[0].json if len(x) > 0 else None, unique_location_geo_data))\n",
    "[x.update({'location' : y}) for x,y in zip(unique_location_geo_data_df, unique_locations) if x is not None]\n",
    "unique_location_geo_data_df = list(filter(lambda x: x is not None, unique_location_geo_data_df))\n",
    "unique_location_geo_data_df = pd.DataFrame(unique_location_geo_data_df)#.assign(**{'location' : unique_locations})\n",
    "unique_location_geo_data_df = unique_location_geo_data_df.loc[:, ['location', 'accuracy', 'country_code', 'state']]\n",
    "# drop low-confidence scores\n",
    "loc_conf_cutoff = 0.5\n",
    "unique_location_geo_data_df = unique_location_geo_data_df[unique_location_geo_data_df.loc[:, 'accuracy'] >= loc_conf_cutoff]\n",
    "# remove nan vals\n",
    "unique_location_geo_data_df = unique_location_geo_data_df[unique_location_geo_data_df.loc[:, 'location'].apply(lambda x: type(x) is str)]\n",
    "display(unique_location_geo_data_df.head(20))\n",
    "display(unique_location_geo_data_df.loc[:, 'country_code'].value_counts().head(20))\n",
    "display(unique_location_geo_data_df.loc[:, 'state'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not great!! But we have to do something with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    972\n",
       "F    323\n",
       "Name: norm_gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30+    129\n",
       "<30    120\n",
       "Name: norm_age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "US        1324\n",
       "non_US     927\n",
       "Name: norm_location, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## normalize all the demographics\n",
    "# gender\n",
    "gender_norm_lookup = {\n",
    "    'man' : 'M',\n",
    "    'male' : 'M',\n",
    "    'woman' : 'F',\n",
    "    'female' : 'F',\n",
    "}\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_gender' : flat_author_attribute_data.loc[:, 'gender'].apply(gender_norm_lookup.get)\n",
    "})\n",
    "# age\n",
    "age_cutoff = 30\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_age' : flat_author_attribute_data.loc[:, 'age'].apply(lambda x: x if np.isnan(x) else f'{age_cutoff}+' if x >= age_cutoff else f'<{age_cutoff}')\n",
    "})\n",
    "# location\n",
    "location_country_lookup = dict(zip(unique_location_geo_data_df.loc[:, 'location'].values, \n",
    "                                   unique_location_geo_data_df.loc[:, 'country_code'].values))\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_location' : flat_author_attribute_data.loc[:, 'location'].apply(location_country_lookup.get)\n",
    "})\n",
    "# simplify US vs. non-US\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_location' : flat_author_attribute_data.loc[:, 'norm_location'].apply(lambda x: 'US' if x=='us' else 'non_US' if type(x) is str else None)\n",
    "})\n",
    "## show all distributions\n",
    "demo_vars = ['norm_gender', 'norm_age', 'norm_location']\n",
    "for demo_var_i in demo_vars:\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    display(flat_author_attribute_data.loc[:, demo_var_i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge with the full question data and see if we can differentiate the groups based on questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n",
      "demo = norm_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    1613\n",
       "F     513\n",
       "Name: norm_gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<30    177\n",
       "30+    173\n",
       "Name: norm_age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "US        2019\n",
       "non_US    1359\n",
       "Name: norm_location, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "valid_submission_question_data = pd.merge(valid_submission_question_data,\n",
    "                                          flat_author_attribute_data.rename(columns={'author' : 'author_comment'}).loc[:, ['author_comment',]+demo_vars],\n",
    "                                          on='author_comment', how='inner')\n",
    "print(valid_submission_question_data.shape[0])\n",
    "for demo_var_i in demo_vars:\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    display(valid_submission_question_data.loc[:, demo_var_i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less terrible than I thought. Let's look for some differences in question asking!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n",
      "top words for val = M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "produce       2.060049\n",
       "natural       1.991056\n",
       "probably      1.991056\n",
       "law           1.836905\n",
       "price         1.749894\n",
       "away          1.749894\n",
       "factor        1.749894\n",
       "antibodies    1.654584\n",
       "diet          1.654584\n",
       "reduce        1.654584\n",
       "use           1.654584\n",
       "tests         1.654584\n",
       "agree         1.654584\n",
       "caused        1.549223\n",
       "chance        1.549223\n",
       "man           1.549223\n",
       "plastic       1.549223\n",
       "jobs          1.549223\n",
       "keep          1.549223\n",
       "outside       1.549223\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fish              -2.439761\n",
       "14                -2.034296\n",
       "obese             -2.034296\n",
       "pain              -2.034296\n",
       "alone             -2.034296\n",
       "institutions      -2.034296\n",
       "inquisitiveness   -2.034296\n",
       "proof             -2.034296\n",
       "risks             -2.034296\n",
       "games             -2.034296\n",
       "americans         -2.034296\n",
       "abstract          -2.034296\n",
       "careful           -2.034296\n",
       "binaural          -2.034296\n",
       "beats             -2.034296\n",
       "infected          -2.034296\n",
       "regulation        -1.746614\n",
       "reduces           -1.746614\n",
       "headed            -1.746614\n",
       "evaluate          -1.746614\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n",
      "top words for val = <30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "risk       1.769956\n",
       "based      1.587634\n",
       "lack       1.587634\n",
       "..         1.587634\n",
       "white      1.587634\n",
       "warmer     1.587634\n",
       "100        1.587634\n",
       "back       1.364490\n",
       "stream     1.364490\n",
       "things     1.364490\n",
       "virus      1.364490\n",
       "number     1.364490\n",
       "old        1.364490\n",
       "useful     1.364490\n",
       "certain    1.364490\n",
       "gun        1.364490\n",
       "gulf       1.364490\n",
       "natural    1.364490\n",
       "theory     1.364490\n",
       "correct    1.364490\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = 30+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "next         -1.967714\n",
       "country      -1.631242\n",
       "trying       -1.631242\n",
       "help         -1.631242\n",
       "please       -1.631242\n",
       "everything   -1.408098\n",
       "basic        -1.408098\n",
       "making       -1.408098\n",
       "found        -1.408098\n",
       "getting      -1.408098\n",
       "say          -1.408098\n",
       "explain      -1.408098\n",
       "happens      -1.408098\n",
       "may          -1.408098\n",
       "true         -1.408098\n",
       "single       -1.408098\n",
       "plastic      -1.408098\n",
       "feel         -1.408098\n",
       "seen         -1.120416\n",
       "seem         -1.120416\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n",
      "top words for val = US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "support      2.379986\n",
       "blood        2.138824\n",
       "policy       2.138824\n",
       "born         2.043514\n",
       "super        1.938153\n",
       "private      1.820370\n",
       "gain         1.820370\n",
       "play         1.820370\n",
       "hair         1.820370\n",
       "universe     1.820370\n",
       "ocean        1.755831\n",
       "state        1.755831\n",
       "combat       1.686839\n",
       "basis        1.686839\n",
       "single       1.686839\n",
       "meant        1.686839\n",
       "serious      1.686839\n",
       "changing     1.686839\n",
       "stress       1.686839\n",
       "naturally    1.686839\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = non_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "discrimination   -2.204982\n",
       "bring            -2.204982\n",
       "stereotypes      -2.050831\n",
       "online           -2.050831\n",
       "therapy          -2.050831\n",
       "substance        -1.868509\n",
       "~                -1.868509\n",
       "underlying       -1.868509\n",
       "unhealthy        -1.868509\n",
       "authority        -1.868509\n",
       "parties          -1.868509\n",
       "behaviour        -1.868509\n",
       "racial           -1.868509\n",
       "build            -1.868509\n",
       "union            -1.868509\n",
       "somewhere        -1.868509\n",
       "chinese          -1.868509\n",
       "victim           -1.868509\n",
       "1000             -1.868509\n",
       "fuck             -1.868509\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import twitter_data_helpers\n",
    "reload(twitter_data_helpers)\n",
    "from twitter_data_helpers import compute_log_odds\n",
    "text_var = 'reply_question'\n",
    "top_k_words = 20\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    (val_1_i, val_2_i), word_ratio_i = compute_log_odds(data_i, text_var, demo_var_i)\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    print(f'top words for val = {val_1_i}')\n",
    "    display(word_ratio_i.head(top_k_words))\n",
    "    print(f'top words for val = {val_2_i}')\n",
    "    display(word_ratio_i.sort_values(ascending=True).head(top_k_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gender\n",
    "    - `male`: economics (`price`, `jobs`), health (`antibodies`, `diet`), male (`man`), discussion (`agree`, `expect`), causality (`produce`, `law`, `factor`, `reduce`, `caused`)\n",
    "    - `female`: organizations (`americans`, `institutions`, `regulation`), health (`infected`, `obese`), caution (`proof`, `risks`, `careful`, `evaluate`), negative experience (`pain`, `alone`)\n",
    "- Age\n",
    "    - `<30`: theoretical (`theory`, `natural`), numeric (`100`, `number`), certainty (`certain`, `correct`, `useful`)\n",
    "    - `30+`: discussion (`please`, `explain`, `say`), uncertainty (`trying`, `may`, `feel`, `seem`), simplicity (`basic`, `everything`, `single`)\n",
    "- Location\n",
    "    - `US`: nature (`universe`, `ocean`), body (`blood`, `born`, `hair`), conflict (`combat`, `stress`)\n",
    "    - `non_US`: social problems (`discrimination`, `stereotypes`, `underlying`, `racial`, `victim`, `unhealthy`), labor (`authority`, `union`), orthography (`~`, `behaviour`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ACHIEV         re.compile('^(abilit.*|able|accomplish.*|ace|achievable|achieve.*|achievi.*|acquir.*|acquisition.*|actualiz.*|adequa.*|advanc.*|advantag.*|ahead|ambition|ambitions|ambitious|ambitiously|ambitiousness|attain|atta)\n",
       "ADJ            re.compile(\"^(abnormal.*|academic|active|additional|affordable|afraid|after|aggressive|agreeable|alike|alive|alone|amazing|ambitious|ancient|angrier|angriest|angry|annoying|antisocial|anxious|apparent|approachab)\n",
       "ADVERB         re.compile(\"^(about|absolutely|actually|again|almost|already|also|anyway.*|anywhere|apparently|around|awhile|back|barely|basically|beyond|briefly|clearly|commonly|completely|constantly|continually|definitely|esp)\n",
       "AFFECT         re.compile(\"^(abandon.*|abuse.*|abusi.*|accept|accepta.*|accepted|accepting|accepts|ache.*|aching.*|active|actively|admir.*|ador.*|advantag.*|adventur.*|advers.*|affection.*|afraid|aggravat.*|aggress|aggressed|a)\n",
       "AFFILIATION    re.compile(\"^(accompan.*|accomplice.*|affil.*|alliance.*|allies|ally|amigo.*|associate|associates|associating|association|associations|bae|banter.*|belong.*|bestfriend.*|bf|bff.*|bfs|boyfriend.*|breakup|bro|bro')\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^(abilit.*|able|accomplish.*|ace|achievable|achieve.*|achievi.*|acquir.*|acquisition.*|actualiz.*|adequa.*|advanc.*|advantag.*|ahead|ambition|ambitions|ambitious|ambitiously|ambitiousness|attain|attainable|attained|attaining|attainment|attains|authorit.*|award.*|beat|beaten|best|better|bonus.*|burnout.*|capab.*|celebrat.*|challeng.*|champ.*|cheat.*|climb.*|compet.*|confidence|confident|confidently|conquer.*|conscientious.*|create|created|creates|creating|creation|creations|creative|creativity|defeat.*|demot.*|determina.*|determined|diligen.*|domina.*|driven|dropout.*|earn|earned|earning|earns|efficien.*|effort.*|elit.*|emptier|emptiest|emptiness|empty|enabl.*|endeav.*|excel|excellent|excels|fail.*|finaliz.*|first|firsts|flunk.*|founded|founder.*|founding|fulfill.*|gain.*|glory|goal.*|gpa|honor.*|honour.*|ideal.*|importance|improve.*|improving|inadequa.*|incapab.*|incentive.*|incompeten.*|ineffect.*|initiat.*|irresponsible.*|lazier|laziest|lazy|lead|leader.*|leading|leads|limit.*|lose|loser.*|loses|losing|loss.*|lost|mastered|mastery|medal.*|mediocr.*|motiv.*|obtain|obtainable|obtained|obtaining|obtains|opportun.*|overcame|overcome|overcomes|overcoming|overconfiden.*|overtak.*|perfected|perfecting|perfection|perfectly|perfects|persever.*|persist.*|plan|planned|planning|plans|potential.*|powerful|powerless.*|practice|practiced|practices|practicing|prais.*|pride|prize.*|proficien.*|progress|promot.*|proud|prouder|proudest|proudly|purpose.*|queen|quit|quitt.*|rank|ranked|ranking|ranks|recover.*|resolv.*|resourceful.*|reward.*|skill.*|solution.*|solve|solved|solves|solving|strateg.*|striv.*|succeed.*|success|successes|successful|successfully|super|superb.*|surpass.*|surviv.*|team.*|top|tried|tries|triumph.*|try|trying|unable|unbeat.*|unproduc.*|unsuccessful.*|victor.*|win|winn.*|wins|won|work|workabl.*|worked|worker.*|working|works)$\n"
     ]
    }
   ],
   "source": [
    "## same thing but LIWC categories\n",
    "import re\n",
    "LIWC_data = pd.read_csv('/home/cfwelch/LIWC.2015.all', sep=',', header=None)\n",
    "LIWC_data.columns = ['word', 'category']\n",
    "LIWC_data = LIWC_data.assign(**{'word' : LIWC_data.loc[:, 'word'].apply(lambda x: x.replace('*', '.*').strip())})\n",
    "def try_compile(x):\n",
    "    try:\n",
    "        return re.compile(x)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "LIWC_data = LIWC_data.assign(**{'word_matcher' : LIWC_data.loc[:, 'word'].apply(try_compile)})\n",
    "LIWC_data = LIWC_data[LIWC_data.loc[:, 'word_matcher'].apply(lambda x: x is not None)]\n",
    "LIWC_combined_word_patterns = LIWC_data.groupby('category').apply(lambda x: re.compile('^(' + '|'.join(x.loc[:, 'word_matcher'].apply(lambda y: y.pattern)) + ')$'))\n",
    "# get rid of bad categories\n",
    "LIWC_filter_categories = ['NETSPEAK']\n",
    "LIWC_combined_word_patterns.drop(LIWC_filter_categories, inplace=True)\n",
    "display(LIWC_combined_word_patterns.head())\n",
    "print(LIWC_combined_word_patterns.loc['ACHIEV'].pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n",
      "top words for val = M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THEY           1.223712\n",
       "YOU            0.792929\n",
       "INGEST         0.605466\n",
       "MONEY          0.465764\n",
       "FOCUSFUTURE    0.340646\n",
       "SPACE          0.263618\n",
       "ACHIEV         0.261436\n",
       "MALE           0.250262\n",
       "MOTION         0.247783\n",
       "NUMBER         0.244507\n",
       "ASSENT         0.220409\n",
       "PPRON          0.191161\n",
       "WE             0.182258\n",
       "QUANT          0.173889\n",
       "RELATIV        0.170115\n",
       "PREP           0.156940\n",
       "SEE            0.155540\n",
       "CAUSE          0.143676\n",
       "DISCREP        0.119369\n",
       "HEAR           0.114230\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SHEHE         -1.078874\n",
       "FAMILY        -0.593366\n",
       "HOME          -0.422094\n",
       "FILLER        -0.385726\n",
       "FEEL          -0.365524\n",
       "SAD           -0.331659\n",
       "HEALTH        -0.306193\n",
       "DIFFER        -0.303355\n",
       "INTERROG      -0.283944\n",
       "AFFILIATION   -0.278628\n",
       "DEATH         -0.267943\n",
       "NEGEMO        -0.246174\n",
       "RELIG         -0.238090\n",
       "IPRON         -0.226903\n",
       "RISK          -0.208392\n",
       "ANX           -0.168313\n",
       "SEXUAL        -0.162583\n",
       "AFFECT        -0.134548\n",
       "LEISURE       -0.119458\n",
       "ANGER         -0.105424\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n",
      "top words for val = <30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RELIG       0.883518\n",
       "FEEL        0.798360\n",
       "RISK        0.506224\n",
       "POSEMO      0.420179\n",
       "NEGATE      0.392895\n",
       "PREP        0.354674\n",
       "ADVERB      0.329568\n",
       "HEALTH      0.323902\n",
       "INTERROG    0.259364\n",
       "DISCREP     0.259364\n",
       "COMPARE     0.253943\n",
       "INGEST      0.236891\n",
       "ANX         0.236891\n",
       "NONFLU      0.218542\n",
       "QUANT       0.203274\n",
       "DIFFER      0.203274\n",
       "CERTAIN     0.190371\n",
       "ADJ         0.181932\n",
       "AFFECT      0.180470\n",
       "BIO         0.138874\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = 30+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FRIEND        -0.944609\n",
       "HEAR          -0.811078\n",
       "SHEHE         -0.656927\n",
       "SWEAR         -0.656927\n",
       "I             -0.656927\n",
       "FILLER        -0.656927\n",
       "LEISURE       -0.620560\n",
       "MALE          -0.474606\n",
       "MONEY         -0.426404\n",
       "AFFILIATION   -0.320455\n",
       "FOCUSPAST     -0.295137\n",
       "SOCIAL        -0.257852\n",
       "FEMALE        -0.251462\n",
       "PPRON         -0.251462\n",
       "HOME          -0.251462\n",
       "IPRON         -0.251462\n",
       "ANGER         -0.238217\n",
       "PRONOUN       -0.233444\n",
       "ACHIEV        -0.232044\n",
       "FOCUSFUTURE   -0.197395\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n",
      "top words for val = US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THEY           1.121185\n",
       "WE             0.455437\n",
       "FAMILY         0.355718\n",
       "PPRON          0.303566\n",
       "ANX            0.237684\n",
       "SAD            0.221844\n",
       "REWARD         0.197831\n",
       "RISK           0.178854\n",
       "FILLER         0.176724\n",
       "FOCUSFUTURE    0.175280\n",
       "HOME           0.173396\n",
       "MONEY          0.172748\n",
       "ASSENT         0.147736\n",
       "INFORMAL       0.136064\n",
       "I              0.133799\n",
       "MALE           0.119737\n",
       "WORK           0.107314\n",
       "DISCREP        0.102000\n",
       "NONFLU         0.098081\n",
       "MOTION         0.075948\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = non_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SHEHE         -0.670574\n",
       "CONJ          -0.370916\n",
       "SEXUAL        -0.360913\n",
       "INTERROG      -0.287582\n",
       "ANGER         -0.255961\n",
       "QUANT         -0.205378\n",
       "SEE           -0.189106\n",
       "BODY          -0.152182\n",
       "NEGATE        -0.144481\n",
       "DIFFER        -0.141730\n",
       "DEATH         -0.121527\n",
       "BIO           -0.116229\n",
       "HEALTH        -0.113288\n",
       "NUMBER        -0.104582\n",
       "FEMALE        -0.100029\n",
       "AFFILIATION   -0.097475\n",
       "CERTAIN       -0.095210\n",
       "PERCEPT       -0.074700\n",
       "HEAR          -0.070020\n",
       "ADVERB        -0.060119\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import twitter_data_helpers\n",
    "reload(twitter_data_helpers)\n",
    "from twitter_data_helpers import compute_log_odds\n",
    "text_var = 'reply_question'\n",
    "top_k_words = 20\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    (val_1_i, val_2_i), word_ratio_i = compute_log_odds(data_i, text_var, demo_var_i, word_categories=LIWC_combined_word_patterns)\n",
    "    val_1_counts_i = word_ratio_i[word_ratio_i > 0.].head(top_k_words)\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    print(f'top words for val = {val_1_i}')\n",
    "    display(val_1_counts_i)\n",
    "    val_2_counts_i = word_ratio_i[word_ratio_i < 0.].sort_values(ascending=True).head(top_k_words)\n",
    "    print(f'top words for val = {val_2_i}')\n",
    "    display(val_2_counts_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gender\n",
    "    - `male`: others (`THEY`, `YOU`), quantity (`MONEY`, `NUMBER`, `QUANT`), self (`MALE`, `WE`, `PPRON`)\n",
    "    - `female`: close relations (`SHEHE`, `FAMILY`, `HOME`), emotion (`SAD`, `FEEL`, `NEGEMO`), health (`HEALTH`, `DEATH`)\n",
    "- Age\n",
    "    - `<30`: emotion (`POSEMO`, `FEEL`, `AFFECT`), health (`HEALTH`, `INGEST`, `BIO`), certainty (`RISK`, `COMPARE`, `CERTAIN`)\n",
    "    - `30+`: social (`FRIEND`, `SHEHE`, `MALE`, `SOCIAL`), negative emotion (`NEGEMO`, `ANGER`), social institutions (`MOENY`, `AFFILIATION`), time (`FOCUSPAST`, `FOCUSFUTURE`), comfort (`LEISURE`, `HOME`)\n",
    "- Location\n",
    "    - `US`: social (`THEY`, `WE`, `FAMILY`, `PPRON`), negative emotion (`SAD`, `ANX`), action (`RISK`, `FOCUSFUTURE`, `WORK`, `REWARD`)\n",
    "    - `non_US`: immediate social (`SHEHE`), health (`SEXUAL`, `BODY`, `BIO`, `HEALTH`, `DEATH`), quantity (`QUANT`, `NUMBER`), evidence (`INTERROG`, `SEE`, `NEGATE`, `CERTAIN`, `PERCEPT`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some example questions for each group? Let's see if these would actually be useful for writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_gender; val=F\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pf6roe</td>\n",
       "      <td>The Pandemic Caused a Baby Bust, Not a Boom</td>\n",
       "      <td>Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?</td>\n",
       "      <td>Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pdxkgp</td>\n",
       "      <td>'We can build a real time machine'</td>\n",
       "      <td>I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right? So I'm guessing they never figure it out.</td>\n",
       "      <td>I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>pdcrj0</td>\n",
       "      <td>Scientists at the US Department of Energy’s laser facility shattered their own record earlier this month by generating more than 10 quadrillion watts of fusion power for a fraction of a second — roughly 700 times the generating capacity of the entire US electrical grid at any given moment</td>\n",
       "      <td>So could they do this again and connect it to a bunch of batteries this time?</td>\n",
       "      <td>So could they do this again and connect it to a bunch of batteries this time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>nuf1ej</td>\n",
       "      <td>Spanking has effects on early childhood behavior similar to those of adverse childhood experiences (ACEs) such as physical or emotional abuse or neglect, parental mental illness, parental substance use, and others, a study in the Journal of Pediatrics has found</td>\n",
       "      <td>As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?</td>\n",
       "      <td>As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ikuz9f</td>\n",
       "      <td>Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)</td>\n",
       "      <td>Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?</td>\n",
       "      <td>Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ikuz9f</td>\n",
       "      <td>Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)</td>\n",
       "      <td>Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?</td>\n",
       "      <td>Isn't that why everyone should be wearing a mask?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>p92ahi</td>\n",
       "      <td>People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon</td>\n",
       "      <td>This is really scary in itself - but what happens to those who get sick with COVID again? Further deficits?</td>\n",
       "      <td>This is really scary in itself - but what happens to those who get sick with COVID again?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>oxw9y7</td>\n",
       "      <td>Scientists were surprised when mice they were treating for diabetes dropped half their weight and developed extra-shiny coats. The cytokine they had administered led to fast fat-loss via an oily substance we secrete through skin - and could point toward future treatments for obesity and skin issues.</td>\n",
       "      <td>Let me guess - this will get developed and marketed by a company named Adipose Industries?</td>\n",
       "      <td>Let me guess - this will get developed and marketed by a company named Adipose Industries?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>mn2fqq</td>\n",
       "      <td>First evidence that dogs can mentally represent jealousy: Some researchers have suggested that jealousy is linked to self-awareness and theory of mind, leading to claims that it is unique to humans. A new study found evidence for three signatures of jealous behavior in dogs.</td>\n",
       "      <td>Who hasn't lavished attention on a stuffed animal to get a dog jealous?</td>\n",
       "      <td>Who hasn't lavished attention on a stuffed animal to get a dog jealous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  \\\n",
       "7      pf6roe   \n",
       "16     pf5phr   \n",
       "49     pdxkgp   \n",
       "57     pdcrj0   \n",
       "58     nuf1ej   \n",
       "59     ikuz9f   \n",
       "60     ikuz9f   \n",
       "171    p92ahi   \n",
       "172    oxw9y7   \n",
       "173    mn2fqq   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            title  \\\n",
       "7                                                                                                                                                                                                                                                                     The Pandemic Caused a Baby Bust, Not a Boom   \n",
       "16                                                                                                                                                                                          Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "49                                                                                                                                                                                                                                                                             'We can build a real time machine'   \n",
       "57              Scientists at the US Department of Energy’s laser facility shattered their own record earlier this month by generating more than 10 quadrillion watts of fusion power for a fraction of a second — roughly 700 times the generating capacity of the entire US electrical grid at any given moment   \n",
       "58                                          Spanking has effects on early childhood behavior similar to those of adverse childhood experiences (ACEs) such as physical or emotional abuse or neglect, parental mental illness, parental substance use, and others, a study in the Journal of Pediatrics has found   \n",
       "59                                                                                                                        Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)   \n",
       "60                                                                                                                        Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)   \n",
       "171                                     People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon   \n",
       "172  Scientists were surprised when mice they were treating for diabetes dropped half their weight and developed extra-shiny coats. The cytokine they had administered led to fast fat-loss via an oily substance we secrete through skin - and could point toward future treatments for obesity and skin issues.   \n",
       "173                           First evidence that dogs can mentally represent jealousy: Some researchers have suggested that jealousy is linked to self-awareness and theory of mind, leading to claims that it is unique to humans. A new study found evidence for three signatures of jealous behavior in dogs.   \n",
       "\n",
       "                                                                                                                                                                                                                                  body  \\\n",
       "7                                                                                                                                 Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?   \n",
       "16                      I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.   \n",
       "49   I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right? So I'm guessing they never figure it out.   \n",
       "57                                                                                                                                                       So could they do this again and connect it to a bunch of batteries this time?   \n",
       "58                                            As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?   \n",
       "59                                 Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?   \n",
       "60                                 Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?   \n",
       "171                                                                                                                        This is really scary in itself - but what happens to those who get sick with COVID again? Further deficits?   \n",
       "172                                                                                                                                         Let me guess - this will get developed and marketed by a company named Adipose Industries?   \n",
       "173                                                                                                                                                            Who hasn't lavished attention on a stuffed animal to get a dog jealous?   \n",
       "\n",
       "                                                                                                                                                                               reply_question  \n",
       "7                                                                                        Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?  \n",
       "16                                                                        I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?  \n",
       "49    I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right?  \n",
       "57                                                                                                              So could they do this again and connect it to a bunch of batteries this time?  \n",
       "58   As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?  \n",
       "59                                          Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you?  \n",
       "60                                                                                                                                          Isn't that why everyone should be wearing a mask?  \n",
       "171                                                                                                 This is really scary in itself - but what happens to those who get sick with COVID again?  \n",
       "172                                                                                                Let me guess - this will get developed and marketed by a company named Adipose Industries?  \n",
       "173                                                                                                                   Who hasn't lavished attention on a stuffed animal to get a dog jealous?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_gender; val=M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pfb3gq</td>\n",
       "      <td>Food chemists have just invented a better chocolate using chemistry and cocoa butter to perfect the tempering process. This technique could help make this treat more sustainable by saving energy and improving its carbon footprint.</td>\n",
       "      <td>Where can one get their hands on phospholipid powder, though? Anyway, I'm only interested if it raises the melting temperature of chocolate.</td>\n",
       "      <td>Where can one get their hands on phospholipid powder, though?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p4uuhq</td>\n",
       "      <td>New findings show Bennu—one of the most hazardous known asteroids—has a 1 in 1,750 chance of impacting Earth through 2300, higher than previously thought. It could create a crater between 10 to 20 times its size and cause an area of devastation that could reach 100 times the size of the crater.</td>\n",
       "      <td>We can predict probabilities of collision that far ahead? So we don't have to worry about any Deep Impact scenario happening  in our near future, I guess.</td>\n",
       "      <td>We can predict probabilities of collision that far ahead?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p1t4an</td>\n",
       "      <td>When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.</td>\n",
       "      <td>I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.</td>\n",
       "      <td>Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p1t4an</td>\n",
       "      <td>When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.</td>\n",
       "      <td>I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.</td>\n",
       "      <td>If so, then the same is not true for domestic cats?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orstcv</td>\n",
       "      <td>The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.</td>\n",
       "      <td>This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "      <td>Why would there be any reason to expect that to be so?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>orstcv</td>\n",
       "      <td>The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.</td>\n",
       "      <td>This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "      <td>Why would we assume that produce used in deli products be the absolute freshest?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>orstcv</td>\n",
       "      <td>The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.</td>\n",
       "      <td>This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "      <td>Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>What's novel in this study? Is there a specific finding or method that the researchers are introducing? I imagine it's something to do with tightly controlling variables since it was performed in-clinic.</td>\n",
       "      <td>Is there a specific finding or method that the researchers are introducing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>oy6x6a</td>\n",
       "      <td>Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp;amp; violence.</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  \\\n",
       "0     pfb3gq   \n",
       "1     p4uuhq   \n",
       "2     p1t4an   \n",
       "3     p1t4an   \n",
       "4     orstcv   \n",
       "5     orstcv   \n",
       "6     orstcv   \n",
       "19    peudnq   \n",
       "20    peudnq   \n",
       "21    oy6x6a   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           title  \\\n",
       "0                                                                         Food chemists have just invented a better chocolate using chemistry and cocoa butter to perfect the tempering process. This technique could help make this treat more sustainable by saving energy and improving its carbon footprint.   \n",
       "1        New findings show Bennu—one of the most hazardous known asteroids—has a 1 in 1,750 chance of impacting Earth through 2300, higher than previously thought. It could create a crater between 10 to 20 times its size and cause an area of devastation that could reach 100 times the size of the crater.   \n",
       "2   When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.   \n",
       "3   When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.   \n",
       "4                                                                                                               The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.   \n",
       "5                                                                                                               The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.   \n",
       "6                                                                                                               The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.   \n",
       "19                                                                                                                                                                        Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "20                                                                                                                                                                        Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "21                                               Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp; violence.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         body  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Where can one get their hands on phospholipid powder, though? Anyway, I'm only interested if it raises the melting temperature of chocolate.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We can predict probabilities of collision that far ahead? So we don't have to worry about any Deep Impact scenario happening  in our near future, I guess.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                  I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                  I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.   \n",
       "4   This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?   \n",
       "5   This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?   \n",
       "6   This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                What's novel in this study? Is there a specific finding or method that the researchers are introducing? I imagine it's something to do with tightly controlling variables since it was performed in-clinic.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                               Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ok. Who, out there, wants more men in the world?   \n",
       "\n",
       "                                                                                                                                 reply_question  \n",
       "0                                                                                 Where can one get their hands on phospholipid powder, though?  \n",
       "1                                                                                     We can predict probabilities of collision that far ahead?  \n",
       "2                           Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them?  \n",
       "3                                                                                           If so, then the same is not true for domestic cats?  \n",
       "4                                                                                        Why would there be any reason to expect that to be so?  \n",
       "5                                                              Why would we assume that produce used in deli products be the absolute freshest?  \n",
       "6                                                                            Why would it be less expensive at a grocery store even if in bulk?  \n",
       "19                                                                  Is there a specific finding or method that the researchers are introducing?  \n",
       "20  Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?  \n",
       "21                                                                                             Ok. Who, out there, wants more men in the world?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_age; val=30+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>pcdoho</td>\n",
       "      <td>The city of Liverpool has boycotted the British tabloid, the Sun, since 1989 (due to the tabloid's coverage of the Hillsborough disaster). As a consequence, the city of Liverpool has held more favorable views towards the European Union. This suggests that the Sun played a key role in Brexit.</td>\n",
       "      <td>By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?</td>\n",
       "      <td>By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>p9yba5</td>\n",
       "      <td>Feeling like leisure is wasteful and unproductive may lead to less happiness and higher levels of stress and depression, new research suggests (Four studies, n = 1310)</td>\n",
       "      <td>Maybe more stress is required. What business does anyone have enjoying themselves while people suffer?</td>\n",
       "      <td>What business does anyone have enjoying themselves while people suffer?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>oz6erc</td>\n",
       "      <td>A study suggests that people adjusted their music listening habits as a way of coping with the COVID-19 crisis. Survey respondents reported that they increased their music listening during the initial lockdown and that they used music to help them cope with emotional challenges during the pandemic.</td>\n",
       "      <td>What does it mean if I only listen to music if someone else has it on?</td>\n",
       "      <td>What does it mean if I only listen to music if someone else has it on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>p9h593</td>\n",
       "      <td>Exercise blunts cravings for alcohol among young, problematic drinkers</td>\n",
       "      <td>Did someone already ask what is an exercise blunt?</td>\n",
       "      <td>Did someone already ask what is an exercise blunt?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>p92ahi</td>\n",
       "      <td>People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon</td>\n",
       "      <td>With SARS (and just a reminder that covid is a SARS-like virus), a year after they recovered, 20-30% said they had worse health than before the infection.   4 years later, 87% of them had seen large declines in health since then, with pretty much all of the 87% reporting *neurological issues* developing.   There is really no doubt that SARS-like viruses have impacts in the brain, mostly through the vascular system. We know that the virus is able to attach to ACE2 receptors in the vascular system, which allows it to cross the blood brain barrier. The question is, does it stay there at a low level and slowly damage the brain?</td>\n",
       "      <td>The question is, does it stay there at a low level and slowly damage the brain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>lgsqc0</td>\n",
       "      <td>Singapore, with almost 200,00 migrant workers exposed to COVID-19 and more than 111,000 confirmed infections, has had only 20 ICU patients and 1 death, because of highly effective mass testing, contact tracing and isolation, finds a new study in JAMA.</td>\n",
       "      <td>Yeah, something tells me they likely had issues with the false positivity rating of their tests.   111,000 confirmed cases and 1 single death? I considerably doubt this.</td>\n",
       "      <td>111,000 confirmed cases and 1 single death?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>p6hr9o</td>\n",
       "      <td>Histamine could be a key player in depression, according to study in mice - Findings from research at Imperial College London &amp;amp; Uni South Carolina add to mounting evidence that inflammation and the accompanying release of histamine affects a key molecule responsible for mood in the brain – serotonin</td>\n",
       "      <td>So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?</td>\n",
       "      <td>So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>k6lc30</td>\n",
       "      <td>Jupiter and Saturn will come within 0.1 degrees of each other, forming the first visible \"double planet\" in 800 years</td>\n",
       "      <td>On the west coast of the US here, where should I look to see this? Or what website should I use to find it.</td>\n",
       "      <td>On the west coast of the US here, where should I look to see this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>p5tmfv</td>\n",
       "      <td>Swiss researchers calculate pi to new record of 62.8tn figures. Supercomputer calculation took 108 days and nine hours – 3.5 times as fast as previous record</td>\n",
       "      <td>The picture in the article shows Pi to 90 digits past the decimal. Is there any practical use for 62.8tn? The article does mention RNA analysis.</td>\n",
       "      <td>Is there any practical use for 62.8tn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>p5jeib</td>\n",
       "      <td>Pfizer pharmacokinetics and toxicity – a study on mRNA vaccine clotting in test subjects</td>\n",
       "      <td>This reads like a hit piece.  I'm used to scientific papers that are neutral in tone and state the facts they've discovered, and this one is laced with emotion-laden language and conclusions.  Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?</td>\n",
       "      <td>Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  \\\n",
       "81     pcdoho   \n",
       "129    p9yba5   \n",
       "130    oz6erc   \n",
       "163    p9h593   \n",
       "177    p92ahi   \n",
       "178    lgsqc0   \n",
       "223    p6hr9o   \n",
       "224    k6lc30   \n",
       "239    p5tmfv   \n",
       "245    p5jeib   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                title  \\\n",
       "81               The city of Liverpool has boycotted the British tabloid, the Sun, since 1989 (due to the tabloid's coverage of the Hillsborough disaster). As a consequence, the city of Liverpool has held more favorable views towards the European Union. This suggests that the Sun played a key role in Brexit.   \n",
       "129                                                                                                                                           Feeling like leisure is wasteful and unproductive may lead to less happiness and higher levels of stress and depression, new research suggests (Four studies, n = 1310)   \n",
       "130       A study suggests that people adjusted their music listening habits as a way of coping with the COVID-19 crisis. Survey respondents reported that they increased their music listening during the initial lockdown and that they used music to help them cope with emotional challenges during the pandemic.   \n",
       "163                                                                                                                                                                                                                                            Exercise blunts cravings for alcohol among young, problematic drinkers   \n",
       "177                                         People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon   \n",
       "178                                                       Singapore, with almost 200,00 migrant workers exposed to COVID-19 and more than 111,000 confirmed infections, has had only 20 ICU patients and 1 death, because of highly effective mass testing, contact tracing and isolation, finds a new study in JAMA.   \n",
       "223  Histamine could be a key player in depression, according to study in mice - Findings from research at Imperial College London &amp; Uni South Carolina add to mounting evidence that inflammation and the accompanying release of histamine affects a key molecule responsible for mood in the brain – serotonin   \n",
       "224                                                                                                                                                                                             Jupiter and Saturn will come within 0.1 degrees of each other, forming the first visible \"double planet\" in 800 years   \n",
       "239                                                                                                                                                     Swiss researchers calculate pi to new record of 62.8tn figures. Supercomputer calculation took 108 days and nine hours – 3.5 times as fast as previous record   \n",
       "245                                                                                                                                                                                                                          Pfizer pharmacokinetics and toxicity – a study on mRNA vaccine clotting in test subjects   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body  \\\n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?   \n",
       "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Maybe more stress is required. What business does anyone have enjoying themselves while people suffer?   \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   What does it mean if I only listen to music if someone else has it on?   \n",
       "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Did someone already ask what is an exercise blunt?   \n",
       "177  With SARS (and just a reminder that covid is a SARS-like virus), a year after they recovered, 20-30% said they had worse health than before the infection.   4 years later, 87% of them had seen large declines in health since then, with pretty much all of the 87% reporting *neurological issues* developing.   There is really no doubt that SARS-like viruses have impacts in the brain, mostly through the vascular system. We know that the virus is able to attach to ACE2 receptors in the vascular system, which allows it to cross the blood brain barrier. The question is, does it stay there at a low level and slowly damage the brain?   \n",
       "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Yeah, something tells me they likely had issues with the false positivity rating of their tests.   111,000 confirmed cases and 1 single death? I considerably doubt this.   \n",
       "223                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?   \n",
       "224                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              On the west coast of the US here, where should I look to see this? Or what website should I use to find it.   \n",
       "239                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The picture in the article shows Pi to 90 digits past the decimal. Is there any practical use for 62.8tn? The article does mention RNA analysis.   \n",
       "245                                                                                                                                                                                                                                                                                                                                                      This reads like a hit piece.  I'm used to scientific papers that are neutral in tone and state the facts they've discovered, and this one is laced with emotion-laden language and conclusions.  Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?   \n",
       "\n",
       "                                                                                                               reply_question  \n",
       "81   By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?  \n",
       "129                                                   What business does anyone have enjoying themselves while people suffer?  \n",
       "130                                                    What does it mean if I only listen to music if someone else has it on?  \n",
       "163                                                                        Did someone already ask what is an exercise blunt?  \n",
       "177                                           The question is, does it stay there at a low level and slowly damage the brain?  \n",
       "178                                                                               111,000 confirmed cases and 1 single death?  \n",
       "223                               So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?  \n",
       "224                                                        On the west coast of the US here, where should I look to see this?  \n",
       "239                                                                                    Is there any practical use for 62.8tn?  \n",
       "245                        Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_age; val=<30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>pd3eo2</td>\n",
       "      <td>We continue to evolve: more and more human beings are born with an extra artery in the arm</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>nayk3b</td>\n",
       "      <td>For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.</td>\n",
       "      <td>Hey kids. I just thought of a dumb joke you can try on your dads:  &amp;gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &amp;gt;  &amp;gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &amp;gt;  &amp;gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?</td>\n",
       "      <td>Oh god, what am I even doing with my life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>p7cnwt</td>\n",
       "      <td>Substituting only 10% of daily caloric intake of beef and processed meats for a diverse mix of whole grains, fruits, vegetables, nuts, legumes and select seafood could reduce, on average, the dietary carbon footprint of a U.S. consumer by one-third and add 48 healthy minutes of life per day.</td>\n",
       "      <td>Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides? Don't talk to me until you're eating nothing but farmed bugs for protein and veggies grown indoors in hydroponic skyscrapers</td>\n",
       "      <td>Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>p31nv2</td>\n",
       "      <td>Ethnic Group in the Philippines Have Highest Level of Denisovan DNA: The Ayta Magbukon, who live in Central Luzon, were found to have higher levels of DNA from the extinct hominid Denisovan subspecies than any previously recorded.</td>\n",
       "      <td>I wonder if they have Genghis  Khan’s DNA as well?</td>\n",
       "      <td>I wonder if they have Genghis  Khan’s DNA as well?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>o0f1ag</td>\n",
       "      <td>For reasons unknown, Earth’s solid-iron inner core is growing faster on one side than the other, and it has been ever since it started to freeze out from molten iron more than half a billion years ago, according to a new study by seismologists at the University of California, Berkeley.</td>\n",
       "      <td>Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?</td>\n",
       "      <td>Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>jbjmyy</td>\n",
       "      <td>Children whose outdoor play areas were transformed from gravel yards to mini-forests showed improved immune systems within a month, research has shown.</td>\n",
       "      <td>Would this have something to do with why some folks are at higher risk of death from certain medical concerns? Less access to health care and healthy environments?</td>\n",
       "      <td>Would this have something to do with why some folks are at higher risk of death from certain medical concerns?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>p1mr5y</td>\n",
       "      <td>Climate change ‘double whammy’ could kill off fish species. A new study of 150 million years of fish evolution provides first evidence to support scientific theory that commonly-eaten fish species will become smaller as waters warm under climate change.</td>\n",
       "      <td>I want to be clear before I ask this that I 100% believe in climate change and we 1000% need to do something about it yesterday. This is a question relating to my understanding and interest in fish biology.   A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?</td>\n",
       "      <td>A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>oykg1y</td>\n",
       "      <td>Climate crisis: Scientists spot warning signs of Gulf Stream collapse</td>\n",
       "      <td>Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno</td>\n",
       "      <td>Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>oykg1y</td>\n",
       "      <td>Climate crisis: Scientists spot warning signs of Gulf Stream collapse</td>\n",
       "      <td>Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno</td>\n",
       "      <td>will Maine be warmer some day because of this?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  \\\n",
       "16     pf5phr   \n",
       "62     pd3eo2   \n",
       "63     nayk3b   \n",
       "213    p7cnwt   \n",
       "322    p31nv2   \n",
       "323    o0f1ag   \n",
       "324    jbjmyy   \n",
       "363    p1mr5y   \n",
       "413    oykg1y   \n",
       "414    oykg1y   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    title  \\\n",
       "16                                                                                                                                                                                  Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "62                                                                                                                                                                                                             We continue to evolve: more and more human beings are born with an extra artery in the arm   \n",
       "63                                                                                 For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.   \n",
       "213  Substituting only 10% of daily caloric intake of beef and processed meats for a diverse mix of whole grains, fruits, vegetables, nuts, legumes and select seafood could reduce, on average, the dietary carbon footprint of a U.S. consumer by one-third and add 48 healthy minutes of life per day.   \n",
       "322                                                                Ethnic Group in the Philippines Have Highest Level of Denisovan DNA: The Ayta Magbukon, who live in Central Luzon, were found to have higher levels of DNA from the extinct hominid Denisovan subspecies than any previously recorded.   \n",
       "323        For reasons unknown, Earth’s solid-iron inner core is growing faster on one side than the other, and it has been ever since it started to freeze out from molten iron more than half a billion years ago, according to a new study by seismologists at the University of California, Berkeley.   \n",
       "324                                                                                                                                               Children whose outdoor play areas were transformed from gravel yards to mini-forests showed improved immune systems within a month, research has shown.   \n",
       "363                                         Climate change ‘double whammy’ could kill off fish species. A new study of 150 million years of fish evolution provides first evidence to support scientific theory that commonly-eaten fish species will become smaller as waters warm under climate change.   \n",
       "413                                                                                                                                                                                                                                 Climate crisis: Scientists spot warning signs of Gulf Stream collapse   \n",
       "414                                                                                                                                                                                                                                 Climate crisis: Scientists spot warning signs of Gulf Stream collapse   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
       "16                                                                                                                                                                                                          I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.   \n",
       "62                                                                                                                                                                                                                                                                                            Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.   \n",
       "63   Hey kids. I just thought of a dumb joke you can try on your dads:  &gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &gt;  &gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &gt;  &gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?   \n",
       "213                                                                                                                                                                                             Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides? Don't talk to me until you're eating nothing but farmed bugs for protein and veggies grown indoors in hydroponic skyscrapers   \n",
       "322                                                                                                                                                                                                                                                                                                                                                                     I wonder if they have Genghis  Khan’s DNA as well?   \n",
       "323                                                                                                                                                                                                                                                                                                        Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?   \n",
       "324                                                                                                                                                                                                                                                    Would this have something to do with why some folks are at higher risk of death from certain medical concerns? Less access to health care and healthy environments?   \n",
       "363                                                                         I want to be clear before I ask this that I 100% believe in climate change and we 1000% need to do something about it yesterday. This is a question relating to my understanding and interest in fish biology.   A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?   \n",
       "413                                                                                                                     Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno   \n",
       "414                                                                                                                     Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno   \n",
       "\n",
       "                                                                                                                                                                    reply_question  \n",
       "16                                                             I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?  \n",
       "62                                                                                                                         Is there an easy way to tell if I have a second artery?  \n",
       "63                                                                                                                                      Oh god, what am I even doing with my life?  \n",
       "213                                                                                  Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides?  \n",
       "322                                                                                                                             I wonder if they have Genghis  Khan’s DNA as well?  \n",
       "323                                                                Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?  \n",
       "324                                                                 Would this have something to do with why some folks are at higher risk of death from certain medical concerns?  \n",
       "363                                                  A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?  \n",
       "413  Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  \n",
       "414                                                                                                                                 will Maine be warmer some day because of this?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_location; val=US\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab? A collaboration with abortion clinics and hospitals around the country sounds like a win to me.</td>\n",
       "      <td>why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?</td>\n",
       "      <td>14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>oy6x6a</td>\n",
       "      <td>Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp;amp; violence.</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>pdpt3h</td>\n",
       "      <td>This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"</td>\n",
       "      <td>Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?</td>\n",
       "      <td>Can this help explain that phenomenon where people are under but still conscious??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>pdpt3h</td>\n",
       "      <td>This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"</td>\n",
       "      <td>Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?</td>\n",
       "      <td>Does anyone know what I'm talking about?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>pd3eo2</td>\n",
       "      <td>We continue to evolve: more and more human beings are born with an extra artery in the arm</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>nayk3b</td>\n",
       "      <td>For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.</td>\n",
       "      <td>Hey kids. I just thought of a dumb joke you can try on your dads:  &amp;gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &amp;gt;  &amp;gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &amp;gt;  &amp;gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?</td>\n",
       "      <td>Oh god, what am I even doing with my life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>pckqep</td>\n",
       "      <td>We Think Anger Is A Sign Of Guilt — But It May Actually Be A Better Sign Of Innocence</td>\n",
       "      <td>This makes total sense to me as someone who gets absolutely livid when falsely accused of something.  In fact, I'm not sure I accept the premise of this title.  Do we generally think anger is a sign of guilt?  I usually don't.</td>\n",
       "      <td>Do we generally think anger is a sign of guilt?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ooictx</td>\n",
       "      <td>Alarming climate change: Earth heads for its tipping point as it could reach +1.5 °C over the next 5 years, WMO finds in the latest study</td>\n",
       "      <td>Why do you think these asshole billionaires are so anxious to get off this planet?</td>\n",
       "      <td>Why do you think these asshole billionaires are so anxious to get off this planet?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  \\\n",
       "8     pf5phr   \n",
       "17    pf5phr   \n",
       "20    peudnq   \n",
       "21    oy6x6a   \n",
       "55    pdpt3h   \n",
       "56    pdpt3h   \n",
       "62    pd3eo2   \n",
       "63    nayk3b   \n",
       "74    pckqep   \n",
       "75    ooictx   \n",
       "\n",
       "                                                                                                                                                                                                                                                              title  \\\n",
       "8                                                                                                                                             Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "17                                                                                                                                            Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "20                                                                                                                           Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "21  Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp; violence.   \n",
       "55                                                                                              This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"   \n",
       "56                                                                                              This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"   \n",
       "62                                                                                                                                                                       We continue to evolve: more and more human beings are born with an extra artery in the arm   \n",
       "63                                           For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.   \n",
       "74                                                                                                                                                                            We Think Anger Is A Sign Of Guilt — But It May Actually Be A Better Sign Of Innocence   \n",
       "75                                                                                                                        Alarming climate change: Earth heads for its tipping point as it could reach +1.5 °C over the next 5 years, WMO finds in the latest study   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                     body  \\\n",
       "8                                                                                                                                                                                                                   why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab? A collaboration with abortion clinics and hospitals around the country sounds like a win to me.   \n",
       "17                                                                                                                                                                                                                                                                                                                                    14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?   \n",
       "20                                                                                                                                                           Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                       Ok. Who, out there, wants more men in the world?   \n",
       "55                                                                                                                                                                                            Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?   \n",
       "56                                                                                                                                                                                            Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?   \n",
       "62                                                                                                                                                                                                                                                                                           Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.   \n",
       "63  Hey kids. I just thought of a dumb joke you can try on your dads:  &gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &gt;  &gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &gt;  &gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?   \n",
       "74                                                                                                                                                                                     This makes total sense to me as someone who gets absolutely livid when falsely accused of something.  In fact, I'm not sure I accept the premise of this title.  Do we generally think anger is a sign of guilt?  I usually don't.   \n",
       "75                                                                                                                                                                                                                                                                                                                                     Why do you think these asshole billionaires are so anxious to get off this planet?   \n",
       "\n",
       "                                                                                                                                 reply_question  \n",
       "8                                         why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab?  \n",
       "17                                                          14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?  \n",
       "20  Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?  \n",
       "21                                                                                             Ok. Who, out there, wants more men in the world?  \n",
       "55                                                           Can this help explain that phenomenon where people are under but still conscious??  \n",
       "56                                                                                                     Does anyone know what I'm talking about?  \n",
       "62                                                                                      Is there an easy way to tell if I have a second artery?  \n",
       "63                                                                                                   Oh god, what am I even doing with my life?  \n",
       "74                                                                                              Do we generally think anger is a sign of guilt?  \n",
       "75                                                           Why do you think these asshole billionaires are so anxious to get off this planet?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_location; val=non_US\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>So how long until lab embryos are legal for longer than the term for abortion? Toying around with human (to be) lives like this makes \"researchers\" more and more like doctor Frankenstein. So not be surprised that at a certain moment pharmaceuticals will be categorically rejected by certain groups, and we cannot predict which groups that will be.</td>\n",
       "      <td>So how long until lab embryos are legal for longer than the term for abortion?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>p7m46h</td>\n",
       "      <td>COVID-19 is a vascular disease not a respiratory one, says study - The study, published in the journal Circulation Research, shows with precision how virus damages the cells of the vascular system.</td>\n",
       "      <td>Yeah, I told that a year ago. I'm not even a physician, nor a biologist. It's just common sense.  If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?</td>\n",
       "      <td>If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>p6nwn9</td>\n",
       "      <td>Newly published research indicates that people who more frequently signal their victimhood (whether real, exaggerated, or false) are more likely to lie and cheat for material gain and denigrate others as a means to get ahead.</td>\n",
       "      <td>&amp;gt;Newly published ~~research~~ **open door** indicates that people who more frequently ~~signal their victimhood (whether real, exaggerated, or false)~~ **manipulate** are more likely to ~~lie and cheat~~ **manipulate** for material gain and ~~denigrate others as a means~~ **manipulate** to get ahead.  There, I fixed the title. Can anyone tell me which subreddit shows *actual* science?</td>\n",
       "      <td>Can anyone tell me which subreddit shows *actual* science?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n97dh7</td>\n",
       "      <td>67% of participants who received three MDMA-assisted therapy sessions no longer qualified for a PTSD diagnosis, results published in Nature Medicine</td>\n",
       "      <td>When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?</td>\n",
       "      <td>When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n7ebfz</td>\n",
       "      <td>Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.</td>\n",
       "      <td>&amp;gt;Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.  Isn't that the entire concept behind chemo therapy?</td>\n",
       "      <td>Isn't that the entire concept behind chemo therapy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mj5mp9</td>\n",
       "      <td>Microdosing psychedelic drugs is associated with increases in conscientiousness and reductions in neuroticism, finds a new Australian study, which may explain the positive effects on performance and psychological well-being respectively.</td>\n",
       "      <td>Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?</td>\n",
       "      <td>Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hhz1ps</td>\n",
       "      <td>New research has suggested that the MMR (measles, mumps, and rubella) vaccine can stimulate immune cells in a way that primes white blood cells to react more effectively when they encounter other unrelated infections, including SARS-coV-2</td>\n",
       "      <td>So Covid-19 is weeding out the anti-vaxxers among us?</td>\n",
       "      <td>So Covid-19 is weeding out the anti-vaxxers among us?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pf1g2g</td>\n",
       "      <td>Birth rates in many high-income countries declined in the months following the first Covid wave, possibly because of economic uncertainty, with particularly strong declines in southern Europe: Italy (−9.1%), Spain (−8.4%), and Portugal (−6.6%).</td>\n",
       "      <td>I was trying for a third, pandemic started and we decided 2 was enough. Who wants to go through pregnancy and a newborn during a pandemic? Not this girl.</td>\n",
       "      <td>Who wants to go through pregnancy and a newborn during a pandemic?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>So is this effect cumulative over life or is it okay to eat a low sucrose diet?</td>\n",
       "      <td>So is this effect cumulative over life or is it okay to eat a low sucrose diet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?</td>\n",
       "      <td>Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  \\\n",
       "9     pf5phr   \n",
       "10    p7m46h   \n",
       "11    p6nwn9   \n",
       "12    n97dh7   \n",
       "13    n7ebfz   \n",
       "14    mj5mp9   \n",
       "15    hhz1ps   \n",
       "18    pf1g2g   \n",
       "22    peudnq   \n",
       "23    peudnq   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          title  \\\n",
       "9                                                                                                                                                                                         Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "10                                                                                                        COVID-19 is a vascular disease not a respiratory one, says study - The study, published in the journal Circulation Research, shows with precision how virus damages the cells of the vascular system.   \n",
       "11                                                                            Newly published research indicates that people who more frequently signal their victimhood (whether real, exaggerated, or false) are more likely to lie and cheat for material gain and denigrate others as a means to get ahead.   \n",
       "12                                                                                                                                                         67% of participants who received three MDMA-assisted therapy sessions no longer qualified for a PTSD diagnosis, results published in Nature Medicine   \n",
       "13  Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.   \n",
       "14                                                                Microdosing psychedelic drugs is associated with increases in conscientiousness and reductions in neuroticism, finds a new Australian study, which may explain the positive effects on performance and psychological well-being respectively.   \n",
       "15                                                               New research has suggested that the MMR (measles, mumps, and rubella) vaccine can stimulate immune cells in a way that primes white blood cells to react more effectively when they encounter other unrelated infections, including SARS-coV-2   \n",
       "18                                                         Birth rates in many high-income countries declined in the months following the first Covid wave, possibly because of economic uncertainty, with particularly strong declines in southern Europe: Italy (−9.1%), Spain (−8.4%), and Portugal (−6.6%).   \n",
       "22                                                                                                                                                                       Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "23                                                                                                                                                                       Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
       "9                                              So how long until lab embryos are legal for longer than the term for abortion? Toying around with human (to be) lives like this makes \"researchers\" more and more like doctor Frankenstein. So not be surprised that at a certain moment pharmaceuticals will be categorically rejected by certain groups, and we cannot predict which groups that will be.   \n",
       "10                                                                                                                                                                         Yeah, I told that a year ago. I'm not even a physician, nor a biologist. It's just common sense.  If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?   \n",
       "11  &gt;Newly published ~~research~~ **open door** indicates that people who more frequently ~~signal their victimhood (whether real, exaggerated, or false)~~ **manipulate** are more likely to ~~lie and cheat~~ **manipulate** for material gain and ~~denigrate others as a means~~ **manipulate** to get ahead.  There, I fixed the title. Can anyone tell me which subreddit shows *actual* science?   \n",
       "12                                                                                                                                                                                                                                                                                 When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?   \n",
       "13                                    &gt;Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.  Isn't that the entire concept behind chemo therapy?   \n",
       "14                                                                                                                                                                                                                                                                                    Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?   \n",
       "15                                                                                                                                                                                                                                                                                                                                                   So Covid-19 is weeding out the anti-vaxxers among us?   \n",
       "18                                                                                                                                                                                                                                               I was trying for a third, pandemic started and we decided 2 was enough. Who wants to go through pregnancy and a newborn during a pandemic? Not this girl.   \n",
       "22                                                                                                                                                                                                                                                                                                                         So is this effect cumulative over life or is it okay to eat a low sucrose diet?   \n",
       "23                                                                                                                                                                                                                                                                                 Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?   \n",
       "\n",
       "                                                                                                                   reply_question  \n",
       "9                                                  So how long until lab embryos are legal for longer than the term for abortion?  \n",
       "10  If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?  \n",
       "11                                                                     Can anyone tell me which subreddit shows *actual* science?  \n",
       "12        When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?  \n",
       "13                                                                            Isn't that the entire concept behind chemo therapy?  \n",
       "14           Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?  \n",
       "15                                                                          So Covid-19 is weeding out the anti-vaxxers among us?  \n",
       "18                                                             Who wants to go through pregnancy and a newborn during a pandemic?  \n",
       "22                                                So is this effect cumulative over life or is it okay to eat a low sucrose diet?  \n",
       "23        Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    for demo_val_j, data_j in data_i.groupby(demo_var_i):\n",
    "        print(f'demo={demo_var_i}; val={demo_val_j}')\n",
    "        display(data_j.loc[:, ['parent_id', 'title', 'body', 'reply_question']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the questions are off-topic but most of them are generally relevant to the original post topic.\n",
    "\n",
    "Let's try to classify them with a basic BERT classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to file\n",
    "valid_submission_question_data.to_csv('science_submission_question_reply_author_attr_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-author classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch import tensor, Tensor\n",
    "np.random.seed(123)\n",
    "def resample_data_by_class(data, label_var):\n",
    "    class_counts = data.loc[:, label_var].value_counts()\n",
    "    max_class = class_counts.index[class_counts.argmax()]\n",
    "    min_class = class_counts.index[class_counts.argmin()]\n",
    "    max_class_data = data[data.loc[:, label_var]==max_class]\n",
    "    min_class_data = data[data.loc[:, label_var]==min_class].sample(max_class_data.shape[0], replace=True, random_state=123)\n",
    "    resample_data = pd.concat([max_class_data, min_class_data], axis=0)\n",
    "    return resample_data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key : tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = tensor([self.labels[idx]])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "def load_training_model(model_name, out_dir):\n",
    "    device_name = 'cuda'\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name).to(device_name)\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=20,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        output_dir=os.path.join(out_dir, 'results'),\n",
    "        logging_dir=os.path.join(out_dir, 'logs'),\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy='steps',\n",
    "        seed=123,\n",
    "    )\n",
    "    return model, training_args\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }\n",
    "def train_test_BERT_classifier(data, label_var, txt_var, model_name, \n",
    "                               train_pct=0.9, special_tokens=None,\n",
    "                               max_length=512,\n",
    "                               out_dir='question_author_classification/',\n",
    "                               training_args=None):\n",
    "    # resample for max-class\n",
    "    data = resample_data_by_class(data, label_var)\n",
    "    # get labels\n",
    "    N_train = int(data.shape[0] * train_pct)\n",
    "    data = data.sample(data.shape[0], replace=False, random_state=123)\n",
    "    train_data = data.iloc[:N_train, :]\n",
    "    test_data = data.iloc[N_train:, :]\n",
    "    train_txt = train_data.loc[:, txt_var].values.tolist()\n",
    "    train_labels = train_data.loc[:, label_var].values.tolist()\n",
    "    test_txt = test_data.loc[:, txt_var].values.tolist()\n",
    "    test_labels = test_data.loc[:, label_var].values.tolist()\n",
    "    # convert labels to ID\n",
    "    label_id_lookup = {v : i for i, v in enumerate(data.loc[:, label_var].unique())}\n",
    "    id_label_lookup = {v : k for k,v in label_id_lookup.items()}\n",
    "    train_labels = list(map(label_id_lookup.get, train_labels))\n",
    "    test_labels = list(map(label_id_lookup.get, test_labels))\n",
    "    # tokenize things\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    if(special_tokens is not None):\n",
    "        tokenizer.add_special_tokens(special_tokens, special_tokens=True)\n",
    "    train_data_encoded = tokenizer(train_txt, truncation=True, padding=True, max_length=max_length)\n",
    "    test_data_encoded = tokenizer(test_txt, truncation=True, padding=True, max_length=max_length)\n",
    "    # load data set\n",
    "    train_data_set = CustomDataset(train_data_encoded, train_labels)\n",
    "    test_data_set = CustomDataset(test_data_encoded, test_labels)\n",
    "    # load model\n",
    "    model, model_training_args = load_training_model(model_name, out_dir=out_dir)\n",
    "    if(training_args is not None):\n",
    "        model_training_args.__dict__.update(training_args)\n",
    "    # build trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=model_training_args,\n",
    "        train_dataset=train_data_set,\n",
    "        eval_dataset=test_data_set,\n",
    "        compute_metrics=compute_metrics,\n",
    "        \n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(out_dir)\n",
    "    trainer.evaluate()\n",
    "    # evaluate on test data\n",
    "    test_data_pred = trainer.predict(test_data_set)\n",
    "    test_data_pred_labels = test_data_pred.predictions.argmax(-1).flatten().tolist()\n",
    "    print(classification_report(test_labels, test_data_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n"
     ]
    }
   ],
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "submission_question_data = pd.read_csv('science_submission_question_reply_author_attr_data.gz', sep='\\t', compression='gzip')\n",
    "print(submission_question_data.shape[0])\n",
    "## TODO: combine post title + question w/ special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_test_all_demos(data, demo_vars, device_num=1, txt_var='reply_question', training_args=None):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(device_num)\n",
    "    out_dir = 'question_author_classification/'\n",
    "    model_name = 'distilbert-base-cased'\n",
    "    device_name = 'cuda'\n",
    "    train_pct = 0.9\n",
    "    for demo_var_i in demo_vars:\n",
    "        print(f'testing demo var = {demo_var_i}')\n",
    "        data_i = data.dropna(subset=[demo_var_i])\n",
    "        train_test_BERT_classifier(data_i, demo_var_i, txt_var, model_name,\n",
    "                                   train_pct=train_pct, out_dir=out_dir,\n",
    "                                   training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.622213</td>\n",
       "      <td>0.650155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>0.597750</td>\n",
       "      <td>0.758514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.362431</td>\n",
       "      <td>0.873065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.419830</td>\n",
       "      <td>0.891641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.416057</td>\n",
       "      <td>0.919505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       160\n",
      "           1       0.94      0.87      0.90       163\n",
      "\n",
      "    accuracy                           0.91       323\n",
      "   macro avg       0.91      0.91      0.91       323\n",
      "weighted avg       0.91      0.91      0.91       323\n",
      "\n",
      "testing demo var = norm_age\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.33      0.32        15\n",
      "           1       0.50      0.48      0.49        21\n",
      "\n",
      "    accuracy                           0.42        36\n",
      "   macro avg       0.41      0.40      0.41        36\n",
      "weighted avg       0.42      0.42      0.42        36\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/684 01:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.706201</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>0.679983</td>\n",
       "      <td>0.571782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.652966</td>\n",
       "      <td>0.646040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.647252</td>\n",
       "      <td>0.641089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.674224</td>\n",
       "      <td>0.725248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.783124</td>\n",
       "      <td>0.732673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73       201\n",
      "           1       0.73      0.78      0.75       203\n",
      "\n",
      "    accuracy                           0.74       404\n",
      "   macro avg       0.74      0.74      0.74       404\n",
      "weighted avg       0.74      0.74      0.74       404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_vars = ['norm_gender', 'norm_age', 'norm_location']\n",
    "device_num = 1\n",
    "train_test_all_demos(submission_question_data, demo_vars, device_num=device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Gender`: >90%\n",
    "- `Age`: <50%\n",
    "- `Location`: >70%\n",
    "\n",
    "Overall not terrible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing but balance the data to have same label distribution per-post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_post_question_pairs(data, group_var, group_class_vals, id_var='conversation_id', sample_type='max'):\n",
    "    # get at least one pair of questions per post\n",
    "    sample_data = []\n",
    "    group_class_1, group_class_2 = group_class_vals\n",
    "    for id_i, data_i in data.groupby(id_var):\n",
    "        class_counts = data_i.loc[:, group_var].value_counts().sort_values(inplace=False, ascending=False)\n",
    "        if(len(class_counts) == len(group_class_vals)):\n",
    "            min_class = class_counts.index[-1]\n",
    "            max_class = class_counts.index[0]\n",
    "            N_min_class = class_counts.loc[min_class]\n",
    "            N_max_class = class_counts.loc[max_class]\n",
    "#             print(f'class counts = {class_counts}')\n",
    "            # oversample min class to handle data sparsity\n",
    "            if(sample_type == 'max'):\n",
    "                min_class_data_i = data_i[data_i.loc[:, group_var]==min_class].sample(N_max_class, replace=True, random_state=123)\n",
    "                max_class_data_i = data_i[data_i.loc[:, group_var]==max_class].sample(N_max_class, replace=False)\n",
    "            elif(sample_type == 'min'):\n",
    "                min_class_data_i = data_i[data_i.loc[:, group_var]==min_class]\n",
    "                max_class_data_i = data_i[data_i.loc[:, group_var]==max_class].sample(N_min_class, replace=False, random_state=123)\n",
    "            sample_data_i = pd.concat([min_class_data_i, max_class_data_i], axis=0)\n",
    "#             print(f'id={id_i} has question pair')\n",
    "            sample_data.append(sample_data_i)\n",
    "    sample_data = pd.concat(sample_data, axis=0)\n",
    "    print(f'N={sample_data.shape[0]}/{data.shape[0]} paired data')\n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_balanced_data(data, sample_type='min', demo_vars=['norm_age', 'norm_gender', 'norm_location']):\n",
    "    sample_balanced_data = []\n",
    "    id_var = 'id_submission'\n",
    "    for demo_var_i in demo_vars:\n",
    "        data_i = data.dropna(subset=[demo_var_i])\n",
    "        demo_vals_i = data_i.loc[:, demo_var_i].unique()\n",
    "        sample_data_i = sample_post_question_pairs(data, demo_var_i, \n",
    "                                                   demo_vals_i, id_var=id_var,\n",
    "                                                   sample_type=sample_type)\n",
    "        print(f'demo={demo_var_i} has N={sample_data_i.shape[0]}')\n",
    "        sample_balanced_data.append(sample_data_i)\n",
    "    sample_balanced_data = pd.concat(sample_balanced_data, axis=0)\n",
    "    return sample_balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=356/5095 paired data\n",
      "demo=norm_gender has N=356\n",
      "N=24/5095 paired data\n",
      "demo=norm_age has N=24\n",
      "N=1048/5095 paired data\n",
      "demo=norm_location has N=1048\n"
     ]
    }
   ],
   "source": [
    "sample_balanced_data = get_sample_balanced_data(submission_question_data, sample_type='min',\n",
    "                                                demo_vars=demo_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our modeling to the larger author groups (gender, location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.454013</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.394496</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92        55\n",
      "           1       1.00      0.87      0.93        70\n",
      "\n",
      "    accuracy                           0.93       125\n",
      "   macro avg       0.93      0.94      0.93       125\n",
      "weighted avg       0.94      0.93      0.93       125\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 00:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686700</td>\n",
       "      <td>0.645402</td>\n",
       "      <td>0.640167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.824268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.445559</td>\n",
       "      <td>0.845188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.476584</td>\n",
       "      <td>0.882845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88       123\n",
      "           1       0.86      0.91      0.88       116\n",
      "\n",
      "    accuracy                           0.88       239\n",
      "   macro avg       0.88      0.88      0.88       239\n",
      "weighted avg       0.88      0.88      0.88       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_demo_vars = ['norm_gender', 'norm_location']\n",
    "device_num = 1\n",
    "train_test_all_demos(sample_balanced_data, valid_demo_vars, device_num=device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The scores are still very high which might be overfitting but is hopefully generalizable to a larger population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention prediction\n",
    "1. Can we predict which posts will receive more attention from a particular audience group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender has N=9 posts\n",
      "Series([], dtype: int64)\n",
      "demo = norm_age has N=3 posts\n",
      "Series([], dtype: int64)\n",
      "demo = norm_location has N=11 posts\n",
      "non_US    2\n",
      "US        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## limit to posts w/ at least N posts from known authors\n",
    "def assign_max_group(data, group_var):\n",
    "    group_counts = data.loc[:, group_var].value_counts().sort_values(ascending=False)\n",
    "    max_group = group_counts.index[0]\n",
    "    if(group_counts.max() == group_counts.min()):\n",
    "        max_group = None\n",
    "    return max_group\n",
    "min_comment_count = 2\n",
    "id_var = 'id_submission'\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = submission_question_data.dropna(subset=[demo_var_i])\n",
    "    post_comment_counts_i = data_i.groupby(id_var).apply(lambda x: x.loc[:, 'id_comment'].nunique())\n",
    "    valid_post_ids_i = set(post_comment_counts_i[post_comment_counts_i].index)\n",
    "    max_demo_counts_i = data_i[data_i.loc[:, id_var].isin(valid_post_ids_i)].groupby(id_var).apply(lambda x: assign_max_group(x, demo_var_i))\n",
    "    print(f'demo = {demo_var_i} has N={len(valid_post_ids_i)} posts')\n",
    "    print(max_demo_counts_i.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This max-demographic strategy won't work. \n",
    "\n",
    "Let's just do the simplest thing and predict the author group based only on the post text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.780961</td>\n",
       "      <td>0.522388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.61      0.52        31\n",
      "           1       0.52      0.36      0.43        36\n",
      "\n",
      "    accuracy                           0.48        67\n",
      "   macro avg       0.49      0.49      0.47        67\n",
      "weighted avg       0.49      0.48      0.47        67\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.696200</td>\n",
       "      <td>0.704791</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.709397</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.82      0.58        57\n",
      "           1       0.55      0.17      0.26        71\n",
      "\n",
      "    accuracy                           0.46       128\n",
      "   macro avg       0.49      0.50      0.42       128\n",
      "weighted avg       0.50      0.46      0.40       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_balanced_data = get_sample_balanced_data(submission_question_data, sample_type='min',\n",
    "                                                demo_vars=demo_vars)\n",
    "valid_demo_vars = ['norm_gender', 'norm_location']\n",
    "device_num = 1\n",
    "txt_var = 'title'\n",
    "train_test_all_demos(sample_balanced_data, valid_demo_vars, device_num=device_num, txt_var=txt_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=672/5095 paired data\n",
      "demo=norm_gender has N=672\n",
      "N=30/5095 paired data\n",
      "demo=norm_age has N=30\n",
      "N=1946/5095 paired data\n",
      "demo=norm_location has N=1946\n",
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.699170</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.701641</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.45      0.46        55\n",
      "           1       0.58      0.59      0.58        70\n",
      "\n",
      "    accuracy                           0.53       125\n",
      "   macro avg       0.52      0.52      0.52       125\n",
      "weighted avg       0.53      0.53      0.53       125\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>0.694767</td>\n",
       "      <td>0.481172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691501</td>\n",
       "      <td>0.523013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.689704</td>\n",
       "      <td>0.543933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.688267</td>\n",
       "      <td>0.535565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.67      0.60       123\n",
      "           1       0.53      0.39      0.45       116\n",
      "\n",
      "    accuracy                           0.54       239\n",
      "   macro avg       0.53      0.53      0.52       239\n",
      "weighted avg       0.53      0.54      0.53       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## same thing but with max-sampled data\n",
    "sample_balanced_data = get_sample_balanced_data(submission_question_data, sample_type='max',\n",
    "                                                demo_vars=demo_vars)\n",
    "valid_demo_vars = ['norm_gender', 'norm_location']\n",
    "device_num = 1\n",
    "txt_var = 'title'\n",
    "# fix training args for smaller data?? no too much overfitting\n",
    "# training_args = {\n",
    "#     'per_device_train_batch_size' : 4, \n",
    "#     'learning_rate' : 1e-4, \n",
    "#     'warmup_steps' : 50\n",
    "# }\n",
    "training_args = None\n",
    "train_test_all_demos(sample_balanced_data, valid_demo_vars, \n",
    "                     device_num=device_num, txt_var=txt_var, \n",
    "                     training_args=training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Too much overfitting here. It's harder to predict which group of people will ask questions about a particular post, at least according to our current set-up.\n",
    "\n",
    "TODO: re-collect comment data, save, re-run analysis on all comments (i.e. not just questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction\n",
    "1. Can we predict which words will be mentioned in questions based on the post text? \n",
    "2. Can we do this for specific reader groups?\n",
    "\n",
    "Here the accuracy metric is F1: precision + recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## extract keywords\n",
    "import pandas as pd\n",
    "full_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "print(full_submission_question_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78108/78108 [00:10<00:00, 7142.87it/s]\n",
      " 92%|█████████▏| 71480/78108 [00:21<00:01, 3556.68it/s]"
     ]
    }
   ],
   "source": [
    "## tokenize, convert to lemmas\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# get stops and stop lemmas!\n",
    "punct = list(',!/&|-:\\'\"()[]\\\\/’?.') + ['...']\n",
    "stops = stopwords.words('english')\n",
    "stops += punct\n",
    "stops += list(set(map(stemmer.stem, stops)))\n",
    "stops = set(stops)\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_tokens' : full_submission_question_data.loc[:, 'title'].progress_apply(lambda x: tokenizer.tokenize(x)),\n",
    "    'question_tokens' : full_submission_question_data.loc[:, 'reply_question'].progress_apply(lambda x: list(map(lambda y: stemmer.stem(y), tokenizer.tokenize(x)))),\n",
    "})\n",
    "# stem words\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_tokens_clean' : full_submission_question_data.loc[:, 'title_tokens'].progress_apply(lambda x: list(map(lambda y: stemmer.stem(y), x))),\n",
    "    'question_tokens_clean' : full_submission_question_data.loc[:, 'question_tokens'].progress_apply(lambda x: list(map(lambda y: stemmer.stem(y), x))),\n",
    "})\n",
    "## get overlap\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_question_overlap' : full_submission_question_data.progress_apply(lambda x: (set(x.loc['title_tokens_clean']) & set(x.loc['question_tokens_clean'])) - stops, axis=1)\n",
    "})\n",
    "## match overlap tokens with original post tokens\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_question_overlap_clean' : full_submission_question_data.progress_apply(lambda x: [t1 for t1, t2 in zip(x.loc['title_tokens'], x.loc['title_tokens_clean']) if t2 in x.loc['title_question_overlap']], axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "overlap_thresh = 1\n",
    "display(full_submission_question_data[full_submission_question_data.loc[:, 'title_question_overlap'].apply(lambda x: len(x) >= overlap_thresh)].head(10).loc[:, ['title', 'title_tokens', 'reply_question', 'question_tokens', 'title_question_overlap_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract relevant words: proper nouns, noun chunks, verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question generation\n",
    "1. Can we generate the questions asked in comments based on the post text?\n",
    "2. Can we do this for specific reader groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: taxonomy of clarification questions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

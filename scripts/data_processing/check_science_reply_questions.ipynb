{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check science reply questions\n",
    "Let's look at some data related to sharing new science articles and the questions that people pose in response to the articles.\n",
    "\n",
    "We'll see how readily we can predict author background using the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "1001it [00:05, 286.60it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "47091it [07:52, 99.66it/s] \n"
     ]
    }
   ],
   "source": [
    "## get Reddit data!!\n",
    "from datetime import datetime\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm\n",
    "# from data_helpers import load_reddit_api\n",
    "# reddit_api, pushshift_api = load_reddit_api('../../data/auth_data/reddit_auth.csv')\n",
    "pushshift_api = PushshiftAPI()\n",
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['url', 'title', 'author', 'score', 'text', 'created_utc', 'id', 'upvote_ratio', 'num_comments']\n",
    "submissions = pushshift_api.search_submissions(q=\"*\", after=date_range[0], before=date_range[1],\n",
    "                                               subreddit=subreddit, filter=filter_fields)\n",
    "submissions_results = []\n",
    "for s in tqdm(submissions):\n",
    "    submissions_results.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>d_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talismanbrandi</td>\n",
       "      <td>1630462648</td>\n",
       "      <td>pfkdt5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Socio-economic disparities and COVID-19 in the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/pfkd...</td>\n",
       "      <td>1630480648.0</td>\n",
       "      <td>{'author': 'talismanbrandi', 'created_utc': 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BeforeYourBBQ</td>\n",
       "      <td>1630462436</td>\n",
       "      <td>pfkbn2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing SARS-CoV-2 natural immunity to vacci...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.medrxiv.org/content/10.1101/2021.0...</td>\n",
       "      <td>1630480436.0</td>\n",
       "      <td>{'author': 'BeforeYourBBQ', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462250</td>\n",
       "      <td>pfk9q9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Scientists Figured Out How Much Exercise You N...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.sciencealert.com/scientists-figure...</td>\n",
       "      <td>1630480250.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462179</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "      <td>Female octopuses throw shells at males annoyin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.independent.co.uk/climate-change/n...</td>\n",
       "      <td>1630480179.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doozenburg</td>\n",
       "      <td>1630461660</td>\n",
       "      <td>pfk3ts</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Who is Anti-Vax Dr. Wendy Menigoz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.slugbrain.com/post/who-is-anti-vax...</td>\n",
       "      <td>1630479660.0</td>\n",
       "      <td>{'author': 'Doozenburg', 'created_utc': 163046...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  created_utc  ...       created                                                 d_\n",
       "0  talismanbrandi   1630462648  ...  1630480648.0  {'author': 'talismanbrandi', 'created_utc': 16...\n",
       "1   BeforeYourBBQ   1630462436  ...  1630480436.0  {'author': 'BeforeYourBBQ', 'created_utc': 163...\n",
       "2   key__lime_pie   1630462250  ...  1630480250.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "3   key__lime_pie   1630462179  ...  1630480179.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "4      Doozenburg   1630461660  ...  1630479660.0  {'author': 'Doozenburg', 'created_utc': 163046...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## convert to data frame\n",
    "import pandas as pd\n",
    "submission_data = pd.DataFrame(submissions_results)\n",
    "display(submission_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect all comments from the same time frame, and align them to submissions afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "901it [00:03, 289.96it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "18800it [03:16, 23.04it/s] /home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "907905it [2:33:53, 83.42it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "1445247it [4:03:56, 98.15it/s] "
     ]
    }
   ],
   "source": [
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['id', 'link_id', 'parent_id', 'body', 'author', 'created_utc', 'score']\n",
    "comments = pushshift_api.search_comments(after=date_range[0], before=date_range[1],\n",
    "                                         subreddit=subreddit, filter=filter_fields)\n",
    "comments_results = []\n",
    "for c in tqdm(comments):\n",
    "    comments_results.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Etna</td>\n",
       "      <td>oh I misread, it's the males that are being an...</td>\n",
       "      <td>1630468787</td>\n",
       "      <td>hb5b6ss</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>27</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lalauna</td>\n",
       "      <td>Please tell me something i didn't know before.</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zz</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DivineBanana</td>\n",
       "      <td>I haven't followed the debate but I'm sure it'...</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zy</td>\n",
       "      <td>pf5phr</td>\n",
       "      <td>hb58igp</td>\n",
       "      <td>3</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_MASTADONG_</td>\n",
       "      <td>I just linked you to 2 articles on fact checki...</td>\n",
       "      <td>1630468723</td>\n",
       "      <td>hb5b2sg</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>hb5adks</td>\n",
       "      <td>4</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Etna</td>\n",
       "      <td>It's because they can't talk</td>\n",
       "      <td>1630468713</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>37</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               body  created_utc       id link_id parent_id  score       created\n",
       "0          Etna  oh I misread, it's the males that are being an...   1630468787  hb5b6ss  pfk90j   hb5b25v     27  1.630487e+09\n",
       "3       lalauna     Please tell me something i didn't know before.   1630468726  hb5b2zz  pfgvrw    pfgvrw      1  1.630487e+09\n",
       "4  DivineBanana  I haven't followed the debate but I'm sure it'...   1630468726  hb5b2zy  pf5phr   hb58igp      3  1.630487e+09\n",
       "5   _MASTADONG_  I just linked you to 2 articles on fact checki...   1630468723  hb5b2sg  pfgvrw   hb5adks      4  1.630487e+09\n",
       "8          Etna                       It's because they can't talk   1630468713  hb5b25v  pfk90j    pfk90j     37  1.630487e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196985\n"
     ]
    }
   ],
   "source": [
    "## combine/clean\n",
    "comment_data = pd.DataFrame(comments_results)\n",
    "# drop deleted data\n",
    "comment_data = comment_data[(comment_data.loc[:, 'author']!='[deleted]') &\n",
    "                            (comment_data.loc[:, 'body']!='[deleted]')]\n",
    "# fix ID vars\n",
    "comment_data = comment_data.assign(**{\n",
    "    'link_id' : comment_data.loc[:, 'link_id'].apply(lambda x: x.split('_')[1]),\n",
    "    'parent_id' : comment_data.loc[:, 'parent_id'].apply(lambda x: x.split('_')[1]),\n",
    "})\n",
    "# drop extra data\n",
    "comment_data.drop('d_', axis=1, inplace=True)\n",
    "display(comment_data.head())\n",
    "print(comment_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict to comment/submission matches\n",
    "submission_comment_data = pd.merge(submission_data, comment_data, left_on='id', right_on='link_id', how='inner')\n",
    "# fix col names\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_x', '_submission') \n",
    "    for x in list(filter(lambda x: x.endswith('_x'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_y', '_comment') \n",
    "    for x in list(filter(lambda x: x.endswith('_y'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data = submission_comment_data[submission_comment_data.loc[:, 'link_id']==submission_comment_data.loc[:, 'parent_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465354"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_comment_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130878 questions total\n"
     ]
    }
   ],
   "source": [
    "## clean text\n",
    "import re\n",
    "RETURN_MATCHER = re.compile('[\\n\\r]')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'body' : submission_comment_data.loc[:, 'body'].apply(lambda x: RETURN_MATCHER.sub(' ', x))\n",
    "})\n",
    "## filter questions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_sents' : submission_comment_data.loc[:, 'body'].apply(lambda x: sent_tokenize(x))\n",
    "})\n",
    "# look for questions!\n",
    "import re\n",
    "question_matcher = re.compile('\\?$')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_questions' : submission_comment_data.loc[:, 'reply_sents'].apply(lambda x: list(filter(lambda y: question_matcher.search(y) is not None, x)))\n",
    "})\n",
    "submission_question_data = submission_comment_data[submission_comment_data.loc[:, 'reply_questions'].apply(len)>0]\n",
    "## flatten\n",
    "flat_submission_question_data = []\n",
    "for idx_i, data_i in submission_question_data.iterrows():\n",
    "    for q_j in data_i.loc['reply_questions']:\n",
    "        data_j = data_i.copy().drop('reply_questions')\n",
    "        data_j.loc['reply_question'] = q_j\n",
    "        flat_submission_question_data.append(data_j)\n",
    "flat_submission_question_data = pd.concat(flat_submission_question_data, axis=1).transpose()\n",
    "print(f'{flat_submission_question_data.shape[0]} questions total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['die for the economy?', 'how about we topple you instead?',\n",
       "       'Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Have I misread something?',\n",
       "       \"Don't you care about the environment?\"], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## look at sample questions => clarification questions? self-contained? related to post?\n",
    "## sample questions\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(flat_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit the questions to have at least X words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Wait, who was saying there would be a pandemic baby boom and why?',\n",
       "       \"Isn't it established that stress and uncertainty eliminate the desire to be parents, or is that just my intuition?\",\n",
       "       'The baby boom thing was just a joke right?',\n",
       "       'Was there any real feeling that that would happen?'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "min_question_words = 10\n",
    "tokenizer = WordPunctTokenizer()\n",
    "valid_submission_question_data = flat_submission_question_data[flat_submission_question_data.loc[:, 'reply_question'].apply(lambda x: len(tokenizer.tokenize(x)) >= min_question_words)]\n",
    "display(valid_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for posterity!!\n",
    "valid_submission_question_data.to_csv('science_submission_question_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "2it [00:03,  2.19s/it]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "3099it [6:11:34,  6.67s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10000it [20:27:42,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## TODO: mine previous history for N=10000 commenters; extract location + age + gender (?)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from psaw import PushshiftAPI\n",
    "pushshift_api = PushshiftAPI()\n",
    "N_commenters = 10000\n",
    "N_previous_comments = 1000\n",
    "sample_question_author_data = valid_submission_question_data.sort_values(['author_comment', 'created_utc_comment'], ascending=True).drop_duplicates('author_comment').loc[:, ['author_comment', 'created_utc_comment']]\n",
    "sample_question_author_data = sample_question_author_data.sample(N_commenters, replace=False, random_state=123)\n",
    "sample_question_author_prior_data = []\n",
    "author_filter_cols = ['body', 'id', 'created_utc', 'author', 'subreddit']\n",
    "for idx_i, data_i in tqdm(sample_question_author_data.iterrows()):\n",
    "    author_i = data_i.loc['author_comment']\n",
    "    time_i = int(data_i.loc['created_utc_comment'])\n",
    "    prior_comments_i = list(pushshift_api.search_comments(author=author_i, limit=N_previous_comments, before=time_i, filter=author_filter_cols))\n",
    "    prior_comments_i = pd.DataFrame(prior_comments_i)\n",
    "    if('d_' in prior_comments_i.columns):\n",
    "        prior_comments_i.drop('d_', axis=1, inplace=True)\n",
    "    sample_question_author_prior_data.append(prior_comments_i)\n",
    "sample_question_author_prior_data = pd.concat(sample_question_author_prior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_question_author_prior_data.to_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (0,1,2,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data = pd.read_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data.dropna(subset=['body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>You're not even Scottish, are you?</td>\n",
       "      <td>1582310896</td>\n",
       "      <td>fiayfoz</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582329e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.</td>\n",
       "      <td>1582298322</td>\n",
       "      <td>fiacrvb</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582316e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.</td>\n",
       "      <td>1582291726</td>\n",
       "      <td>fia35ak</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582310e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...</td>\n",
       "      <td>1582258926</td>\n",
       "      <td>fi99f50</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582277e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog</td>\n",
       "      <td>1582258153</td>\n",
       "      <td>fi98eh0</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582276e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  \\\n",
       "0  mahogany555   \n",
       "1  mahogany555   \n",
       "2  mahogany555   \n",
       "3  mahogany555   \n",
       "4  mahogany555   \n",
       "\n",
       "                                                                                                                                                                                                                                              body  \\\n",
       "0                                                                                                                                                                                                               You're not even Scottish, are you?   \n",
       "1                                                                                                                                                         'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.   \n",
       "2  Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.   \n",
       "3                                                                                                                   No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...   \n",
       "4                                                                                                             Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog   \n",
       "\n",
       "  created_utc       id              subreddit       created  \n",
       "0  1582310896  fiayfoz  ScottishPeopleTwitter  1.582329e+09  \n",
       "1  1582298322  fiacrvb  ScottishPeopleTwitter  1.582316e+09  \n",
       "2  1582291726  fia35ak  ScottishPeopleTwitter  1.582310e+09  \n",
       "3  1582258926  fi99f50  ScottishPeopleTwitter  1.582277e+09  \n",
       "4  1582258153  fi98eh0  ScottishPeopleTwitter  1.582276e+09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(sample_question_author_prior_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 9), match='I live in'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>This is only something I realized 2 days ago but out of nowhere I gained 18lbs.\\n\\nI’m really happy! I am a super underweight (29yo, 5’8) male and have never been able to gain weight. I’ve been so hungry this week and am constantly eating. \\n\\nI’ve been around 90-100lbs for years and weighed my self and am pushing 119lbs. LOL I know how stupid this sounds, but for me it is a fun fact.\\n\\nI just hope my metabolism hasn’t permanently ghosted me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>is it Hardwon or Magnus from Adventure Zone? They are basically the same....buff human bearded fighters. I am also guessing Hardwon though, since his beard is so dwarvish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>i have a feeling that part is a work in progress, and I am guessing it will also hinge on an actual MLB ruling.\\n\\nEven before this there were weird things showing up, like the choice to include Ross Barnes' 1876 as official and part of the top batting averages, but his 1873 was not included. I am assuming it was because one is for the National League and one is for the National Association, but the difference in plate appearances in those two seasons is two.\\n\\nThe stat nerd side of me sort of cringes at a lot of these, but another part of me wants old boomers who scream about tradition and complain about 'nerds ruining the game' to have to swallow Tetelo Vargas being the all-time batting average champ with his .471 in 30 games in 1943.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>The first time I closed a bank account:\\n\\nI needed some cash so I went to an ATM outside my bank to get $20 but the minimum was $40. Fine, it said, not realizing I had about $39.50 in my checking. Get receipt and see I am in the negative. No problem, I can just use the ATM to transfer some money from my savings. I do that, moving $20 from my savings.\\n\\nNext bill I am at -$4 and change because when I went negative it immediately charged me the overdraft...but didn't show it on the receipt, and the money I put on it was less than the fee.\\n\\nSo I went to the actual bank to argue my case and the teller tried to explain to me that they did me a favor for not charging me TWO (2) overdraft fees because I had gone negative then positive then back to negative. \\n\\nSo I did the only thing I could do, I closed both my savings and checking and went to a credit union.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>yeah. I am a tiger fan and I have always hated the white sox. Then the last few years while they were building this fun young team I started to like watching them. Then they hired LaRussa. I HATE LaRussa more than I hate the White Sox.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>The picture used here though is from the second episode when a horny disease hits the enterprise.\\n\\nSource: I just started watching TNG for the first time so I am an 'expert' on episodes 1 and 2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Maybe I am a bit cynical. But using your definition we can clearly see we are not at the point you think we are.\\n\\nThis good young core? We have 5 top prospects. Two are pitchers in their first full seasons and two are batters, one who is still only 19 and basically didn't play last season and the other just finished his first professional spring training. Where would you expect them to be?\\n\\nWhat we do have are a bunch of AAA of AAAA talents who are holding roster spots until we get to that next point.\\n\\n\\nlooking at this current roster I cannot imagine thinking we should be good, or average even, at hitting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>Yeah, there seems to be backlash all over when it comes to the changing of traditional gender norms and being more open with your feelings, but if the result is a generation of men who can deal with their emotions in a healthy way and not just feel alone then I am all for it. But that is sort of what seems to be lost in these types of debates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>I live in Boston. It did rain sort of hard for a bit...but they could have played.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Personally I am a steroid apologist so personally I wouldn't lose my shit. But do you think r/baseball would be this nuanced if he suddenly went  .320/.400//.550?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       body\n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                          This is only something I realized 2 days ago but out of nowhere I gained 18lbs.\\n\\nI’m really happy! I am a super underweight (29yo, 5’8) male and have never been able to gain weight. I’ve been so hungry this week and am constantly eating. \\n\\nI’ve been around 90-100lbs for years and weighed my self and am pushing 119lbs. LOL I know how stupid this sounds, but for me it is a fun fact.\\n\\nI just hope my metabolism hasn’t permanently ghosted me.\n",
       "436                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             is it Hardwon or Magnus from Adventure Zone? They are basically the same....buff human bearded fighters. I am also guessing Hardwon though, since his beard is so dwarvish.\n",
       "540                                                                                                                             i have a feeling that part is a work in progress, and I am guessing it will also hinge on an actual MLB ruling.\\n\\nEven before this there were weird things showing up, like the choice to include Ross Barnes' 1876 as official and part of the top batting averages, but his 1873 was not included. I am assuming it was because one is for the National League and one is for the National Association, but the difference in plate appearances in those two seasons is two.\\n\\nThe stat nerd side of me sort of cringes at a lot of these, but another part of me wants old boomers who scream about tradition and complain about 'nerds ruining the game' to have to swallow Tetelo Vargas being the all-time batting average champ with his .471 in 30 games in 1943.\n",
       "561  The first time I closed a bank account:\\n\\nI needed some cash so I went to an ATM outside my bank to get $20 but the minimum was $40. Fine, it said, not realizing I had about $39.50 in my checking. Get receipt and see I am in the negative. No problem, I can just use the ATM to transfer some money from my savings. I do that, moving $20 from my savings.\\n\\nNext bill I am at -$4 and change because when I went negative it immediately charged me the overdraft...but didn't show it on the receipt, and the money I put on it was less than the fee.\\n\\nSo I went to the actual bank to argue my case and the teller tried to explain to me that they did me a favor for not charging me TWO (2) overdraft fees because I had gone negative then positive then back to negative. \\n\\nSo I did the only thing I could do, I closed both my savings and checking and went to a credit union.\n",
       "670                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             yeah. I am a tiger fan and I have always hated the white sox. Then the last few years while they were building this fun young team I started to like watching them. Then they hired LaRussa. I HATE LaRussa more than I hate the White Sox.\n",
       "682                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The picture used here though is from the second episode when a horny disease hits the enterprise.\\n\\nSource: I just started watching TNG for the first time so I am an 'expert' on episodes 1 and 2.\n",
       "748                                                                                                                                                                                                                                                            Maybe I am a bit cynical. But using your definition we can clearly see we are not at the point you think we are.\\n\\nThis good young core? We have 5 top prospects. Two are pitchers in their first full seasons and two are batters, one who is still only 19 and basically didn't play last season and the other just finished his first professional spring training. Where would you expect them to be?\\n\\nWhat we do have are a bunch of AAA of AAAA talents who are holding roster spots until we get to that next point.\\n\\n\\nlooking at this current roster I cannot imagine thinking we should be good, or average even, at hitting.\n",
       "812                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Yeah, there seems to be backlash all over when it comes to the changing of traditional gender norms and being more open with your feelings, but if the result is a generation of men who can deal with their emotions in a healthy way and not just feel alone then I am all for it. But that is sort of what seems to be lost in these types of debates.\n",
       "813                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      I live in Boston. It did rain sort of hard for a bit...but they could have played.\n",
       "820                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Personally I am a steroid apologist so personally I wouldn't lose my shit. But do you think r/baseball would be this nuanced if he suddenly went  .320/.400//.550?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## check for self-disclosure statements\n",
    "import re\n",
    "self_disclosure_matcher = re.compile(f'((I\\'m|I am) a)|(I live in)|(I am from)')\n",
    "print(self_disclosure_matcher.search('I live in MI'))\n",
    "tmp = sample_question_author_prior_data[sample_question_author_prior_data.loc[:, 'body'].apply(lambda x: self_disclosure_matcher.search(x) is not None)]\n",
    "display(tmp.loc[:, ['body']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPULA_LEMMA = 'be'\n",
    "EXIST_LEMMA = 'live'\n",
    "GENDER_MATCHER = re.compile('^(man|woman|male|female)$')\n",
    "AGE_NUM_MATCHER = re.compile('[0-9]+')\n",
    "def collect_propn(token):\n",
    "    loc_noun_parts = [token]\n",
    "    location_children = list(filter(lambda x: x.pos_=='PROPN', token.children))\n",
    "    while(len(location_children) > 0):\n",
    "        loc_noun_part = location_children.pop()\n",
    "        loc_noun_parts.append(loc_noun_part)\n",
    "        location_children += list(filter(lambda x: x.pos_=='PROPN', loc_noun_part.children))\n",
    "    # sort noun parts\n",
    "    loc_noun_parts = list(sorted(loc_noun_parts, key=lambda x: x.idx))\n",
    "    loc_noun = ' '.join(list(map(lambda x: x.lemma_, loc_noun_parts)))\n",
    "    return loc_noun\n",
    "def extract_self_statement_targets(sent, pipeline):\n",
    "    parse = pipeline(sent)\n",
    "    identity_attributes = []\n",
    "    parse_sents = list(parse.doc.sents)\n",
    "    for parse_sent in parse_sents:\n",
    "        for token in parse_sent:\n",
    "            # get children nouns for \"I\" via root\n",
    "            if(token.lemma_ == 'I' and token.dep_ == 'nsubj'):\n",
    "                token_root_ancestors = list(filter(lambda x: x.dep_=='ROOT', token.ancestors))\n",
    "    #             print(f'parse ents = {list(map(lambda x: x.label_, parse.ents))}')\n",
    "                loc_ents = list(filter(lambda x: x.label_=='GPE', parse.ents))\n",
    "                if(len(token_root_ancestors) > 0):\n",
    "                    token_root = token_root_ancestors[0]\n",
    "                    root_children = list(token_root.children)\n",
    "                    if(token_root.lemma_ == COPULA_LEMMA):\n",
    "                        for child in root_children:\n",
    "                            if(child.dep_ == 'attr'):\n",
    "                                # gender\n",
    "                                gender_match = GENDER_MATCHER.match(child.lemma_)\n",
    "                                if(gender_match is not None):\n",
    "                                    identity_attributes.append(['gender', gender_match.group(0)])\n",
    "                                # age => NOPE false positives abound\n",
    "#                                 age_match = AGE_NUM_MATCHER.match(child.lemma_)\n",
    "#                                 if(age_match is not None):\n",
    "#                                     identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # age\n",
    "                            elif(child.dep_ == 'acomp'):\n",
    "                                if(child.lemma_ == 'old'):\n",
    "                                    # look for children (\"30 years old\")\n",
    "                                    age_children_1 = list(child.children)\n",
    "                                    if(len(age_children_1) > 0 and age_children_1[0].lemma_=='year'):\n",
    "                                        age_children_2 = list(age_children_1[0].children)\n",
    "                                        if(len(age_children_2) > 0):\n",
    "                                            age_match = AGE_NUM_MATCHER.match(age_children_2[0].lemma_)\n",
    "                                            if(age_match is not None):\n",
    "                                                identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # location\n",
    "                            elif(child.dep_ == 'prep' and child.lemma_ == 'from'):\n",
    "                                location_children_1 = list(child.children)\n",
    "                                if(len(location_children_1) > 0 and location_children_1[0].pos_ == 'PROPN'):\n",
    "                                    ent_start = location_children_1[0].i\n",
    "    #                                 child_1_idx = location_children_1[0].idx\n",
    "    #                                 child_1_ent = \n",
    "    #                                 main_loc = location_children_1[0]\n",
    "    #                                 loc_noun = collect_propn(main_loc)\n",
    "                                    # find ENT that contains child\n",
    "    #                                 print(f'ent start = {ent_start}')\n",
    "    #                                 print(f'{[(x.start, x.end) for x in loc_ents]}')\n",
    "                                    containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "    #                                 print(f'containing loc ents {containing_loc_ents}')\n",
    "                                    if(len(containing_loc_ents) > 0):\n",
    "                                        loc_noun = containing_loc_ents[0].text\n",
    "                                        identity_attributes.append(['location', loc_noun])\n",
    "                    # \"I live in the US\"\n",
    "                    elif(token_root.lemma_ == EXIST_LEMMA):\n",
    "                        root_prep_children = list(filter(lambda x: x.lemma_=='in' and x.dep_=='prep', token_root.children))\n",
    "                        if(len(root_prep_children) > 0):\n",
    "                            prep_children_2 = list(filter(lambda x: x.pos_ == 'PROPN', root_prep_children[0].children))\n",
    "                            if(len(prep_children_2) > 0):\n",
    "                                ent_start = prep_children_2[0].i\n",
    "                                containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "                                if(len(containing_loc_ents)):\n",
    "                                    loc_noun = containing_loc_ents[0].text\n",
    "                                    identity_attributes.append(['location', loc_noun])\n",
    "    return identity_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent = I am 30 years old has attr [['age', '30']]\n",
      "sent = I am a woman has attr [['gender', 'woman']]\n",
      "sent = I live in London, England has attr [['location', 'London']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# dumb\n",
    "# self_statement = '(I\\'m|I am)'\n",
    "# age_matcher = f'({self_statement} [0-9]+ years old)|({self_statement} a [0-9]+ yo )'\n",
    "# gender_matcher = f'({self_statement} a[ a-zA-Z]? (man|woman|male|female))'\n",
    "# smart => parse then extract\n",
    "import spacy\n",
    "nlp_pipeline = spacy.load('en_core_web_sm')\n",
    "# gender_dep = attr (\"I'm a man\")\n",
    "# age_dep = acomp (\"I'm 50 years old\")\n",
    "# loc_dep = prep (\"I live in Michigan\")\n",
    "test_sents = [\n",
    "    'I am 30 years old',\n",
    "    'I am a woman',\n",
    "    'I live in London, England',\n",
    "]\n",
    "for sent in test_sents:\n",
    "    sent_attr = extract_self_statement_targets(sent, nlp_pipeline)\n",
    "    print(f'sent = {sent} has attr {sent_attr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have an extremely brittle attribute extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee375c045dd45d2b72914c96444cd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=680400), Label(value='0 / 680400')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## extract all attributes!!\n",
    "# from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=10, progress_bar=True)\n",
    "# tqdm.pandas()ai \n",
    "sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "    'identity_attributes' : sample_question_author_prior_data.loc[:, 'body'].parallel_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:00<00:00, 149.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>location</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0wnzl1f3</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A4_Ts</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Born2Rune</td>\n",
       "      <td>UK</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bwanatumbo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>man</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check_My_Dubs_Friend</td>\n",
       "      <td>NaN</td>\n",
       "      <td>woman</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author    location gender  age\n",
       "0              0wnzl1f3      Canada    NaN  NaN\n",
       "0                 A4_Ts  California    NaN  NaN\n",
       "0             Born2Rune          UK   male  NaN\n",
       "0            Bwanatumbo         NaN    man    3\n",
       "0  Check_My_Dubs_Friend         NaN  woman  NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_author_attribute_data = sample_question_author_prior_data[sample_question_author_prior_data.loc[:, 'identity_attributes'].apply(lambda x: len(x) > 0)]\n",
    "# get one line per author\n",
    "flat_author_attribute_data = []\n",
    "for author_i, data_i in tqdm(question_author_attribute_data.groupby('author')):\n",
    "    attr_data_i = []\n",
    "    for idx_j, data_j in data_i.iterrows():\n",
    "        # keep track of dates!!\n",
    "        date_j = data_j.loc['created_utc']\n",
    "        for attr_k, val_k in data_j.loc['identity_attributes']:\n",
    "            attr_data_i.append({\n",
    "                'author' : author_i,\n",
    "                'date' : date_j,\n",
    "                'attr' : attr_k,\n",
    "                'val' : val_k\n",
    "            })\n",
    "    attr_data_i = pd.DataFrame(attr_data_i)\n",
    "    attr_data_i.sort_values(['attr', 'date'], inplace=True, ascending=False)\n",
    "    attr_data_i = attr_data_i.drop_duplicates(['attr'], keep='first').drop('date', axis=1)    \n",
    "    attr_data_i = attr_data_i.pivot(index='author', columns=['attr'], values=['val']).reset_index()\n",
    "    attr_data_i.columns = list(map(lambda x: x[0] if x[1]=='' else x[1], attr_data_i.columns))\n",
    "    flat_author_attribute_data.append(attr_data_i)\n",
    "flat_author_attribute_data = pd.concat(flat_author_attribute_data, axis=0)\n",
    "display(flat_author_attribute_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada           6\n",
      "US               4\n",
      "UK               3\n",
      "California       2\n",
      "Vancouver        2\n",
      "Texas            2\n",
      "America          2\n",
      "Sweden           1\n",
      "Oklahoma City    1\n",
      "SF               1\n",
      "Name: location, dtype: int64\n",
      "man       20\n",
      "woman      9\n",
      "male       3\n",
      "female     2\n",
      "Name: gender, dtype: int64\n",
      "3      1\n",
      "10     1\n",
      "33     1\n",
      "500    1\n",
      "24     1\n",
      "2      1\n",
      "22     1\n",
      "1      1\n",
      "69     1\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_attr = ['location', 'gender', 'age']\n",
    "for attr_i in all_attr:\n",
    "    print(flat_author_attribute_data.loc[:, attr_i].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: taxonomy of clarification questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

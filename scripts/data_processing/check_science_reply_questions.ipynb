{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check science reply questions\n",
    "Let's look at some data related to sharing new science articles and the questions that people pose in response to the articles.\n",
    "\n",
    "We'll see how readily we can predict author background using the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "1001it [00:05, 286.60it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "47091it [07:52, 99.66it/s] \n"
     ]
    }
   ],
   "source": [
    "## get Reddit data!!\n",
    "from datetime import datetime\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm\n",
    "# from data_helpers import load_reddit_api\n",
    "# reddit_api, pushshift_api = load_reddit_api('../../data/auth_data/reddit_auth.csv')\n",
    "pushshift_api = PushshiftAPI()\n",
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['url', 'title', 'author', 'score', 'text', 'created_utc', 'id', 'upvote_ratio', 'num_comments']\n",
    "submissions = pushshift_api.search_submissions(q=\"*\", after=date_range[0], before=date_range[1],\n",
    "                                               subreddit=subreddit, filter=filter_fields)\n",
    "submissions_results = []\n",
    "for s in tqdm(submissions):\n",
    "    submissions_results.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>d_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talismanbrandi</td>\n",
       "      <td>1630462648</td>\n",
       "      <td>pfkdt5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Socio-economic disparities and COVID-19 in the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/pfkd...</td>\n",
       "      <td>1630480648.0</td>\n",
       "      <td>{'author': 'talismanbrandi', 'created_utc': 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BeforeYourBBQ</td>\n",
       "      <td>1630462436</td>\n",
       "      <td>pfkbn2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing SARS-CoV-2 natural immunity to vacci...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.medrxiv.org/content/10.1101/2021.0...</td>\n",
       "      <td>1630480436.0</td>\n",
       "      <td>{'author': 'BeforeYourBBQ', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462250</td>\n",
       "      <td>pfk9q9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Scientists Figured Out How Much Exercise You N...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.sciencealert.com/scientists-figure...</td>\n",
       "      <td>1630480250.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>key__lime_pie</td>\n",
       "      <td>1630462179</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "      <td>Female octopuses throw shells at males annoyin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.independent.co.uk/climate-change/n...</td>\n",
       "      <td>1630480179.0</td>\n",
       "      <td>{'author': 'key__lime_pie', 'created_utc': 163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doozenburg</td>\n",
       "      <td>1630461660</td>\n",
       "      <td>pfk3ts</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Who is Anti-Vax Dr. Wendy Menigoz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.slugbrain.com/post/who-is-anti-vax...</td>\n",
       "      <td>1630479660.0</td>\n",
       "      <td>{'author': 'Doozenburg', 'created_utc': 163046...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  created_utc  ...       created                                                 d_\n",
       "0  talismanbrandi   1630462648  ...  1630480648.0  {'author': 'talismanbrandi', 'created_utc': 16...\n",
       "1   BeforeYourBBQ   1630462436  ...  1630480436.0  {'author': 'BeforeYourBBQ', 'created_utc': 163...\n",
       "2   key__lime_pie   1630462250  ...  1630480250.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "3   key__lime_pie   1630462179  ...  1630480179.0  {'author': 'key__lime_pie', 'created_utc': 163...\n",
       "4      Doozenburg   1630461660  ...  1630479660.0  {'author': 'Doozenburg', 'created_utc': 163046...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## convert to data frame\n",
    "import pandas as pd\n",
    "submission_data = pd.DataFrame(submissions_results)\n",
    "display(submission_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect all comments from the same time frame, and align them to submissions afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "901it [00:03, 289.96it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "18800it [03:16, 23.04it/s] /home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "907905it [2:33:53, 83.42it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "1445247it [4:03:56, 98.15it/s] "
     ]
    }
   ],
   "source": [
    "date_range = ['2020-01-01', '2021-09-01']\n",
    "date_range = list(map(lambda x: int(datetime.strptime(x, '%Y-%m-%d').timestamp()), date_range))\n",
    "subreddit = 'science'\n",
    "filter_fields = ['id', 'link_id', 'parent_id', 'body', 'author', 'created_utc', 'score']\n",
    "comments = pushshift_api.search_comments(after=date_range[0], before=date_range[1],\n",
    "                                         subreddit=subreddit, filter=filter_fields)\n",
    "comments_results = []\n",
    "for c in tqdm(comments):\n",
    "    comments_results.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Etna</td>\n",
       "      <td>oh I misread, it's the males that are being an...</td>\n",
       "      <td>1630468787</td>\n",
       "      <td>hb5b6ss</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>27</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lalauna</td>\n",
       "      <td>Please tell me something i didn't know before.</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zz</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DivineBanana</td>\n",
       "      <td>I haven't followed the debate but I'm sure it'...</td>\n",
       "      <td>1630468726</td>\n",
       "      <td>hb5b2zy</td>\n",
       "      <td>pf5phr</td>\n",
       "      <td>hb58igp</td>\n",
       "      <td>3</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_MASTADONG_</td>\n",
       "      <td>I just linked you to 2 articles on fact checki...</td>\n",
       "      <td>1630468723</td>\n",
       "      <td>hb5b2sg</td>\n",
       "      <td>pfgvrw</td>\n",
       "      <td>hb5adks</td>\n",
       "      <td>4</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Etna</td>\n",
       "      <td>It's because they can't talk</td>\n",
       "      <td>1630468713</td>\n",
       "      <td>hb5b25v</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>pfk90j</td>\n",
       "      <td>37</td>\n",
       "      <td>1.630487e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               body  created_utc       id link_id parent_id  score       created\n",
       "0          Etna  oh I misread, it's the males that are being an...   1630468787  hb5b6ss  pfk90j   hb5b25v     27  1.630487e+09\n",
       "3       lalauna     Please tell me something i didn't know before.   1630468726  hb5b2zz  pfgvrw    pfgvrw      1  1.630487e+09\n",
       "4  DivineBanana  I haven't followed the debate but I'm sure it'...   1630468726  hb5b2zy  pf5phr   hb58igp      3  1.630487e+09\n",
       "5   _MASTADONG_  I just linked you to 2 articles on fact checki...   1630468723  hb5b2sg  pfgvrw   hb5adks      4  1.630487e+09\n",
       "8          Etna                       It's because they can't talk   1630468713  hb5b25v  pfk90j    pfk90j     37  1.630487e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196985\n"
     ]
    }
   ],
   "source": [
    "## combine/clean\n",
    "comment_data = pd.DataFrame(comments_results)\n",
    "# drop deleted data\n",
    "comment_data = comment_data[(comment_data.loc[:, 'author']!='[deleted]') &\n",
    "                            (comment_data.loc[:, 'body']!='[deleted]')]\n",
    "# fix ID vars\n",
    "comment_data = comment_data.assign(**{\n",
    "    'link_id' : comment_data.loc[:, 'link_id'].apply(lambda x: x.split('_')[1]),\n",
    "    'parent_id' : comment_data.loc[:, 'parent_id'].apply(lambda x: x.split('_')[1]),\n",
    "})\n",
    "# drop extra data\n",
    "comment_data.drop('d_', axis=1, inplace=True)\n",
    "display(comment_data.head())\n",
    "print(comment_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict to comment/submission matches\n",
    "submission_comment_data = pd.merge(submission_data, comment_data, left_on='id', right_on='link_id', how='inner')\n",
    "# fix col names\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_x', '_submission') \n",
    "    for x in list(filter(lambda x: x.endswith('_x'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data.rename(columns={\n",
    "    x : x.replace('_y', '_comment') \n",
    "    for x in list(filter(lambda x: x.endswith('_y'), submission_comment_data.columns))\n",
    "}, inplace=True)\n",
    "submission_comment_data = submission_comment_data[submission_comment_data.loc[:, 'link_id']==submission_comment_data.loc[:, 'parent_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465354"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_comment_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130878 questions total\n"
     ]
    }
   ],
   "source": [
    "## clean text\n",
    "import re\n",
    "RETURN_MATCHER = re.compile('[\\n\\r]')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'body' : submission_comment_data.loc[:, 'body'].apply(lambda x: RETURN_MATCHER.sub(' ', x))\n",
    "})\n",
    "## filter questions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_sents' : submission_comment_data.loc[:, 'body'].apply(lambda x: sent_tokenize(x))\n",
    "})\n",
    "# look for questions!\n",
    "import re\n",
    "question_matcher = re.compile('\\?$')\n",
    "submission_comment_data = submission_comment_data.assign(**{\n",
    "    'reply_questions' : submission_comment_data.loc[:, 'reply_sents'].apply(lambda x: list(filter(lambda y: question_matcher.search(y) is not None, x)))\n",
    "})\n",
    "submission_question_data = submission_comment_data[submission_comment_data.loc[:, 'reply_questions'].apply(len)>0]\n",
    "## flatten\n",
    "flat_submission_question_data = []\n",
    "for idx_i, data_i in submission_question_data.iterrows():\n",
    "    for q_j in data_i.loc['reply_questions']:\n",
    "        data_j = data_i.copy().drop('reply_questions')\n",
    "        data_j.loc['reply_question'] = q_j\n",
    "        flat_submission_question_data.append(data_j)\n",
    "flat_submission_question_data = pd.concat(flat_submission_question_data, axis=1).transpose()\n",
    "print(f'{flat_submission_question_data.shape[0]} questions total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['die for the economy?', 'how about we topple you instead?',\n",
       "       'Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Have I misread something?',\n",
       "       \"Don't you care about the environment?\"], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## look at sample questions => clarification questions? self-contained? related to post?\n",
    "## sample questions\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(flat_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit the questions to have at least X words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Are we really trying to blame covid for why my political leadership is incompetent and worthless?',\n",
       "       'You mean covid unveils a psychological burden of perpetual political unrest in USA?',\n",
       "       'Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?',\n",
       "       'Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?',\n",
       "       'So, when they collide, they destroy each other.”  [source](https://www.cam.ac.uk/research/news/astronomers-show-how-planets-form-in-binary-systems-without-getting-crushed)  Am I missing something here?',\n",
       "       'How would these evictions **double** the Covid rate in an **area**?',\n",
       "       'Wait, who was saying there would be a pandemic baby boom and why?',\n",
       "       \"Isn't it established that stress and uncertainty eliminate the desire to be parents, or is that just my intuition?\",\n",
       "       'The baby boom thing was just a joke right?',\n",
       "       'Was there any real feeling that that would happen?'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "min_question_words = 10\n",
    "tokenizer = WordPunctTokenizer()\n",
    "valid_submission_question_data = flat_submission_question_data[flat_submission_question_data.loc[:, 'reply_question'].apply(lambda x: len(tokenizer.tokenize(x)) >= min_question_words)]\n",
    "display(valid_submission_question_data.loc[:, 'reply_question'].iloc[:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for posterity!!\n",
    "valid_submission_question_data.to_csv('science_submission_question_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "2it [00:03,  2.19s/it]/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "3099it [6:11:34,  6.67s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10000it [20:27:42,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "## TODO: mine previous history for N=10000 commenters; extract location + age + gender (?)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from psaw import PushshiftAPI\n",
    "pushshift_api = PushshiftAPI()\n",
    "N_commenters = 10000\n",
    "N_previous_comments = 1000\n",
    "sample_question_author_data = valid_submission_question_data.sort_values(['author_comment', 'created_utc_comment'], ascending=True).drop_duplicates('author_comment').loc[:, ['author_comment', 'created_utc_comment']]\n",
    "sample_question_author_data = sample_question_author_data.sample(N_commenters, replace=False, random_state=123)\n",
    "sample_question_author_prior_data = []\n",
    "author_filter_cols = ['body', 'id', 'created_utc', 'author', 'subreddit']\n",
    "for idx_i, data_i in tqdm(sample_question_author_data.iterrows()):\n",
    "    author_i = data_i.loc['author_comment']\n",
    "    time_i = int(data_i.loc['created_utc_comment'])\n",
    "    prior_comments_i = list(pushshift_api.search_comments(author=author_i, limit=N_previous_comments, before=time_i, filter=author_filter_cols))\n",
    "    prior_comments_i = pd.DataFrame(prior_comments_i)\n",
    "    if('d_' in prior_comments_i.columns):\n",
    "        prior_comments_i.drop('d_', axis=1, inplace=True)\n",
    "    sample_question_author_prior_data.append(prior_comments_i)\n",
    "sample_question_author_prior_data = pd.concat(sample_question_author_prior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_question_author_prior_data.to_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (0,1,2,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data = pd.read_csv('science_submission_question_reply_author_data.gz', sep='\\t', compression='gzip')\n",
    "sample_question_author_prior_data.dropna(subset=['body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>You're not even Scottish, are you?</td>\n",
       "      <td>1582310896</td>\n",
       "      <td>fiayfoz</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582329e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.</td>\n",
       "      <td>1582298322</td>\n",
       "      <td>fiacrvb</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582316e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.</td>\n",
       "      <td>1582291726</td>\n",
       "      <td>fia35ak</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582310e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...</td>\n",
       "      <td>1582258926</td>\n",
       "      <td>fi99f50</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582277e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mahogany555</td>\n",
       "      <td>Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog</td>\n",
       "      <td>1582258153</td>\n",
       "      <td>fi98eh0</td>\n",
       "      <td>ScottishPeopleTwitter</td>\n",
       "      <td>1.582276e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  \\\n",
       "0  mahogany555   \n",
       "1  mahogany555   \n",
       "2  mahogany555   \n",
       "3  mahogany555   \n",
       "4  mahogany555   \n",
       "\n",
       "                                                                                                                                                                                                                                              body  \\\n",
       "0                                                                                                                                                                                                               You're not even Scottish, are you?   \n",
       "1                                                                                                                                                         'The people of Scotland' don't spell it that way, losers seeking attention on reddit do.   \n",
       "2  Firstly, most of these people probably aren't Scots. Secondly the ones writing this are loser millennials doing it to look cool on reddit...\\n\\nPlease tell me normal scottish people don't do this and would thoroughly mock anyone who would.   \n",
       "3                                                                                                                   No, I definitely don't. I just stick to the 80000000 existing words that make up the language I'm writing in. Crazy, I know...   \n",
       "4                                                                                                             Do they that's super interesting...\\n\\nHere's the word 'dog' from the dictionary of that 'language'\\n\\nhttps://dsl.ac.uk/results/dog   \n",
       "\n",
       "  created_utc       id              subreddit       created  \n",
       "0  1582310896  fiayfoz  ScottishPeopleTwitter  1.582329e+09  \n",
       "1  1582298322  fiacrvb  ScottishPeopleTwitter  1.582316e+09  \n",
       "2  1582291726  fia35ak  ScottishPeopleTwitter  1.582310e+09  \n",
       "3  1582258926  fi99f50  ScottishPeopleTwitter  1.582277e+09  \n",
       "4  1582258153  fi98eh0  ScottishPeopleTwitter  1.582276e+09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(sample_question_author_prior_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='I live'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'416971/6804000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## check for self-disclosure statements\n",
    "import re\n",
    "self_disclosure_matcher = re.compile(f'(I\\'m|I am|I live)')\n",
    "print(self_disclosure_matcher.search('I live in MI'))\n",
    "sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "    'body_contain_self_disclosure' : sample_question_author_prior_data.loc[:, 'body'].apply(lambda x: self_disclosure_matcher.search(x) is not None)\n",
    "})\n",
    "display(f'{sample_question_author_prior_data.loc[:, \"body_contain_self_disclosure\"].sum()}/{sample_question_author_prior_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPULA_LEMMA = 'be'\n",
    "EXIST_LEMMA = 'live'\n",
    "GENDER_MATCHER = re.compile('^(man|woman|male|female)$')\n",
    "AGE_NUM_MATCHER = re.compile('[0-9]+')\n",
    "def collect_propn(token):\n",
    "    loc_noun_parts = [token]\n",
    "    location_children = list(filter(lambda x: x.pos_=='PROPN', token.children))\n",
    "    while(len(location_children) > 0):\n",
    "        loc_noun_part = location_children.pop()\n",
    "        loc_noun_parts.append(loc_noun_part)\n",
    "        location_children += list(filter(lambda x: x.pos_=='PROPN', loc_noun_part.children))\n",
    "    # sort noun parts\n",
    "    loc_noun_parts = list(sorted(loc_noun_parts, key=lambda x: x.idx))\n",
    "    loc_noun = ' '.join(list(map(lambda x: x.lemma_, loc_noun_parts)))\n",
    "    return loc_noun\n",
    "def extract_self_statement_targets(sent, pipeline=None):\n",
    "    if(pipeline is not None):\n",
    "        parse = pipeline(sent)\n",
    "    else:\n",
    "        parse = sent\n",
    "    identity_attributes = []\n",
    "    parse_sents = list(parse.doc.sents)\n",
    "    for parse_sent in parse_sents:\n",
    "        for token in parse_sent:\n",
    "            # get children nouns for \"I\" via root\n",
    "            if(token.lemma_ == 'I' and token.dep_ == 'nsubj'):\n",
    "                token_root_ancestors = list(filter(lambda x: x.dep_=='ROOT', token.ancestors))\n",
    "    #             print(f'parse ents = {list(map(lambda x: x.label_, parse.ents))}')\n",
    "                loc_ents = list(filter(lambda x: x.label_=='GPE', parse.ents))\n",
    "                if(len(token_root_ancestors) > 0):\n",
    "                    token_root = token_root_ancestors[0]\n",
    "                    root_children = list(token_root.children)\n",
    "                    if(token_root.lemma_ == COPULA_LEMMA):\n",
    "                        for child in root_children:\n",
    "                            if(child.dep_ == 'attr'):\n",
    "                                # gender\n",
    "                                gender_match = GENDER_MATCHER.match(child.lemma_)\n",
    "                                if(gender_match is not None):\n",
    "                                    identity_attributes.append(['gender', gender_match.group(0)])\n",
    "                                # age => NOPE false positives abound\n",
    "#                                 age_match = AGE_NUM_MATCHER.match(child.lemma_)\n",
    "#                                 if(age_match is not None):\n",
    "#                                     identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # age\n",
    "                            elif(child.dep_ == 'acomp'):\n",
    "                                if(child.lemma_ == 'old'):\n",
    "                                    # look for children (\"30 years old\")\n",
    "                                    age_children_1 = list(child.children)\n",
    "                                    if(len(age_children_1) > 0 and age_children_1[0].lemma_=='year'):\n",
    "                                        age_children_2 = list(age_children_1[0].children)\n",
    "                                        if(len(age_children_2) > 0):\n",
    "                                            age_match = AGE_NUM_MATCHER.match(age_children_2[0].lemma_)\n",
    "                                            if(age_match is not None):\n",
    "                                                identity_attributes.append(['age', age_match.group(0)])\n",
    "                            # location\n",
    "                            elif(child.dep_ == 'prep' and child.lemma_ == 'from'):\n",
    "                                location_children_1 = list(child.children)\n",
    "                                if(len(location_children_1) > 0 and location_children_1[0].pos_ == 'PROPN'):\n",
    "                                    ent_start = location_children_1[0].i\n",
    "    #                                 child_1_idx = location_children_1[0].idx\n",
    "    #                                 child_1_ent = \n",
    "    #                                 main_loc = location_children_1[0]\n",
    "    #                                 loc_noun = collect_propn(main_loc)\n",
    "                                    # find ENT that contains child\n",
    "    #                                 print(f'ent start = {ent_start}')\n",
    "    #                                 print(f'{[(x.start, x.end) for x in loc_ents]}')\n",
    "                                    containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "    #                                 print(f'containing loc ents {containing_loc_ents}')\n",
    "                                    if(len(containing_loc_ents) > 0):\n",
    "                                        loc_noun = containing_loc_ents[0].text\n",
    "                                        identity_attributes.append(['location', loc_noun])\n",
    "                    # \"I live in the US\"\n",
    "                    elif(token_root.lemma_ == EXIST_LEMMA):\n",
    "                        root_prep_children = list(filter(lambda x: x.lemma_=='in' and x.dep_=='prep', token_root.children))\n",
    "                        if(len(root_prep_children) > 0):\n",
    "                            prep_children_2 = list(filter(lambda x: x.pos_ == 'PROPN', root_prep_children[0].children))\n",
    "                            if(len(prep_children_2) > 0):\n",
    "                                ent_start = prep_children_2[0].i\n",
    "                                containing_loc_ents = list(filter(lambda x: x.start <= ent_start and x.end >= ent_start, loc_ents))\n",
    "                                if(len(containing_loc_ents)):\n",
    "                                    loc_noun = containing_loc_ents[0].text\n",
    "                                    identity_attributes.append(['location', loc_noun])\n",
    "    return identity_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent = I am 30 years old has attr [['age', '30']]\n",
      "sent = I am a woman has attr [['gender', 'woman']]\n",
      "sent = I live in London, England has attr [['location', 'London']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# dumb\n",
    "# self_statement = '(I\\'m|I am)'\n",
    "# age_matcher = f'({self_statement} [0-9]+ years old)|({self_statement} a [0-9]+ yo )'\n",
    "# gender_matcher = f'({self_statement} a[ a-zA-Z]? (man|woman|male|female))'\n",
    "# smart => parse then extract\n",
    "import spacy\n",
    "nlp_pipeline = spacy.load('en_core_web_sm')\n",
    "# gender_dep = attr (\"I'm a man\")\n",
    "# age_dep = acomp (\"I'm 50 years old\")\n",
    "# loc_dep = prep (\"I live in Michigan\")\n",
    "test_sents = [\n",
    "    'I am 30 years old',\n",
    "    'I am a woman',\n",
    "    'I live in London, England',\n",
    "]\n",
    "for sent in test_sents:\n",
    "    sent_attr = extract_self_statement_targets(sent, nlp_pipeline)\n",
    "    print(f'sent = {sent} has attr {sent_attr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have an extremely brittle attribute extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nope\n",
    "# ## get parses first\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# from pandarallel import pandarallel\n",
    "# # pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "#     'body_parse' : list(tqdm(nlp_pipeline.pipe(sample_question_author_prior_data.loc[:, 'body'].values, batch_size=1000)))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parallelized pipeline\n",
    "# https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html#Option-3:-Parallelize-the-work-using-joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "N_JOBS=7\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "def process_chunk(texts):\n",
    "    proc_results = []\n",
    "    for doc in nlp_pipeline.pipe(texts, batch_size=20):\n",
    "#         preproc_pipe.append(lemmatize_pipe(doc))\n",
    "        proc_results.append(doc)\n",
    "    return proc_results\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=N_JOBS, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in tqdm(chunker(texts, len(texts), chunksize=chunksize)))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [18:22,  2.04s/it]"
     ]
    }
   ],
   "source": [
    "# takes too much memory!!\n",
    "# body_parse = preprocess_parallel(sample_question_author_prior_data.loc[:, 'body'].values, chunksize=1000)\n",
    "# print(body_parse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "askmen                   16907\n",
       "askwomen                  3898\n",
       "mensrights                2745\n",
       "menwritingwomen           1279\n",
       "menslib                    903\n",
       "whereareallthegoodmen      583\n",
       "mentalhealth               513\n",
       "watchmen                   453\n",
       "menopause                  444\n",
       "mensa                      299\n",
       "xmen                       291\n",
       "redpillwomen               274\n",
       "mendrawingwomen            230\n",
       "ramen                      208\n",
       "askgaymen                  204\n",
       "armoredwomen               187\n",
       "madmen                     150\n",
       "adhdwomen                  137\n",
       "mentalillness              123\n",
       "menkampf                   103\n",
       "women                       99\n",
       "menshealth                  59\n",
       "men                         55\n",
       "autisminwomen               50\n",
       "menieres                    48\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "askwomen            3898\n",
       "menwritingwomen     1279\n",
       "redpillwomen         274\n",
       "mendrawingwomen      230\n",
       "armoredwomen         187\n",
       "adhdwomen            137\n",
       "womenshealth         109\n",
       "women                 99\n",
       "autisminwomen         50\n",
       "womenofcolor          20\n",
       "prettyolderwomen      17\n",
       "womensstreetwear      17\n",
       "justhotwomen          16\n",
       "womensrightsnews      14\n",
       "bluecollarwomen       11\n",
       "womenengineers        10\n",
       "womenwhodontsell       9\n",
       "darkestwomen           8\n",
       "whipped_women          8\n",
       "womenwritingmen        7\n",
       "womenbendingover       7\n",
       "actualwomen            5\n",
       "womensfashion          5\n",
       "womenstyleadvice       4\n",
       "womenofcolorxxx        4\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nyc           3219\n",
       "losangeles    3279\n",
       "chicago       4038\n",
       "houston       2261\n",
       "phoenix        714\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "oldschoolcool          15862\n",
       "oldpeoplefacebook        446\n",
       "oldphotosinreallife      364\n",
       "youngpeopleyoutube       356\n",
       "gold                     293\n",
       "old_recipes              266\n",
       "galaxyfold               184\n",
       "goforgold                180\n",
       "oldschoolhot             173\n",
       "negativewithgold         141\n",
       "fuckimold                121\n",
       "oldmandog                117\n",
       "youngjustice             116\n",
       "oldschoolcoolnsfw        110\n",
       "oldfreefolk              107\n",
       "cuckold                  106\n",
       "gayyoungold              105\n",
       "mold                      94\n",
       "oldschoolcelebs           84\n",
       "freezingfuckingcold       64\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show \"men\"/\"women\" subreddits\n",
    "subreddit_counts = sample_question_author_prior_data.loc[:, 'subreddit'].value_counts()\n",
    "subreddit_counts.index = list(map(lambda x: str(x.lower()), subreddit_counts.index))\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^men)|(men$)', x) is not None, subreddit_counts.index))].sort_values(ascending=False).head(25))\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^women)|(women$)', x) is not None, subreddit_counts.index))].sort_values(ascending=False).head(25))\n",
    "# show location subreddits\n",
    "display(subreddit_counts.loc[['nyc', 'losangeles', 'chicago', 'houston', 'phoenix']])\n",
    "# show age subreddits\n",
    "display(subreddit_counts.loc[list(filter(lambda x: re.search('(^old)|(old$)|(^young)|(young$)', x), subreddit_counts.index))].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416971/416971 [1:55:00<00:00, 60.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "## extract all attributes!!\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "out_file_name = 'science_sample_reply_author_attrs.gz'\n",
    "id_cols = ['author', 'created_utc']\n",
    "self_disclosure_author_prior_data = sample_question_author_prior_data[sample_question_author_prior_data.loc[:, 'body_contain_self_disclosure']]\n",
    "self_disclosure_author_prior_data = self_disclosure_author_prior_data.assign(**{\n",
    "    'id_attrs' : self_disclosure_author_prior_data.loc[:, 'body'].progress_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline))\n",
    "})\n",
    "# pandarallel => memory isssues\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# sample_question_author_prior_data = sample_question_author_prior_data.assign(**{\n",
    "#     'identity_attributes' : sample_question_author_prior_data.loc[:, 'body'].parallel_apply(lambda x: extract_self_statement_targets(x, nlp_pipeline)),\n",
    "# #     'identity_attributes' : sample_question_author_prior_data.loc[:, 'body_parse'].progress_apply(lambda x: extract_self_statement_targets(x))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3292/3292 [00:25<00:00, 129.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-BluBone-</td>\n",
       "      <td>man</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Cerberus</td>\n",
       "      <td>man</td>\n",
       "      <td>41</td>\n",
       "      <td>KC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Daetrax-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Negative-Karma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Nycter-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>saudi arabia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author gender  age      location\n",
       "0        -BluBone-    man  NaN           NaN\n",
       "0        -Cerberus    man   41            KC\n",
       "0        -Daetrax-    NaN  NaN       Denmark\n",
       "0  -Negative-Karma    NaN  NaN        Kansas\n",
       "0         -Nycter-    NaN  NaN  saudi arabia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_author_attribute_data = self_disclosure_author_prior_data[self_disclosure_author_prior_data.loc[:, 'id_attrs'].apply(lambda x: len(x) > 0)]\n",
    "# get one line per author\n",
    "flat_author_attribute_data = []\n",
    "for author_i, data_i in tqdm(question_author_attribute_data.groupby('author')):\n",
    "    attr_data_i = []\n",
    "    for idx_j, data_j in data_i.iterrows():\n",
    "        # keep track of dates!!\n",
    "        date_j = data_j.loc['created_utc']\n",
    "        for attr_k, val_k in data_j.loc['id_attrs']:\n",
    "            attr_data_i.append({\n",
    "                'author' : author_i,\n",
    "                'date' : date_j,\n",
    "                'attr' : attr_k,\n",
    "                'val' : val_k\n",
    "            })\n",
    "    attr_data_i = pd.DataFrame(attr_data_i)\n",
    "    attr_data_i.sort_values(['attr', 'date'], inplace=True, ascending=False)\n",
    "    attr_data_i = attr_data_i.drop_duplicates(['attr'], keep='first').drop('date', axis=1)    \n",
    "    attr_data_i = attr_data_i.pivot(index='author', columns=['attr'], values=['val']).reset_index()\n",
    "    attr_data_i.columns = list(map(lambda x: x[0] if x[1]=='' else x[1], attr_data_i.columns))\n",
    "    flat_author_attribute_data.append(attr_data_i)\n",
    "flat_author_attribute_data = pd.concat(flat_author_attribute_data, axis=0)\n",
    "display(flat_author_attribute_data.head())\n",
    "## save for later\n",
    "flat_author_attribute_data.to_csv('science_reply_author_attr_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US            150\n",
      "Canada        100\n",
      "UK             81\n",
      "Texas          79\n",
      "California     62\n",
      "Florida        52\n",
      "America        49\n",
      "Australia      39\n",
      "Germany        35\n",
      "USA            34\n",
      "Name: location, dtype: int64\n",
      "man       789\n",
      "woman     275\n",
      "male      184\n",
      "female     48\n",
      "Name: gender, dtype: int64\n",
      "30    19\n",
      "20    11\n",
      "40    11\n",
      "33    10\n",
      "5     10\n",
      "31     9\n",
      "10     9\n",
      "50     8\n",
      "36     6\n",
      "19     6\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_attr = ['location', 'gender', 'age']\n",
    "for attr_i in all_attr:\n",
    "    print(flat_author_attribute_data.loc[:, attr_i].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This isn't the best but we'll see what we can do with the aggregate categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median age = 30.000\n"
     ]
    }
   ],
   "source": [
    "# fix age\n",
    "import numpy as np\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'age' : flat_author_attribute_data.loc[:, 'age'].apply(lambda x: x if type(x) is float and np.isnan(x) else int(x))\n",
    "})\n",
    "max_age = 100\n",
    "flat_author_attribute_data = flat_author_attribute_data[flat_author_attribute_data.loc[:, 'age'].apply(lambda x: np.isnan(x) or x <= max_age)]\n",
    "## plot age distribution\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "age_vals = flat_author_attribute_data.loc[:, 'age'].dropna().astype(int)\n",
    "plt.hist(age_vals)\n",
    "plt.show()\n",
    "print(f'median age = {age_vals.median():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>0.806806</td>\n",
       "      <td>dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>0.826542</td>\n",
       "      <td>us</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saudi arabia</td>\n",
       "      <td>0.735263</td>\n",
       "      <td>sa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.841632</td>\n",
       "      <td>se</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Italy</td>\n",
       "      <td>0.883102</td>\n",
       "      <td>it</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0.800803</td>\n",
       "      <td>us</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California</td>\n",
       "      <td>0.922136</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Canada</td>\n",
       "      <td>0.976126</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baltimore</td>\n",
       "      <td>0.765295</td>\n",
       "      <td>us</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>0.810392</td>\n",
       "      <td>us</td>\n",
       "      <td>Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Riverside</td>\n",
       "      <td>0.673493</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finland</td>\n",
       "      <td>0.907328</td>\n",
       "      <td>fi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Orange County</td>\n",
       "      <td>0.793026</td>\n",
       "      <td>us</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NZ</td>\n",
       "      <td>0.791711</td>\n",
       "      <td>nz</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DC</td>\n",
       "      <td>0.749289</td>\n",
       "      <td>us</td>\n",
       "      <td>District of Columbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>0.786380</td>\n",
       "      <td>us</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Grand Rapids</td>\n",
       "      <td>0.765069</td>\n",
       "      <td>us</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>US</td>\n",
       "      <td>0.935691</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alberta</td>\n",
       "      <td>0.782431</td>\n",
       "      <td>ca</td>\n",
       "      <td>Alberta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         location  accuracy country_code                 state\n",
       "2         Denmark  0.806806           dk                   NaN\n",
       "3          Kansas  0.826542           us                Kansas\n",
       "4    saudi arabia  0.735263           sa                   NaN\n",
       "5         Germany  0.889681           de                   NaN\n",
       "6          Sweden  0.841632           se                   NaN\n",
       "7           Italy  0.883102           it                   NaN\n",
       "8         Atlanta  0.800803           us               Georgia\n",
       "9      California  0.922136           us            California\n",
       "10         Canada  0.976126           ca                   NaN\n",
       "11      Baltimore  0.765295           us              Maryland\n",
       "12    Mississippi  0.810392           us           Mississippi\n",
       "13      Riverside  0.673493           us            California\n",
       "15        Finland  0.907328           fi                   NaN\n",
       "16  Orange County  0.793026           us            California\n",
       "17             NZ  0.791711           nz                   NaN\n",
       "18             DC  0.749289           us  District of Columbia\n",
       "19       Brooklyn  0.786380           us              New York\n",
       "20   Grand Rapids  0.765069           us              Michigan\n",
       "21             US  0.935691           us                   NaN\n",
       "22        Alberta  0.782431           ca               Alberta"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "us    259\n",
       "gb     31\n",
       "ca     29\n",
       "cn     10\n",
       "au     10\n",
       "it      9\n",
       "mx      6\n",
       "nl      6\n",
       "de      6\n",
       "co      5\n",
       "jp      5\n",
       "in      4\n",
       "es      4\n",
       "fr      4\n",
       "ch      4\n",
       "th      3\n",
       "kr      3\n",
       "br      3\n",
       "be      3\n",
       "ar      3\n",
       "Name: country_code, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "California          32\n",
       "Texas               25\n",
       "England             21\n",
       "New York            17\n",
       "Florida             11\n",
       "Illinois            11\n",
       "Washington           9\n",
       "Ohio                 8\n",
       "Pennsylvania         8\n",
       "Ontario              7\n",
       "British Columbia     6\n",
       "Virginia             6\n",
       "Michigan             6\n",
       "North Dakota         5\n",
       "Wisconsin            5\n",
       "North Carolina       5\n",
       "Québec               5\n",
       "Massachusetts        5\n",
       "South Carolina       5\n",
       "Oregon               5\n",
       "Name: state, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fix location => geolocate!!\n",
    "from geocoder import osm\n",
    "unique_locations = flat_author_attribute_data.loc[:, 'location'].unique()\n",
    "unique_location_geo_data = list(tqdm(map(lambda x: osm(x), unique_locations), total=len(unique_locations)))\n",
    "unique_location_geo_data = list(map(lambda x: x.json()))\n",
    "unique_location_geo_data_df = list(map(lambda x: x[0].json if len(x) > 0 else None, unique_location_geo_data))\n",
    "[x.update({'location' : y}) for x,y in zip(unique_location_geo_data_df, unique_locations) if x is not None]\n",
    "unique_location_geo_data_df = list(filter(lambda x: x is not None, unique_location_geo_data_df))\n",
    "unique_location_geo_data_df = pd.DataFrame(unique_location_geo_data_df)#.assign(**{'location' : unique_locations})\n",
    "unique_location_geo_data_df = unique_location_geo_data_df.loc[:, ['location', 'accuracy', 'country_code', 'state']]\n",
    "# drop low-confidence scores\n",
    "loc_conf_cutoff = 0.5\n",
    "unique_location_geo_data_df = unique_location_geo_data_df[unique_location_geo_data_df.loc[:, 'accuracy'] >= loc_conf_cutoff]\n",
    "# remove nan vals\n",
    "unique_location_geo_data_df = unique_location_geo_data_df[unique_location_geo_data_df.loc[:, 'location'].apply(lambda x: type(x) is str)]\n",
    "display(unique_location_geo_data_df.head(20))\n",
    "display(unique_location_geo_data_df.loc[:, 'country_code'].value_counts().head(20))\n",
    "display(unique_location_geo_data_df.loc[:, 'state'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not great!! But we have to do something with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    972\n",
       "F    323\n",
       "Name: norm_gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30+    129\n",
       "<30    120\n",
       "Name: norm_age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "US        1324\n",
       "non_US     927\n",
       "Name: norm_location, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## normalize all the demographics\n",
    "# gender\n",
    "gender_norm_lookup = {\n",
    "    'man' : 'M',\n",
    "    'male' : 'M',\n",
    "    'woman' : 'F',\n",
    "    'female' : 'F',\n",
    "}\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_gender' : flat_author_attribute_data.loc[:, 'gender'].apply(gender_norm_lookup.get)\n",
    "})\n",
    "# age\n",
    "age_cutoff = 30\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_age' : flat_author_attribute_data.loc[:, 'age'].apply(lambda x: x if np.isnan(x) else f'{age_cutoff}+' if x >= age_cutoff else f'<{age_cutoff}')\n",
    "})\n",
    "# location\n",
    "location_country_lookup = dict(zip(unique_location_geo_data_df.loc[:, 'location'].values, \n",
    "                                   unique_location_geo_data_df.loc[:, 'country_code'].values))\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_location' : flat_author_attribute_data.loc[:, 'location'].apply(location_country_lookup.get)\n",
    "})\n",
    "# simplify US vs. non-US\n",
    "flat_author_attribute_data = flat_author_attribute_data.assign(**{\n",
    "    'norm_location' : flat_author_attribute_data.loc[:, 'norm_location'].apply(lambda x: 'US' if x=='us' else 'non_US' if type(x) is str else None)\n",
    "})\n",
    "## show all distributions\n",
    "demo_vars = ['norm_gender', 'norm_age', 'norm_location']\n",
    "for demo_var_i in demo_vars:\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    display(flat_author_attribute_data.loc[:, demo_var_i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge with the full question data and see if we can differentiate the groups based on questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n",
      "demo = norm_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    1613\n",
       "F     513\n",
       "Name: norm_gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<30    177\n",
       "30+    173\n",
       "Name: norm_age, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "US        2019\n",
       "non_US    1359\n",
       "Name: norm_location, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "valid_submission_question_data = pd.merge(valid_submission_question_data,\n",
    "                                          flat_author_attribute_data.rename(columns={'author' : 'author_comment'}).loc[:, ['author_comment',]+demo_vars],\n",
    "                                          on='author_comment', how='inner')\n",
    "print(valid_submission_question_data.shape[0])\n",
    "for demo_var_i in demo_vars:\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    display(valid_submission_question_data.loc[:, demo_var_i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less terrible than I thought. Let's look for some differences in question asking!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n",
      "top words for val = M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "produce       2.060049\n",
       "natural       1.991056\n",
       "probably      1.991056\n",
       "law           1.836905\n",
       "price         1.749894\n",
       "away          1.749894\n",
       "factor        1.749894\n",
       "antibodies    1.654584\n",
       "diet          1.654584\n",
       "reduce        1.654584\n",
       "use           1.654584\n",
       "tests         1.654584\n",
       "agree         1.654584\n",
       "caused        1.549223\n",
       "chance        1.549223\n",
       "man           1.549223\n",
       "plastic       1.549223\n",
       "jobs          1.549223\n",
       "keep          1.549223\n",
       "outside       1.549223\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fish              -2.439761\n",
       "14                -2.034296\n",
       "obese             -2.034296\n",
       "pain              -2.034296\n",
       "alone             -2.034296\n",
       "institutions      -2.034296\n",
       "inquisitiveness   -2.034296\n",
       "proof             -2.034296\n",
       "risks             -2.034296\n",
       "games             -2.034296\n",
       "americans         -2.034296\n",
       "abstract          -2.034296\n",
       "careful           -2.034296\n",
       "binaural          -2.034296\n",
       "beats             -2.034296\n",
       "infected          -2.034296\n",
       "regulation        -1.746614\n",
       "reduces           -1.746614\n",
       "headed            -1.746614\n",
       "evaluate          -1.746614\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n",
      "top words for val = <30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "risk       1.769956\n",
       "based      1.587634\n",
       "lack       1.587634\n",
       "..         1.587634\n",
       "white      1.587634\n",
       "warmer     1.587634\n",
       "100        1.587634\n",
       "back       1.364490\n",
       "stream     1.364490\n",
       "things     1.364490\n",
       "virus      1.364490\n",
       "number     1.364490\n",
       "old        1.364490\n",
       "useful     1.364490\n",
       "certain    1.364490\n",
       "gun        1.364490\n",
       "gulf       1.364490\n",
       "natural    1.364490\n",
       "theory     1.364490\n",
       "correct    1.364490\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = 30+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "next         -1.967714\n",
       "country      -1.631242\n",
       "trying       -1.631242\n",
       "help         -1.631242\n",
       "please       -1.631242\n",
       "everything   -1.408098\n",
       "basic        -1.408098\n",
       "making       -1.408098\n",
       "found        -1.408098\n",
       "getting      -1.408098\n",
       "say          -1.408098\n",
       "explain      -1.408098\n",
       "happens      -1.408098\n",
       "may          -1.408098\n",
       "true         -1.408098\n",
       "single       -1.408098\n",
       "plastic      -1.408098\n",
       "feel         -1.408098\n",
       "seen         -1.120416\n",
       "seem         -1.120416\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n",
      "top words for val = US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "support      2.379986\n",
       "blood        2.138824\n",
       "policy       2.138824\n",
       "born         2.043514\n",
       "super        1.938153\n",
       "private      1.820370\n",
       "gain         1.820370\n",
       "play         1.820370\n",
       "hair         1.820370\n",
       "universe     1.820370\n",
       "ocean        1.755831\n",
       "state        1.755831\n",
       "combat       1.686839\n",
       "basis        1.686839\n",
       "single       1.686839\n",
       "meant        1.686839\n",
       "serious      1.686839\n",
       "changing     1.686839\n",
       "stress       1.686839\n",
       "naturally    1.686839\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = non_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "discrimination   -2.204982\n",
       "bring            -2.204982\n",
       "stereotypes      -2.050831\n",
       "online           -2.050831\n",
       "therapy          -2.050831\n",
       "substance        -1.868509\n",
       "~                -1.868509\n",
       "underlying       -1.868509\n",
       "unhealthy        -1.868509\n",
       "authority        -1.868509\n",
       "parties          -1.868509\n",
       "behaviour        -1.868509\n",
       "racial           -1.868509\n",
       "build            -1.868509\n",
       "union            -1.868509\n",
       "somewhere        -1.868509\n",
       "chinese          -1.868509\n",
       "victim           -1.868509\n",
       "1000             -1.868509\n",
       "fuck             -1.868509\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import twitter_data_helpers\n",
    "reload(twitter_data_helpers)\n",
    "from twitter_data_helpers import compute_log_odds\n",
    "text_var = 'reply_question'\n",
    "top_k_words = 20\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    (val_1_i, val_2_i), word_ratio_i = compute_log_odds(data_i, text_var, demo_var_i)\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    print(f'top words for val = {val_1_i}')\n",
    "    display(word_ratio_i.head(top_k_words))\n",
    "    print(f'top words for val = {val_2_i}')\n",
    "    display(word_ratio_i.sort_values(ascending=True).head(top_k_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gender\n",
    "    - `male`: economics (`price`, `jobs`), health (`antibodies`, `diet`), male (`man`), discussion (`agree`, `expect`), causality (`produce`, `law`, `factor`, `reduce`, `caused`)\n",
    "    - `female`: organizations (`americans`, `institutions`, `regulation`), health (`infected`, `obese`), caution (`proof`, `risks`, `careful`, `evaluate`), negative experience (`pain`, `alone`)\n",
    "- Age\n",
    "    - `<30`: theoretical (`theory`, `natural`), numeric (`100`, `number`), certainty (`certain`, `correct`, `useful`)\n",
    "    - `30+`: discussion (`please`, `explain`, `say`), uncertainty (`trying`, `may`, `feel`, `seem`), simplicity (`basic`, `everything`, `single`)\n",
    "- Location\n",
    "    - `US`: nature (`universe`, `ocean`), body (`blood`, `born`, `hair`), conflict (`combat`, `stress`)\n",
    "    - `non_US`: social problems (`discrimination`, `stereotypes`, `underlying`, `racial`, `victim`, `unhealthy`), labor (`authority`, `union`), orthography (`~`, `behaviour`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ACHIEV         re.compile('^(abilit.*|able|accomplish.*|ace|achievable|achieve.*|achievi.*|acquir.*|acquisition.*|actualiz.*|adequa.*|advanc.*|advantag.*|ahead|ambition|ambitions|ambitious|ambitiously|ambitiousness|attain|atta)\n",
       "ADJ            re.compile(\"^(abnormal.*|academic|active|additional|affordable|afraid|after|aggressive|agreeable|alike|alive|alone|amazing|ambitious|ancient|angrier|angriest|angry|annoying|antisocial|anxious|apparent|approachab)\n",
       "ADVERB         re.compile(\"^(about|absolutely|actually|again|almost|already|also|anyway.*|anywhere|apparently|around|awhile|back|barely|basically|beyond|briefly|clearly|commonly|completely|constantly|continually|definitely|esp)\n",
       "AFFECT         re.compile(\"^(abandon.*|abuse.*|abusi.*|accept|accepta.*|accepted|accepting|accepts|ache.*|aching.*|active|actively|admir.*|ador.*|advantag.*|adventur.*|advers.*|affection.*|afraid|aggravat.*|aggress|aggressed|a)\n",
       "AFFILIATION    re.compile(\"^(accompan.*|accomplice.*|affil.*|alliance.*|allies|ally|amigo.*|associate|associates|associating|association|associations|bae|banter.*|belong.*|bestfriend.*|bf|bff.*|bfs|boyfriend.*|breakup|bro|bro')\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^(abilit.*|able|accomplish.*|ace|achievable|achieve.*|achievi.*|acquir.*|acquisition.*|actualiz.*|adequa.*|advanc.*|advantag.*|ahead|ambition|ambitions|ambitious|ambitiously|ambitiousness|attain|attainable|attained|attaining|attainment|attains|authorit.*|award.*|beat|beaten|best|better|bonus.*|burnout.*|capab.*|celebrat.*|challeng.*|champ.*|cheat.*|climb.*|compet.*|confidence|confident|confidently|conquer.*|conscientious.*|create|created|creates|creating|creation|creations|creative|creativity|defeat.*|demot.*|determina.*|determined|diligen.*|domina.*|driven|dropout.*|earn|earned|earning|earns|efficien.*|effort.*|elit.*|emptier|emptiest|emptiness|empty|enabl.*|endeav.*|excel|excellent|excels|fail.*|finaliz.*|first|firsts|flunk.*|founded|founder.*|founding|fulfill.*|gain.*|glory|goal.*|gpa|honor.*|honour.*|ideal.*|importance|improve.*|improving|inadequa.*|incapab.*|incentive.*|incompeten.*|ineffect.*|initiat.*|irresponsible.*|lazier|laziest|lazy|lead|leader.*|leading|leads|limit.*|lose|loser.*|loses|losing|loss.*|lost|mastered|mastery|medal.*|mediocr.*|motiv.*|obtain|obtainable|obtained|obtaining|obtains|opportun.*|overcame|overcome|overcomes|overcoming|overconfiden.*|overtak.*|perfected|perfecting|perfection|perfectly|perfects|persever.*|persist.*|plan|planned|planning|plans|potential.*|powerful|powerless.*|practice|practiced|practices|practicing|prais.*|pride|prize.*|proficien.*|progress|promot.*|proud|prouder|proudest|proudly|purpose.*|queen|quit|quitt.*|rank|ranked|ranking|ranks|recover.*|resolv.*|resourceful.*|reward.*|skill.*|solution.*|solve|solved|solves|solving|strateg.*|striv.*|succeed.*|success|successes|successful|successfully|super|superb.*|surpass.*|surviv.*|team.*|top|tried|tries|triumph.*|try|trying|unable|unbeat.*|unproduc.*|unsuccessful.*|victor.*|win|winn.*|wins|won|work|workabl.*|worked|worker.*|working|works)$\n"
     ]
    }
   ],
   "source": [
    "## same thing but LIWC categories\n",
    "import re\n",
    "LIWC_data = pd.read_csv('/home/cfwelch/LIWC.2015.all', sep=',', header=None)\n",
    "LIWC_data.columns = ['word', 'category']\n",
    "LIWC_data = LIWC_data.assign(**{'word' : LIWC_data.loc[:, 'word'].apply(lambda x: x.replace('*', '.*').strip())})\n",
    "def try_compile(x):\n",
    "    try:\n",
    "        return re.compile(x)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "LIWC_data = LIWC_data.assign(**{'word_matcher' : LIWC_data.loc[:, 'word'].apply(try_compile)})\n",
    "LIWC_data = LIWC_data[LIWC_data.loc[:, 'word_matcher'].apply(lambda x: x is not None)]\n",
    "LIWC_combined_word_patterns = LIWC_data.groupby('category').apply(lambda x: re.compile('^(' + '|'.join(x.loc[:, 'word_matcher'].apply(lambda y: y.pattern)) + ')$'))\n",
    "# get rid of bad categories\n",
    "LIWC_filter_categories = ['NETSPEAK']\n",
    "LIWC_combined_word_patterns.drop(LIWC_filter_categories, inplace=True)\n",
    "display(LIWC_combined_word_patterns.head())\n",
    "print(LIWC_combined_word_patterns.loc['ACHIEV'].pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender\n",
      "top words for val = M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THEY           1.223712\n",
       "YOU            0.792929\n",
       "INGEST         0.605466\n",
       "MONEY          0.465764\n",
       "FOCUSFUTURE    0.340646\n",
       "SPACE          0.263618\n",
       "ACHIEV         0.261436\n",
       "MALE           0.250262\n",
       "MOTION         0.247783\n",
       "NUMBER         0.244507\n",
       "ASSENT         0.220409\n",
       "PPRON          0.191161\n",
       "WE             0.182258\n",
       "QUANT          0.173889\n",
       "RELATIV        0.170115\n",
       "PREP           0.156940\n",
       "SEE            0.155540\n",
       "CAUSE          0.143676\n",
       "DISCREP        0.119369\n",
       "HEAR           0.114230\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SHEHE         -1.078874\n",
       "FAMILY        -0.593366\n",
       "HOME          -0.422094\n",
       "FILLER        -0.385726\n",
       "FEEL          -0.365524\n",
       "SAD           -0.331659\n",
       "HEALTH        -0.306193\n",
       "DIFFER        -0.303355\n",
       "INTERROG      -0.283944\n",
       "AFFILIATION   -0.278628\n",
       "DEATH         -0.267943\n",
       "NEGEMO        -0.246174\n",
       "RELIG         -0.238090\n",
       "IPRON         -0.226903\n",
       "RISK          -0.208392\n",
       "ANX           -0.168313\n",
       "SEXUAL        -0.162583\n",
       "AFFECT        -0.134548\n",
       "LEISURE       -0.119458\n",
       "ANGER         -0.105424\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_age\n",
      "top words for val = <30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RELIG       0.883518\n",
       "FEEL        0.798360\n",
       "RISK        0.506224\n",
       "POSEMO      0.420179\n",
       "NEGATE      0.392895\n",
       "PREP        0.354674\n",
       "ADVERB      0.329568\n",
       "HEALTH      0.323902\n",
       "INTERROG    0.259364\n",
       "DISCREP     0.259364\n",
       "COMPARE     0.253943\n",
       "INGEST      0.236891\n",
       "ANX         0.236891\n",
       "NONFLU      0.218542\n",
       "QUANT       0.203274\n",
       "DIFFER      0.203274\n",
       "CERTAIN     0.190371\n",
       "ADJ         0.181932\n",
       "AFFECT      0.180470\n",
       "BIO         0.138874\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = 30+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FRIEND        -0.944609\n",
       "HEAR          -0.811078\n",
       "SHEHE         -0.656927\n",
       "SWEAR         -0.656927\n",
       "I             -0.656927\n",
       "FILLER        -0.656927\n",
       "LEISURE       -0.620560\n",
       "MALE          -0.474606\n",
       "MONEY         -0.426404\n",
       "AFFILIATION   -0.320455\n",
       "FOCUSPAST     -0.295137\n",
       "SOCIAL        -0.257852\n",
       "FEMALE        -0.251462\n",
       "PPRON         -0.251462\n",
       "HOME          -0.251462\n",
       "IPRON         -0.251462\n",
       "ANGER         -0.238217\n",
       "PRONOUN       -0.233444\n",
       "ACHIEV        -0.232044\n",
       "FOCUSFUTURE   -0.197395\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_location\n",
      "top words for val = US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "THEY           1.121185\n",
       "WE             0.455437\n",
       "FAMILY         0.355718\n",
       "PPRON          0.303566\n",
       "ANX            0.237684\n",
       "SAD            0.221844\n",
       "REWARD         0.197831\n",
       "RISK           0.178854\n",
       "FILLER         0.176724\n",
       "FOCUSFUTURE    0.175280\n",
       "HOME           0.173396\n",
       "MONEY          0.172748\n",
       "ASSENT         0.147736\n",
       "INFORMAL       0.136064\n",
       "I              0.133799\n",
       "MALE           0.119737\n",
       "WORK           0.107314\n",
       "DISCREP        0.102000\n",
       "NONFLU         0.098081\n",
       "MOTION         0.075948\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for val = non_US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SHEHE         -0.670574\n",
       "CONJ          -0.370916\n",
       "SEXUAL        -0.360913\n",
       "INTERROG      -0.287582\n",
       "ANGER         -0.255961\n",
       "QUANT         -0.205378\n",
       "SEE           -0.189106\n",
       "BODY          -0.152182\n",
       "NEGATE        -0.144481\n",
       "DIFFER        -0.141730\n",
       "DEATH         -0.121527\n",
       "BIO           -0.116229\n",
       "HEALTH        -0.113288\n",
       "NUMBER        -0.104582\n",
       "FEMALE        -0.100029\n",
       "AFFILIATION   -0.097475\n",
       "CERTAIN       -0.095210\n",
       "PERCEPT       -0.074700\n",
       "HEAR          -0.070020\n",
       "ADVERB        -0.060119\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import twitter_data_helpers\n",
    "reload(twitter_data_helpers)\n",
    "from twitter_data_helpers import compute_log_odds\n",
    "text_var = 'reply_question'\n",
    "top_k_words = 20\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    (val_1_i, val_2_i), word_ratio_i = compute_log_odds(data_i, text_var, demo_var_i, word_categories=LIWC_combined_word_patterns)\n",
    "    val_1_counts_i = word_ratio_i[word_ratio_i > 0.].head(top_k_words)\n",
    "    print(f'demo = {demo_var_i}')\n",
    "    print(f'top words for val = {val_1_i}')\n",
    "    display(val_1_counts_i)\n",
    "    val_2_counts_i = word_ratio_i[word_ratio_i < 0.].sort_values(ascending=True).head(top_k_words)\n",
    "    print(f'top words for val = {val_2_i}')\n",
    "    display(val_2_counts_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gender\n",
    "    - `male`: others (`THEY`, `YOU`), quantity (`MONEY`, `NUMBER`, `QUANT`), self (`MALE`, `WE`, `PPRON`)\n",
    "    - `female`: close relations (`SHEHE`, `FAMILY`, `HOME`), emotion (`SAD`, `FEEL`, `NEGEMO`), health (`HEALTH`, `DEATH`)\n",
    "- Age\n",
    "    - `<30`: emotion (`POSEMO`, `FEEL`, `AFFECT`), health (`HEALTH`, `INGEST`, `BIO`), certainty (`RISK`, `COMPARE`, `CERTAIN`)\n",
    "    - `30+`: social (`FRIEND`, `SHEHE`, `MALE`, `SOCIAL`), negative emotion (`NEGEMO`, `ANGER`), social institutions (`MOENY`, `AFFILIATION`), time (`FOCUSPAST`, `FOCUSFUTURE`), comfort (`LEISURE`, `HOME`)\n",
    "- Location\n",
    "    - `US`: social (`THEY`, `WE`, `FAMILY`, `PPRON`), negative emotion (`SAD`, `ANX`), action (`RISK`, `FOCUSFUTURE`, `WORK`, `REWARD`)\n",
    "    - `non_US`: immediate social (`SHEHE`), health (`SEXUAL`, `BODY`, `BIO`, `HEALTH`, `DEATH`), quantity (`QUANT`, `NUMBER`), evidence (`INTERROG`, `SEE`, `NEGATE`, `CERTAIN`, `PERCEPT`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some example questions for each group? Let's see if these would actually be useful for writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_gender; val=F\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pf6roe</td>\n",
       "      <td>The Pandemic Caused a Baby Bust, Not a Boom</td>\n",
       "      <td>Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?</td>\n",
       "      <td>Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pdxkgp</td>\n",
       "      <td>'We can build a real time machine'</td>\n",
       "      <td>I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right? So I'm guessing they never figure it out.</td>\n",
       "      <td>I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>pdcrj0</td>\n",
       "      <td>Scientists at the US Department of Energy’s laser facility shattered their own record earlier this month by generating more than 10 quadrillion watts of fusion power for a fraction of a second — roughly 700 times the generating capacity of the entire US electrical grid at any given moment</td>\n",
       "      <td>So could they do this again and connect it to a bunch of batteries this time?</td>\n",
       "      <td>So could they do this again and connect it to a bunch of batteries this time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>nuf1ej</td>\n",
       "      <td>Spanking has effects on early childhood behavior similar to those of adverse childhood experiences (ACEs) such as physical or emotional abuse or neglect, parental mental illness, parental substance use, and others, a study in the Journal of Pediatrics has found</td>\n",
       "      <td>As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?</td>\n",
       "      <td>As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ikuz9f</td>\n",
       "      <td>Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)</td>\n",
       "      <td>Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?</td>\n",
       "      <td>Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ikuz9f</td>\n",
       "      <td>Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)</td>\n",
       "      <td>Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?</td>\n",
       "      <td>Isn't that why everyone should be wearing a mask?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>p92ahi</td>\n",
       "      <td>People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon</td>\n",
       "      <td>This is really scary in itself - but what happens to those who get sick with COVID again? Further deficits?</td>\n",
       "      <td>This is really scary in itself - but what happens to those who get sick with COVID again?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>oxw9y7</td>\n",
       "      <td>Scientists were surprised when mice they were treating for diabetes dropped half their weight and developed extra-shiny coats. The cytokine they had administered led to fast fat-loss via an oily substance we secrete through skin - and could point toward future treatments for obesity and skin issues.</td>\n",
       "      <td>Let me guess - this will get developed and marketed by a company named Adipose Industries?</td>\n",
       "      <td>Let me guess - this will get developed and marketed by a company named Adipose Industries?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>mn2fqq</td>\n",
       "      <td>First evidence that dogs can mentally represent jealousy: Some researchers have suggested that jealousy is linked to self-awareness and theory of mind, leading to claims that it is unique to humans. A new study found evidence for three signatures of jealous behavior in dogs.</td>\n",
       "      <td>Who hasn't lavished attention on a stuffed animal to get a dog jealous?</td>\n",
       "      <td>Who hasn't lavished attention on a stuffed animal to get a dog jealous?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  \\\n",
       "7      pf6roe   \n",
       "16     pf5phr   \n",
       "49     pdxkgp   \n",
       "57     pdcrj0   \n",
       "58     nuf1ej   \n",
       "59     ikuz9f   \n",
       "60     ikuz9f   \n",
       "171    p92ahi   \n",
       "172    oxw9y7   \n",
       "173    mn2fqq   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            title  \\\n",
       "7                                                                                                                                                                                                                                                                     The Pandemic Caused a Baby Bust, Not a Boom   \n",
       "16                                                                                                                                                                                          Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "49                                                                                                                                                                                                                                                                             'We can build a real time machine'   \n",
       "57              Scientists at the US Department of Energy’s laser facility shattered their own record earlier this month by generating more than 10 quadrillion watts of fusion power for a fraction of a second — roughly 700 times the generating capacity of the entire US electrical grid at any given moment   \n",
       "58                                          Spanking has effects on early childhood behavior similar to those of adverse childhood experiences (ACEs) such as physical or emotional abuse or neglect, parental mental illness, parental substance use, and others, a study in the Journal of Pediatrics has found   \n",
       "59                                                                                                                        Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)   \n",
       "60                                                                                                                        Face shields and masks with exhalation valves are not effective at preventing COVID-19 transmission, finds a new droplet dispersal study. (Physics of Fluids journal, 1 September 2020)   \n",
       "171                                     People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon   \n",
       "172  Scientists were surprised when mice they were treating for diabetes dropped half their weight and developed extra-shiny coats. The cytokine they had administered led to fast fat-loss via an oily substance we secrete through skin - and could point toward future treatments for obesity and skin issues.   \n",
       "173                           First evidence that dogs can mentally represent jealousy: Some researchers have suggested that jealousy is linked to self-awareness and theory of mind, leading to claims that it is unique to humans. A new study found evidence for three signatures of jealous behavior in dogs.   \n",
       "\n",
       "                                                                                                                                                                                                                                  body  \\\n",
       "7                                                                                                                                 Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?   \n",
       "16                      I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.   \n",
       "49   I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right? So I'm guessing they never figure it out.   \n",
       "57                                                                                                                                                       So could they do this again and connect it to a bunch of batteries this time?   \n",
       "58                                            As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?   \n",
       "59                                 Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?   \n",
       "60                                 Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you? Isn't that why everyone should be wearing a mask?   \n",
       "171                                                                                                                        This is really scary in itself - but what happens to those who get sick with COVID again? Further deficits?   \n",
       "172                                                                                                                                         Let me guess - this will get developed and marketed by a company named Adipose Industries?   \n",
       "173                                                                                                                                                            Who hasn't lavished attention on a stuffed animal to get a dog jealous?   \n",
       "\n",
       "                                                                                                                                                                               reply_question  \n",
       "7                                                                                        Who'd have thought that the general unease of a deadly pandemic wouldn't get folks hot in the pants?  \n",
       "16                                                                        I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?  \n",
       "49    I'm no expert but I mean if someone ever did figure it out at some point in the very very far future we'd most likely have some sort of proof of it happening now or in the past right?  \n",
       "57                                                                                                              So could they do this again and connect it to a bunch of batteries this time?  \n",
       "58   As a soon to be new parent (who was spanked a grand total of once as a child) can you offer suggestions for alternative forms of discipline or reading material for what actually works?  \n",
       "59                                          Correct me if I'm wrong but isn't the point of wearing a mask to protect you from inhaling droplets rather than to protect the people around you?  \n",
       "60                                                                                                                                          Isn't that why everyone should be wearing a mask?  \n",
       "171                                                                                                 This is really scary in itself - but what happens to those who get sick with COVID again?  \n",
       "172                                                                                                Let me guess - this will get developed and marketed by a company named Adipose Industries?  \n",
       "173                                                                                                                   Who hasn't lavished attention on a stuffed animal to get a dog jealous?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_gender; val=M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pfb3gq</td>\n",
       "      <td>Food chemists have just invented a better chocolate using chemistry and cocoa butter to perfect the tempering process. This technique could help make this treat more sustainable by saving energy and improving its carbon footprint.</td>\n",
       "      <td>Where can one get their hands on phospholipid powder, though? Anyway, I'm only interested if it raises the melting temperature of chocolate.</td>\n",
       "      <td>Where can one get their hands on phospholipid powder, though?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p4uuhq</td>\n",
       "      <td>New findings show Bennu—one of the most hazardous known asteroids—has a 1 in 1,750 chance of impacting Earth through 2300, higher than previously thought. It could create a crater between 10 to 20 times its size and cause an area of devastation that could reach 100 times the size of the crater.</td>\n",
       "      <td>We can predict probabilities of collision that far ahead? So we don't have to worry about any Deep Impact scenario happening  in our near future, I guess.</td>\n",
       "      <td>We can predict probabilities of collision that far ahead?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p1t4an</td>\n",
       "      <td>When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.</td>\n",
       "      <td>I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.</td>\n",
       "      <td>Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p1t4an</td>\n",
       "      <td>When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.</td>\n",
       "      <td>I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.</td>\n",
       "      <td>If so, then the same is not true for domestic cats?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orstcv</td>\n",
       "      <td>The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.</td>\n",
       "      <td>This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "      <td>Why would there be any reason to expect that to be so?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>orstcv</td>\n",
       "      <td>The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.</td>\n",
       "      <td>This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "      <td>Why would we assume that produce used in deli products be the absolute freshest?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>orstcv</td>\n",
       "      <td>The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.</td>\n",
       "      <td>This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "      <td>Why would it be less expensive at a grocery store even if in bulk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>What's novel in this study? Is there a specific finding or method that the researchers are introducing? I imagine it's something to do with tightly controlling variables since it was performed in-clinic.</td>\n",
       "      <td>Is there a specific finding or method that the researchers are introducing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>oy6x6a</td>\n",
       "      <td>Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp;amp; violence.</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  \\\n",
       "0     pfb3gq   \n",
       "1     p4uuhq   \n",
       "2     p1t4an   \n",
       "3     p1t4an   \n",
       "4     orstcv   \n",
       "5     orstcv   \n",
       "6     orstcv   \n",
       "19    peudnq   \n",
       "20    peudnq   \n",
       "21    oy6x6a   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           title  \\\n",
       "0                                                                         Food chemists have just invented a better chocolate using chemistry and cocoa butter to perfect the tempering process. This technique could help make this treat more sustainable by saving energy and improving its carbon footprint.   \n",
       "1        New findings show Bennu—one of the most hazardous known asteroids—has a 1 in 1,750 chance of impacting Earth through 2300, higher than previously thought. It could create a crater between 10 to 20 times its size and cause an area of devastation that could reach 100 times the size of the crater.   \n",
       "2   When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.   \n",
       "3   When given the choice between a free meal and performing a task for a meal, cats would prefer the meal that doesn’t require much effort. While that might not come as a surprise to cat lovers, it does to cat behaviorists. Most animals prefer to work for their food—a behavior called contrafreeloading.   \n",
       "4                                                                                                               The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.   \n",
       "5                                                                                                               The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.   \n",
       "6                                                                                                               The average apple in the supermarket is over a year old. In a warehouse setting, they often sit at least 9 to 12 month, and one investigation showed that, on average, apples are 14 months old.   \n",
       "19                                                                                                                                                                        Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "20                                                                                                                                                                        Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "21                                               Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp; violence.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         body  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Where can one get their hands on phospholipid powder, though? Anyway, I'm only interested if it raises the melting temperature of chocolate.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We can predict probabilities of collision that far ahead? So we don't have to worry about any Deep Impact scenario happening  in our near future, I guess.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                  I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                  I didn't know that many animals were against freeloading. Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them? If so, then the same is not true for domestic cats? As if they still like to hunt but just don't equate it directly to food.   \n",
       "4   This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?   \n",
       "5   This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?   \n",
       "6   This is alarmist stuff. Nothing in this article is beyond common sense. Of course produce in the supermarket isn't straight off the farm. Of course produce isn't super clean. Why would there be any reason to expect that to be so?  Greens do not keep well. Why would we assume that produce used in deli products be the absolute freshest? Product placement happens in all kind of retail, not just grocery stores. Supermarkets don't \"jack up\" prices. Things that may go less expensive locally have to travel much farther and change more hands going to the market first. Why would it be less expensive at a grocery store even if in bulk?   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                What's novel in this study? Is there a specific finding or method that the researchers are introducing? I imagine it's something to do with tightly controlling variables since it was performed in-clinic.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                               Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ok. Who, out there, wants more men in the world?   \n",
       "\n",
       "                                                                                                                                 reply_question  \n",
       "0                                                                                 Where can one get their hands on phospholipid powder, though?  \n",
       "1                                                                                     We can predict probabilities of collision that far ahead?  \n",
       "2                           Perhaps there the food reward was always better if the food was hard to get and it just became hardwired into them?  \n",
       "3                                                                                           If so, then the same is not true for domestic cats?  \n",
       "4                                                                                        Why would there be any reason to expect that to be so?  \n",
       "5                                                              Why would we assume that produce used in deli products be the absolute freshest?  \n",
       "6                                                                            Why would it be less expensive at a grocery store even if in bulk?  \n",
       "19                                                                  Is there a specific finding or method that the researchers are introducing?  \n",
       "20  Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?  \n",
       "21                                                                                             Ok. Who, out there, wants more men in the world?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_age; val=30+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>pcdoho</td>\n",
       "      <td>The city of Liverpool has boycotted the British tabloid, the Sun, since 1989 (due to the tabloid's coverage of the Hillsborough disaster). As a consequence, the city of Liverpool has held more favorable views towards the European Union. This suggests that the Sun played a key role in Brexit.</td>\n",
       "      <td>By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?</td>\n",
       "      <td>By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>p9yba5</td>\n",
       "      <td>Feeling like leisure is wasteful and unproductive may lead to less happiness and higher levels of stress and depression, new research suggests (Four studies, n = 1310)</td>\n",
       "      <td>Maybe more stress is required. What business does anyone have enjoying themselves while people suffer?</td>\n",
       "      <td>What business does anyone have enjoying themselves while people suffer?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>oz6erc</td>\n",
       "      <td>A study suggests that people adjusted their music listening habits as a way of coping with the COVID-19 crisis. Survey respondents reported that they increased their music listening during the initial lockdown and that they used music to help them cope with emotional challenges during the pandemic.</td>\n",
       "      <td>What does it mean if I only listen to music if someone else has it on?</td>\n",
       "      <td>What does it mean if I only listen to music if someone else has it on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>p9h593</td>\n",
       "      <td>Exercise blunts cravings for alcohol among young, problematic drinkers</td>\n",
       "      <td>Did someone already ask what is an exercise blunt?</td>\n",
       "      <td>Did someone already ask what is an exercise blunt?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>p92ahi</td>\n",
       "      <td>People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon</td>\n",
       "      <td>With SARS (and just a reminder that covid is a SARS-like virus), a year after they recovered, 20-30% said they had worse health than before the infection.   4 years later, 87% of them had seen large declines in health since then, with pretty much all of the 87% reporting *neurological issues* developing.   There is really no doubt that SARS-like viruses have impacts in the brain, mostly through the vascular system. We know that the virus is able to attach to ACE2 receptors in the vascular system, which allows it to cross the blood brain barrier. The question is, does it stay there at a low level and slowly damage the brain?</td>\n",
       "      <td>The question is, does it stay there at a low level and slowly damage the brain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>lgsqc0</td>\n",
       "      <td>Singapore, with almost 200,00 migrant workers exposed to COVID-19 and more than 111,000 confirmed infections, has had only 20 ICU patients and 1 death, because of highly effective mass testing, contact tracing and isolation, finds a new study in JAMA.</td>\n",
       "      <td>Yeah, something tells me they likely had issues with the false positivity rating of their tests.   111,000 confirmed cases and 1 single death? I considerably doubt this.</td>\n",
       "      <td>111,000 confirmed cases and 1 single death?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>p6hr9o</td>\n",
       "      <td>Histamine could be a key player in depression, according to study in mice - Findings from research at Imperial College London &amp;amp; Uni South Carolina add to mounting evidence that inflammation and the accompanying release of histamine affects a key molecule responsible for mood in the brain – serotonin</td>\n",
       "      <td>So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?</td>\n",
       "      <td>So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>k6lc30</td>\n",
       "      <td>Jupiter and Saturn will come within 0.1 degrees of each other, forming the first visible \"double planet\" in 800 years</td>\n",
       "      <td>On the west coast of the US here, where should I look to see this? Or what website should I use to find it.</td>\n",
       "      <td>On the west coast of the US here, where should I look to see this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>p5tmfv</td>\n",
       "      <td>Swiss researchers calculate pi to new record of 62.8tn figures. Supercomputer calculation took 108 days and nine hours – 3.5 times as fast as previous record</td>\n",
       "      <td>The picture in the article shows Pi to 90 digits past the decimal. Is there any practical use for 62.8tn? The article does mention RNA analysis.</td>\n",
       "      <td>Is there any practical use for 62.8tn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>p5jeib</td>\n",
       "      <td>Pfizer pharmacokinetics and toxicity – a study on mRNA vaccine clotting in test subjects</td>\n",
       "      <td>This reads like a hit piece.  I'm used to scientific papers that are neutral in tone and state the facts they've discovered, and this one is laced with emotion-laden language and conclusions.  Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?</td>\n",
       "      <td>Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  \\\n",
       "81     pcdoho   \n",
       "129    p9yba5   \n",
       "130    oz6erc   \n",
       "163    p9h593   \n",
       "177    p92ahi   \n",
       "178    lgsqc0   \n",
       "223    p6hr9o   \n",
       "224    k6lc30   \n",
       "239    p5tmfv   \n",
       "245    p5jeib   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                title  \\\n",
       "81               The city of Liverpool has boycotted the British tabloid, the Sun, since 1989 (due to the tabloid's coverage of the Hillsborough disaster). As a consequence, the city of Liverpool has held more favorable views towards the European Union. This suggests that the Sun played a key role in Brexit.   \n",
       "129                                                                                                                                           Feeling like leisure is wasteful and unproductive may lead to less happiness and higher levels of stress and depression, new research suggests (Four studies, n = 1310)   \n",
       "130       A study suggests that people adjusted their music listening habits as a way of coping with the COVID-19 crisis. Survey respondents reported that they increased their music listening during the initial lockdown and that they used music to help them cope with emotional challenges during the pandemic.   \n",
       "163                                                                                                                                                                                                                                            Exercise blunts cravings for alcohol among young, problematic drinkers   \n",
       "177                                         People who have recovered from COVID-19, including those no longer reporting symptoms, exhibit significant cognitive deficits versus controls according to a survey of 80,000+ participants conducted in conjunction with the scientific documentary series, BBC2 Horizon   \n",
       "178                                                       Singapore, with almost 200,00 migrant workers exposed to COVID-19 and more than 111,000 confirmed infections, has had only 20 ICU patients and 1 death, because of highly effective mass testing, contact tracing and isolation, finds a new study in JAMA.   \n",
       "223  Histamine could be a key player in depression, according to study in mice - Findings from research at Imperial College London &amp; Uni South Carolina add to mounting evidence that inflammation and the accompanying release of histamine affects a key molecule responsible for mood in the brain – serotonin   \n",
       "224                                                                                                                                                                                             Jupiter and Saturn will come within 0.1 degrees of each other, forming the first visible \"double planet\" in 800 years   \n",
       "239                                                                                                                                                     Swiss researchers calculate pi to new record of 62.8tn figures. Supercomputer calculation took 108 days and nine hours – 3.5 times as fast as previous record   \n",
       "245                                                                                                                                                                                                                          Pfizer pharmacokinetics and toxicity – a study on mRNA vaccine clotting in test subjects   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body  \\\n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?   \n",
       "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Maybe more stress is required. What business does anyone have enjoying themselves while people suffer?   \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   What does it mean if I only listen to music if someone else has it on?   \n",
       "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Did someone already ask what is an exercise blunt?   \n",
       "177  With SARS (and just a reminder that covid is a SARS-like virus), a year after they recovered, 20-30% said they had worse health than before the infection.   4 years later, 87% of them had seen large declines in health since then, with pretty much all of the 87% reporting *neurological issues* developing.   There is really no doubt that SARS-like viruses have impacts in the brain, mostly through the vascular system. We know that the virus is able to attach to ACE2 receptors in the vascular system, which allows it to cross the blood brain barrier. The question is, does it stay there at a low level and slowly damage the brain?   \n",
       "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Yeah, something tells me they likely had issues with the false positivity rating of their tests.   111,000 confirmed cases and 1 single death? I considerably doubt this.   \n",
       "223                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?   \n",
       "224                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              On the west coast of the US here, where should I look to see this? Or what website should I use to find it.   \n",
       "239                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The picture in the article shows Pi to 90 digits past the decimal. Is there any practical use for 62.8tn? The article does mention RNA analysis.   \n",
       "245                                                                                                                                                                                                                                                                                                                                                      This reads like a hit piece.  I'm used to scientific papers that are neutral in tone and state the facts they've discovered, and this one is laced with emotion-laden language and conclusions.  Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?   \n",
       "\n",
       "                                                                                                               reply_question  \n",
       "81   By boycott, do you mean that the citizens have been refusing to buy the paper or that selling it was prohibited locally?  \n",
       "129                                                   What business does anyone have enjoying themselves while people suffer?  \n",
       "130                                                    What does it mean if I only listen to music if someone else has it on?  \n",
       "163                                                                        Did someone already ask what is an exercise blunt?  \n",
       "177                                           The question is, does it stay there at a low level and slowly damage the brain?  \n",
       "178                                                                               111,000 confirmed cases and 1 single death?  \n",
       "223                               So, im allergic to cats, and I have 2 cats, are you saying it contributes to my depression?  \n",
       "224                                                        On the west coast of the US here, where should I look to see this?  \n",
       "239                                                                                    Is there any practical use for 62.8tn?  \n",
       "245                        Is there somebody here with expertise in this area who can evaluate this paper for the rest of us?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_age; val=<30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.</td>\n",
       "      <td>I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>pd3eo2</td>\n",
       "      <td>We continue to evolve: more and more human beings are born with an extra artery in the arm</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>nayk3b</td>\n",
       "      <td>For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.</td>\n",
       "      <td>Hey kids. I just thought of a dumb joke you can try on your dads:  &amp;gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &amp;gt;  &amp;gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &amp;gt;  &amp;gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?</td>\n",
       "      <td>Oh god, what am I even doing with my life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>p7cnwt</td>\n",
       "      <td>Substituting only 10% of daily caloric intake of beef and processed meats for a diverse mix of whole grains, fruits, vegetables, nuts, legumes and select seafood could reduce, on average, the dietary carbon footprint of a U.S. consumer by one-third and add 48 healthy minutes of life per day.</td>\n",
       "      <td>Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides? Don't talk to me until you're eating nothing but farmed bugs for protein and veggies grown indoors in hydroponic skyscrapers</td>\n",
       "      <td>Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>p31nv2</td>\n",
       "      <td>Ethnic Group in the Philippines Have Highest Level of Denisovan DNA: The Ayta Magbukon, who live in Central Luzon, were found to have higher levels of DNA from the extinct hominid Denisovan subspecies than any previously recorded.</td>\n",
       "      <td>I wonder if they have Genghis  Khan’s DNA as well?</td>\n",
       "      <td>I wonder if they have Genghis  Khan’s DNA as well?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>o0f1ag</td>\n",
       "      <td>For reasons unknown, Earth’s solid-iron inner core is growing faster on one side than the other, and it has been ever since it started to freeze out from molten iron more than half a billion years ago, according to a new study by seismologists at the University of California, Berkeley.</td>\n",
       "      <td>Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?</td>\n",
       "      <td>Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>jbjmyy</td>\n",
       "      <td>Children whose outdoor play areas were transformed from gravel yards to mini-forests showed improved immune systems within a month, research has shown.</td>\n",
       "      <td>Would this have something to do with why some folks are at higher risk of death from certain medical concerns? Less access to health care and healthy environments?</td>\n",
       "      <td>Would this have something to do with why some folks are at higher risk of death from certain medical concerns?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>p1mr5y</td>\n",
       "      <td>Climate change ‘double whammy’ could kill off fish species. A new study of 150 million years of fish evolution provides first evidence to support scientific theory that commonly-eaten fish species will become smaller as waters warm under climate change.</td>\n",
       "      <td>I want to be clear before I ask this that I 100% believe in climate change and we 1000% need to do something about it yesterday. This is a question relating to my understanding and interest in fish biology.   A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?</td>\n",
       "      <td>A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>oykg1y</td>\n",
       "      <td>Climate crisis: Scientists spot warning signs of Gulf Stream collapse</td>\n",
       "      <td>Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno</td>\n",
       "      <td>Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>oykg1y</td>\n",
       "      <td>Climate crisis: Scientists spot warning signs of Gulf Stream collapse</td>\n",
       "      <td>Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno</td>\n",
       "      <td>will Maine be warmer some day because of this?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  \\\n",
       "16     pf5phr   \n",
       "62     pd3eo2   \n",
       "63     nayk3b   \n",
       "213    p7cnwt   \n",
       "322    p31nv2   \n",
       "323    o0f1ag   \n",
       "324    jbjmyy   \n",
       "363    p1mr5y   \n",
       "413    oykg1y   \n",
       "414    oykg1y   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    title  \\\n",
       "16                                                                                                                                                                                  Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "62                                                                                                                                                                                                             We continue to evolve: more and more human beings are born with an extra artery in the arm   \n",
       "63                                                                                 For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.   \n",
       "213  Substituting only 10% of daily caloric intake of beef and processed meats for a diverse mix of whole grains, fruits, vegetables, nuts, legumes and select seafood could reduce, on average, the dietary carbon footprint of a U.S. consumer by one-third and add 48 healthy minutes of life per day.   \n",
       "322                                                                Ethnic Group in the Philippines Have Highest Level of Denisovan DNA: The Ayta Magbukon, who live in Central Luzon, were found to have higher levels of DNA from the extinct hominid Denisovan subspecies than any previously recorded.   \n",
       "323        For reasons unknown, Earth’s solid-iron inner core is growing faster on one side than the other, and it has been ever since it started to freeze out from molten iron more than half a billion years ago, according to a new study by seismologists at the University of California, Berkeley.   \n",
       "324                                                                                                                                               Children whose outdoor play areas were transformed from gravel yards to mini-forests showed improved immune systems within a month, research has shown.   \n",
       "363                                         Climate change ‘double whammy’ could kill off fish species. A new study of 150 million years of fish evolution provides first evidence to support scientific theory that commonly-eaten fish species will become smaller as waters warm under climate change.   \n",
       "413                                                                                                                                                                                                                                 Climate crisis: Scientists spot warning signs of Gulf Stream collapse   \n",
       "414                                                                                                                                                                                                                                 Climate crisis: Scientists spot warning signs of Gulf Stream collapse   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
       "16                                                                                                                                                                                                          I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point? That said, I'm glad the opportunity is opening up - you never know if you don't/can't try.   \n",
       "62                                                                                                                                                                                                                                                                                            Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.   \n",
       "63   Hey kids. I just thought of a dumb joke you can try on your dads:  &gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &gt;  &gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &gt;  &gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?   \n",
       "213                                                                                                                                                                                             Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides? Don't talk to me until you're eating nothing but farmed bugs for protein and veggies grown indoors in hydroponic skyscrapers   \n",
       "322                                                                                                                                                                                                                                                                                                                                                                     I wonder if they have Genghis  Khan’s DNA as well?   \n",
       "323                                                                                                                                                                                                                                                                                                        Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?   \n",
       "324                                                                                                                                                                                                                                                    Would this have something to do with why some folks are at higher risk of death from certain medical concerns? Less access to health care and healthy environments?   \n",
       "363                                                                         I want to be clear before I ask this that I 100% believe in climate change and we 1000% need to do something about it yesterday. This is a question relating to my understanding and interest in fish biology.   A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?   \n",
       "413                                                                                                                     Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno   \n",
       "414                                                                                                                     Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  I.e. will Maine be warmer some day because of this? Or maybe Maine currently benefits from the gulf stream? I dunno   \n",
       "\n",
       "                                                                                                                                                                    reply_question  \n",
       "16                                                             I could be wrong, but last I read about this, I thought we weren't even capable of eclipsing 14 days at this point?  \n",
       "62                                                                                                                         Is there an easy way to tell if I have a second artery?  \n",
       "63                                                                                                                                      Oh god, what am I even doing with my life?  \n",
       "213                                                                                  Yeah and what about the lack of bio diversity and harm to eco systems have due to pesticides?  \n",
       "322                                                                                                                             I wonder if they have Genghis  Khan’s DNA as well?  \n",
       "323                                                                Just curious since the earth is a spinning magnet isn’t that what Tesla was saying about free electrical power?  \n",
       "324                                                                 Would this have something to do with why some folks are at higher risk of death from certain medical concerns?  \n",
       "363                                                  A lot of fish will breed and reproduce faster in warmer water, I wonder if for any fish this might mitigate this risk at all?  \n",
       "413  Serious question, given the gulf stream moves warmer water from the US east coast towards europe, does that mean the US east coast gets warmer if the gulf stream shuts down?  \n",
       "414                                                                                                                                 will Maine be warmer some day because of this?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_location; val=US\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab? A collaboration with abortion clinics and hospitals around the country sounds like a win to me.</td>\n",
       "      <td>why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?</td>\n",
       "      <td>14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.</td>\n",
       "      <td>Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>oy6x6a</td>\n",
       "      <td>Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp;amp; violence.</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "      <td>Ok. Who, out there, wants more men in the world?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>pdpt3h</td>\n",
       "      <td>This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"</td>\n",
       "      <td>Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?</td>\n",
       "      <td>Can this help explain that phenomenon where people are under but still conscious??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>pdpt3h</td>\n",
       "      <td>This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"</td>\n",
       "      <td>Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?</td>\n",
       "      <td>Does anyone know what I'm talking about?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>pd3eo2</td>\n",
       "      <td>We continue to evolve: more and more human beings are born with an extra artery in the arm</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.</td>\n",
       "      <td>Is there an easy way to tell if I have a second artery?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>nayk3b</td>\n",
       "      <td>For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.</td>\n",
       "      <td>Hey kids. I just thought of a dumb joke you can try on your dads:  &amp;gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &amp;gt;  &amp;gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &amp;gt;  &amp;gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?</td>\n",
       "      <td>Oh god, what am I even doing with my life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>pckqep</td>\n",
       "      <td>We Think Anger Is A Sign Of Guilt — But It May Actually Be A Better Sign Of Innocence</td>\n",
       "      <td>This makes total sense to me as someone who gets absolutely livid when falsely accused of something.  In fact, I'm not sure I accept the premise of this title.  Do we generally think anger is a sign of guilt?  I usually don't.</td>\n",
       "      <td>Do we generally think anger is a sign of guilt?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ooictx</td>\n",
       "      <td>Alarming climate change: Earth heads for its tipping point as it could reach +1.5 °C over the next 5 years, WMO finds in the latest study</td>\n",
       "      <td>Why do you think these asshole billionaires are so anxious to get off this planet?</td>\n",
       "      <td>Why do you think these asshole billionaires are so anxious to get off this planet?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  \\\n",
       "8     pf5phr   \n",
       "17    pf5phr   \n",
       "20    peudnq   \n",
       "21    oy6x6a   \n",
       "55    pdpt3h   \n",
       "56    pdpt3h   \n",
       "62    pd3eo2   \n",
       "63    nayk3b   \n",
       "74    pckqep   \n",
       "75    ooictx   \n",
       "\n",
       "                                                                                                                                                                                                                                                              title  \\\n",
       "8                                                                                                                                             Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "17                                                                                                                                            Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "20                                                                                                                           Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "21  Researchers warn trends in sex selection favouring male babies will result in a preponderance of men in over 1/3 of world’s population, and a surplus of men in countries will cause a “marriage squeeze,” and may increase antisocial behavior &amp; violence.   \n",
       "55                                                                                              This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"   \n",
       "56                                                                                              This Is Your Brain Under Anesthesia - \"For the first time, researchers were able to observe, in extra-fine detail, how neurons behave as consciousness shuts down.\"   \n",
       "62                                                                                                                                                                       We continue to evolve: more and more human beings are born with an extra artery in the arm   \n",
       "63                                           For the first time, scientists tracked large shark movements during hurricanes and found that tiger sharks may find the turmoil opportunistic for feeding. While some sharks flee, Tiger sharks brave the stormy seas.   \n",
       "74                                                                                                                                                                            We Think Anger Is A Sign Of Guilt — But It May Actually Be A Better Sign Of Innocence   \n",
       "75                                                                                                                        Alarming climate change: Earth heads for its tipping point as it could reach +1.5 °C over the next 5 years, WMO finds in the latest study   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                     body  \\\n",
       "8                                                                                                                                                                                                                   why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab? A collaboration with abortion clinics and hospitals around the country sounds like a win to me.   \n",
       "17                                                                                                                                                                                                                                                                                                                                    14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?   \n",
       "20                                                                                                                                                           Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body? I appreciate this study but I think we've known these things are bad for individual health for a very long time.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                       Ok. Who, out there, wants more men in the world?   \n",
       "55                                                                                                                                                                                            Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?   \n",
       "56                                                                                                                                                                                            Can this help explain that phenomenon where people are under but still conscious?? Like they can feel , smell ,hear everything still just can't move. Kinda like sleep paralysis?? Does anyone know what I'm talking about?   \n",
       "62                                                                                                                                                                                                                                                                                           Is there an easy way to tell if I have a second artery? It's a Friday night and I just spent two minutes groping my own arm.   \n",
       "63  Hey kids. I just thought of a dumb joke you can try on your dads:  &gt; Jimothy: \"What does Tony the Tiger Shark eat for breakfast?\" &gt;  &gt; Dad: \"Uhh. Frosted flakes in the ocean or something?\" &gt;  &gt; Jimothy: \"No, dad, they eat fish. Sheesh.\"  I know jokes aren't typically allowed here, but I argue that this is relevant, educational and also grrrreat! Oh god, what am I even doing with my life?   \n",
       "74                                                                                                                                                                                     This makes total sense to me as someone who gets absolutely livid when falsely accused of something.  In fact, I'm not sure I accept the premise of this title.  Do we generally think anger is a sign of guilt?  I usually don't.   \n",
       "75                                                                                                                                                                                                                                                                                                                                     Why do you think these asshole billionaires are so anxious to get off this planet?   \n",
       "\n",
       "                                                                                                                                 reply_question  \n",
       "8                                         why can’t scientists just collect unwanted fetuses instead of creating super expensive ones in a lab?  \n",
       "17                                                          14 days/2 weeks old embryo would be considered 4 weeks in a normal pregnancy right?  \n",
       "20  Is there anyone (besides industrial food corporations) seriously arguing that  sucrose or high fructose corn syrup are good for one's body?  \n",
       "21                                                                                             Ok. Who, out there, wants more men in the world?  \n",
       "55                                                           Can this help explain that phenomenon where people are under but still conscious??  \n",
       "56                                                                                                     Does anyone know what I'm talking about?  \n",
       "62                                                                                      Is there an easy way to tell if I have a second artery?  \n",
       "63                                                                                                   Oh god, what am I even doing with my life?  \n",
       "74                                                                                              Do we generally think anger is a sign of guilt?  \n",
       "75                                                           Why do you think these asshole billionaires are so anxious to get off this planet?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo=norm_location; val=non_US\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>reply_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pf5phr</td>\n",
       "      <td>Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.</td>\n",
       "      <td>So how long until lab embryos are legal for longer than the term for abortion? Toying around with human (to be) lives like this makes \"researchers\" more and more like doctor Frankenstein. So not be surprised that at a certain moment pharmaceuticals will be categorically rejected by certain groups, and we cannot predict which groups that will be.</td>\n",
       "      <td>So how long until lab embryos are legal for longer than the term for abortion?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>p7m46h</td>\n",
       "      <td>COVID-19 is a vascular disease not a respiratory one, says study - The study, published in the journal Circulation Research, shows with precision how virus damages the cells of the vascular system.</td>\n",
       "      <td>Yeah, I told that a year ago. I'm not even a physician, nor a biologist. It's just common sense.  If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?</td>\n",
       "      <td>If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>p6nwn9</td>\n",
       "      <td>Newly published research indicates that people who more frequently signal their victimhood (whether real, exaggerated, or false) are more likely to lie and cheat for material gain and denigrate others as a means to get ahead.</td>\n",
       "      <td>&amp;gt;Newly published ~~research~~ **open door** indicates that people who more frequently ~~signal their victimhood (whether real, exaggerated, or false)~~ **manipulate** are more likely to ~~lie and cheat~~ **manipulate** for material gain and ~~denigrate others as a means~~ **manipulate** to get ahead.  There, I fixed the title. Can anyone tell me which subreddit shows *actual* science?</td>\n",
       "      <td>Can anyone tell me which subreddit shows *actual* science?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n97dh7</td>\n",
       "      <td>67% of participants who received three MDMA-assisted therapy sessions no longer qualified for a PTSD diagnosis, results published in Nature Medicine</td>\n",
       "      <td>When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?</td>\n",
       "      <td>When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n7ebfz</td>\n",
       "      <td>Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.</td>\n",
       "      <td>&amp;gt;Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.  Isn't that the entire concept behind chemo therapy?</td>\n",
       "      <td>Isn't that the entire concept behind chemo therapy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mj5mp9</td>\n",
       "      <td>Microdosing psychedelic drugs is associated with increases in conscientiousness and reductions in neuroticism, finds a new Australian study, which may explain the positive effects on performance and psychological well-being respectively.</td>\n",
       "      <td>Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?</td>\n",
       "      <td>Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hhz1ps</td>\n",
       "      <td>New research has suggested that the MMR (measles, mumps, and rubella) vaccine can stimulate immune cells in a way that primes white blood cells to react more effectively when they encounter other unrelated infections, including SARS-coV-2</td>\n",
       "      <td>So Covid-19 is weeding out the anti-vaxxers among us?</td>\n",
       "      <td>So Covid-19 is weeding out the anti-vaxxers among us?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pf1g2g</td>\n",
       "      <td>Birth rates in many high-income countries declined in the months following the first Covid wave, possibly because of economic uncertainty, with particularly strong declines in southern Europe: Italy (−9.1%), Spain (−8.4%), and Portugal (−6.6%).</td>\n",
       "      <td>I was trying for a third, pandemic started and we decided 2 was enough. Who wants to go through pregnancy and a newborn during a pandemic? Not this girl.</td>\n",
       "      <td>Who wants to go through pregnancy and a newborn during a pandemic?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>So is this effect cumulative over life or is it okay to eat a low sucrose diet?</td>\n",
       "      <td>So is this effect cumulative over life or is it okay to eat a low sucrose diet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>peudnq</td>\n",
       "      <td>Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity</td>\n",
       "      <td>Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?</td>\n",
       "      <td>Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  \\\n",
       "9     pf5phr   \n",
       "10    p7m46h   \n",
       "11    p6nwn9   \n",
       "12    n97dh7   \n",
       "13    n7ebfz   \n",
       "14    mj5mp9   \n",
       "15    hhz1ps   \n",
       "18    pf1g2g   \n",
       "22    peudnq   \n",
       "23    peudnq   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          title  \\\n",
       "9                                                                                                                                                                                         Researchers are now permitted to grow human embryos in the lab for longer than 14 days. Here’s what they could learn.   \n",
       "10                                                                                                        COVID-19 is a vascular disease not a respiratory one, says study - The study, published in the journal Circulation Research, shows with precision how virus damages the cells of the vascular system.   \n",
       "11                                                                            Newly published research indicates that people who more frequently signal their victimhood (whether real, exaggerated, or false) are more likely to lie and cheat for material gain and denigrate others as a means to get ahead.   \n",
       "12                                                                                                                                                         67% of participants who received three MDMA-assisted therapy sessions no longer qualified for a PTSD diagnosis, results published in Nature Medicine   \n",
       "13  Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.   \n",
       "14                                                                Microdosing psychedelic drugs is associated with increases in conscientiousness and reductions in neuroticism, finds a new Australian study, which may explain the positive effects on performance and psychological well-being respectively.   \n",
       "15                                                               New research has suggested that the MMR (measles, mumps, and rubella) vaccine can stimulate immune cells in a way that primes white blood cells to react more effectively when they encounter other unrelated infections, including SARS-coV-2   \n",
       "18                                                         Birth rates in many high-income countries declined in the months following the first Covid wave, possibly because of economic uncertainty, with particularly strong declines in southern Europe: Italy (−9.1%), Spain (−8.4%), and Portugal (−6.6%).   \n",
       "22                                                                                                                                                                       Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "23                                                                                                                                                                       Double-blind, in-clinic study shows that both sucrose and high-fructose corn syrup increase liver fat and decrease insulin sensitivity   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
       "9                                              So how long until lab embryos are legal for longer than the term for abortion? Toying around with human (to be) lives like this makes \"researchers\" more and more like doctor Frankenstein. So not be surprised that at a certain moment pharmaceuticals will be categorically rejected by certain groups, and we cannot predict which groups that will be.   \n",
       "10                                                                                                                                                                         Yeah, I told that a year ago. I'm not even a physician, nor a biologist. It's just common sense.  If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?   \n",
       "11  &gt;Newly published ~~research~~ **open door** indicates that people who more frequently ~~signal their victimhood (whether real, exaggerated, or false)~~ **manipulate** are more likely to ~~lie and cheat~~ **manipulate** for material gain and ~~denigrate others as a means~~ **manipulate** to get ahead.  There, I fixed the title. Can anyone tell me which subreddit shows *actual* science?   \n",
       "12                                                                                                                                                                                                                                                                                 When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?   \n",
       "13                                    &gt;Scientists discover how to trick cancer cells to consume toxic drugs - Research could open the doors for a Trojan horse in cancer therapy. The strategy relies on tumors' large appetite for protein nutrients that fuel malignant growth, and tricking the tumors to inadvertently take in attached drugs.  Isn't that the entire concept behind chemo therapy?   \n",
       "14                                                                                                                                                                                                                                                                                    Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?   \n",
       "15                                                                                                                                                                                                                                                                                                                                                   So Covid-19 is weeding out the anti-vaxxers among us?   \n",
       "18                                                                                                                                                                                                                                               I was trying for a third, pandemic started and we decided 2 was enough. Who wants to go through pregnancy and a newborn during a pandemic? Not this girl.   \n",
       "22                                                                                                                                                                                                                                                                                                                         So is this effect cumulative over life or is it okay to eat a low sucrose diet?   \n",
       "23                                                                                                                                                                                                                                                                                 Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?   \n",
       "\n",
       "                                                                                                                   reply_question  \n",
       "9                                                  So how long until lab embryos are legal for longer than the term for abortion?  \n",
       "10  If COVID is accompanied with thrombosis, isn't it obvious that the lung issues are in fact caused by thrombosis in the lungs?  \n",
       "11                                                                     Can anyone tell me which subreddit shows *actual* science?  \n",
       "12        When will this sub get renamed to [pharmakeia](https://www.blueletterbible.org/lang/lexicon/lexicon.cfm?Strongs=G5331)?  \n",
       "13                                                                            Isn't that the entire concept behind chemo therapy?  \n",
       "14           Why is it that a very significant amount of posts on /r/science are about drugs, specifically controlled substances?  \n",
       "15                                                                          So Covid-19 is weeding out the anti-vaxxers among us?  \n",
       "18                                                             Who wants to go through pregnancy and a newborn during a pandemic?  \n",
       "22                                                So is this effect cumulative over life or is it okay to eat a low sucrose diet?  \n",
       "23        Is this similar to the other study posted here the other day about the physical effects of HFCS on the small intestine?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for demo_var_i in demo_vars:\n",
    "    data_i = valid_submission_question_data.dropna(subset=[demo_var_i])\n",
    "    for demo_val_j, data_j in data_i.groupby(demo_var_i):\n",
    "        print(f'demo={demo_var_i}; val={demo_val_j}')\n",
    "        display(data_j.loc[:, ['parent_id', 'title', 'body', 'reply_question']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the questions are off-topic but most of them are generally relevant to the original post topic.\n",
    "\n",
    "Let's try to classify them with a basic BERT classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to file\n",
    "valid_submission_question_data.to_csv('science_submission_question_reply_author_attr_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-author classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch import tensor, Tensor\n",
    "np.random.seed(123)\n",
    "def resample_data_by_class(data, label_var):\n",
    "    class_counts = data.loc[:, label_var].value_counts()\n",
    "    max_class = class_counts.index[class_counts.argmax()]\n",
    "    min_class = class_counts.index[class_counts.argmin()]\n",
    "    max_class_data = data[data.loc[:, label_var]==max_class]\n",
    "    min_class_data = data[data.loc[:, label_var]==min_class].sample(max_class_data.shape[0], replace=True, random_state=123)\n",
    "    resample_data = pd.concat([max_class_data, min_class_data], axis=0)\n",
    "    return resample_data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key : tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = tensor([self.labels[idx]])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "def load_training_model(model_name, out_dir):\n",
    "    device_name = 'cuda'\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name).to(device_name)\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=20,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        output_dir=os.path.join(out_dir, 'results'),\n",
    "        logging_dir=os.path.join(out_dir, 'logs'),\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy='steps',\n",
    "        seed=123,\n",
    "    )\n",
    "    return model, training_args\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }\n",
    "def train_test_BERT_classifier(data, label_var, txt_var, model_name, \n",
    "                               train_pct=0.9, special_tokens=None,\n",
    "                               max_length=512,\n",
    "                               out_dir='question_author_classification/',\n",
    "                               training_args=None):\n",
    "    # resample for max-class\n",
    "    data = resample_data_by_class(data, label_var)\n",
    "    # get labels\n",
    "    N_train = int(data.shape[0] * train_pct)\n",
    "    data = data.sample(data.shape[0], replace=False, random_state=123)\n",
    "    train_data = data.iloc[:N_train, :]\n",
    "    test_data = data.iloc[N_train:, :]\n",
    "    train_txt = train_data.loc[:, txt_var].values.tolist()\n",
    "    train_labels = train_data.loc[:, label_var].values.tolist()\n",
    "    test_txt = test_data.loc[:, txt_var].values.tolist()\n",
    "    test_labels = test_data.loc[:, label_var].values.tolist()\n",
    "    # convert labels to ID\n",
    "    label_id_lookup = {v : i for i, v in enumerate(data.loc[:, label_var].unique())}\n",
    "    id_label_lookup = {v : k for k,v in label_id_lookup.items()}\n",
    "    train_labels = list(map(label_id_lookup.get, train_labels))\n",
    "    test_labels = list(map(label_id_lookup.get, test_labels))\n",
    "    # tokenize things\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    if(special_tokens is not None):\n",
    "        tokenizer.add_special_tokens(special_tokens, special_tokens=True)\n",
    "    train_data_encoded = tokenizer(train_txt, truncation=True, padding=True, max_length=max_length)\n",
    "    test_data_encoded = tokenizer(test_txt, truncation=True, padding=True, max_length=max_length)\n",
    "    # load data set\n",
    "    train_data_set = CustomDataset(train_data_encoded, train_labels)\n",
    "    test_data_set = CustomDataset(test_data_encoded, test_labels)\n",
    "    # load model\n",
    "    model, model_training_args = load_training_model(model_name, out_dir=out_dir)\n",
    "    if(training_args is not None):\n",
    "        model_training_args.__dict__.update(training_args)\n",
    "    # build trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=model_training_args,\n",
    "        train_dataset=train_data_set,\n",
    "        eval_dataset=test_data_set,\n",
    "        compute_metrics=compute_metrics,\n",
    "        \n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(out_dir)\n",
    "    trainer.evaluate()\n",
    "    # evaluate on test data\n",
    "    test_data_pred = trainer.predict(test_data_set)\n",
    "    test_data_pred_labels = test_data_pred.predictions.argmax(-1).flatten().tolist()\n",
    "    print(classification_report(test_labels, test_data_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n"
     ]
    }
   ],
   "source": [
    "## reload\n",
    "import pandas as pd\n",
    "submission_question_data = pd.read_csv('science_submission_question_reply_author_attr_data.gz', sep='\\t', compression='gzip')\n",
    "print(submission_question_data.shape[0])\n",
    "## TODO: combine post title + question w/ special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_test_all_demos(data, demo_vars, device_num=1, txt_var='reply_question', training_args=None):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(device_num)\n",
    "    out_dir = 'question_author_classification/'\n",
    "    model_name = 'distilbert-base-cased'\n",
    "    device_name = 'cuda'\n",
    "    train_pct = 0.9\n",
    "    for demo_var_i in demo_vars:\n",
    "        print(f'testing demo var = {demo_var_i}')\n",
    "        data_i = data.dropna(subset=[demo_var_i])\n",
    "        train_test_BERT_classifier(data_i, demo_var_i, txt_var, model_name,\n",
    "                                   train_pct=train_pct, out_dir=out_dir,\n",
    "                                   training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.622213</td>\n",
       "      <td>0.650155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>0.597750</td>\n",
       "      <td>0.758514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.362431</td>\n",
       "      <td>0.873065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.419830</td>\n",
       "      <td>0.891641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.416057</td>\n",
       "      <td>0.919505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       160\n",
      "           1       0.94      0.87      0.90       163\n",
      "\n",
      "    accuracy                           0.91       323\n",
      "   macro avg       0.91      0.91      0.91       323\n",
      "weighted avg       0.91      0.91      0.91       323\n",
      "\n",
      "testing demo var = norm_age\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.33      0.32        15\n",
      "           1       0.50      0.48      0.49        21\n",
      "\n",
      "    accuracy                           0.42        36\n",
      "   macro avg       0.41      0.40      0.41        36\n",
      "weighted avg       0.42      0.42      0.42        36\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/684 01:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.706201</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>0.679983</td>\n",
       "      <td>0.571782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.652966</td>\n",
       "      <td>0.646040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.647252</td>\n",
       "      <td>0.641089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.674224</td>\n",
       "      <td>0.725248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.783124</td>\n",
       "      <td>0.732673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73       201\n",
      "           1       0.73      0.78      0.75       203\n",
      "\n",
      "    accuracy                           0.74       404\n",
      "   macro avg       0.74      0.74      0.74       404\n",
      "weighted avg       0.74      0.74      0.74       404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_vars = ['norm_gender', 'norm_age', 'norm_location']\n",
    "device_num = 1\n",
    "train_test_all_demos(submission_question_data, demo_vars, device_num=device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Gender`: >90%\n",
    "- `Age`: <50%\n",
    "- `Location`: >70%\n",
    "\n",
    "Overall not terrible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing but balance the data to have same label distribution per-post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_post_question_pairs(data, group_var, group_class_vals, id_var='conversation_id', sample_type='max'):\n",
    "    # get at least one pair of questions per post\n",
    "    sample_data = []\n",
    "    group_class_1, group_class_2 = group_class_vals\n",
    "    for id_i, data_i in data.groupby(id_var):\n",
    "        class_counts = data_i.loc[:, group_var].value_counts().sort_values(inplace=False, ascending=False)\n",
    "        if(len(class_counts) == len(group_class_vals)):\n",
    "            min_class = class_counts.index[-1]\n",
    "            max_class = class_counts.index[0]\n",
    "            N_min_class = class_counts.loc[min_class]\n",
    "            N_max_class = class_counts.loc[max_class]\n",
    "#             print(f'class counts = {class_counts}')\n",
    "            # oversample min class to handle data sparsity\n",
    "            if(sample_type == 'max'):\n",
    "                min_class_data_i = data_i[data_i.loc[:, group_var]==min_class].sample(N_max_class, replace=True, random_state=123)\n",
    "                max_class_data_i = data_i[data_i.loc[:, group_var]==max_class].sample(N_max_class, replace=False)\n",
    "            elif(sample_type == 'min'):\n",
    "                min_class_data_i = data_i[data_i.loc[:, group_var]==min_class]\n",
    "                max_class_data_i = data_i[data_i.loc[:, group_var]==max_class].sample(N_min_class, replace=False, random_state=123)\n",
    "            sample_data_i = pd.concat([min_class_data_i, max_class_data_i], axis=0)\n",
    "#             print(f'id={id_i} has question pair')\n",
    "            sample_data.append(sample_data_i)\n",
    "    sample_data = pd.concat(sample_data, axis=0)\n",
    "    print(f'N={sample_data.shape[0]}/{data.shape[0]} paired data')\n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_balanced_data(data, sample_type='min', demo_vars=['norm_age', 'norm_gender', 'norm_location']):\n",
    "    sample_balanced_data = []\n",
    "    id_var = 'id_submission'\n",
    "    for demo_var_i in demo_vars:\n",
    "        data_i = data.dropna(subset=[demo_var_i])\n",
    "        demo_vals_i = data_i.loc[:, demo_var_i].unique()\n",
    "        sample_data_i = sample_post_question_pairs(data, demo_var_i, \n",
    "                                                   demo_vals_i, id_var=id_var,\n",
    "                                                   sample_type=sample_type)\n",
    "        print(f'demo={demo_var_i} has N={sample_data_i.shape[0]}')\n",
    "        sample_balanced_data.append(sample_data_i)\n",
    "    sample_balanced_data = pd.concat(sample_balanced_data, axis=0)\n",
    "    return sample_balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=356/5095 paired data\n",
      "demo=norm_gender has N=356\n",
      "N=24/5095 paired data\n",
      "demo=norm_age has N=24\n",
      "N=1048/5095 paired data\n",
      "demo=norm_location has N=1048\n"
     ]
    }
   ],
   "source": [
    "sample_balanced_data = get_sample_balanced_data(submission_question_data, sample_type='min',\n",
    "                                                demo_vars=demo_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our modeling to the larger author groups (gender, location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.454013</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.394496</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92        55\n",
      "           1       1.00      0.87      0.93        70\n",
      "\n",
      "    accuracy                           0.93       125\n",
      "   macro avg       0.93      0.94      0.93       125\n",
      "weighted avg       0.94      0.93      0.93       125\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 00:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686700</td>\n",
       "      <td>0.645402</td>\n",
       "      <td>0.640167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.824268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.445559</td>\n",
       "      <td>0.845188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.476584</td>\n",
       "      <td>0.882845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88       123\n",
      "           1       0.86      0.91      0.88       116\n",
      "\n",
      "    accuracy                           0.88       239\n",
      "   macro avg       0.88      0.88      0.88       239\n",
      "weighted avg       0.88      0.88      0.88       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_demo_vars = ['norm_gender', 'norm_location']\n",
    "device_num = 1\n",
    "train_test_all_demos(sample_balanced_data, valid_demo_vars, device_num=device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The scores are still very high which might be overfitting but is hopefully generalizable to a larger population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention prediction\n",
    "1. Can we predict which posts will receive more attention from a particular audience group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo = norm_gender has N=9 posts\n",
      "Series([], dtype: int64)\n",
      "demo = norm_age has N=3 posts\n",
      "Series([], dtype: int64)\n",
      "demo = norm_location has N=11 posts\n",
      "non_US    2\n",
      "US        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## limit to posts w/ at least N posts from known authors\n",
    "def assign_max_group(data, group_var):\n",
    "    group_counts = data.loc[:, group_var].value_counts().sort_values(ascending=False)\n",
    "    max_group = group_counts.index[0]\n",
    "    if(group_counts.max() == group_counts.min()):\n",
    "        max_group = None\n",
    "    return max_group\n",
    "min_comment_count = 2\n",
    "id_var = 'id_submission'\n",
    "for demo_var_i in demo_vars:\n",
    "    data_i = submission_question_data.dropna(subset=[demo_var_i])\n",
    "    post_comment_counts_i = data_i.groupby(id_var).apply(lambda x: x.loc[:, 'id_comment'].nunique())\n",
    "    valid_post_ids_i = set(post_comment_counts_i[post_comment_counts_i].index)\n",
    "    max_demo_counts_i = data_i[data_i.loc[:, id_var].isin(valid_post_ids_i)].groupby(id_var).apply(lambda x: assign_max_group(x, demo_var_i))\n",
    "    print(f'demo = {demo_var_i} has N={len(valid_post_ids_i)} posts')\n",
    "    print(max_demo_counts_i.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This max-demographic strategy won't work. \n",
    "\n",
    "Let's just do the simplest thing and predict the author group based only on the post text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.780961</td>\n",
       "      <td>0.522388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.61      0.52        31\n",
      "           1       0.52      0.36      0.43        36\n",
      "\n",
      "    accuracy                           0.48        67\n",
      "   macro avg       0.49      0.49      0.47        67\n",
      "weighted avg       0.49      0.48      0.47        67\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.696200</td>\n",
       "      <td>0.704791</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.709397</td>\n",
       "      <td>0.453125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.82      0.58        57\n",
      "           1       0.55      0.17      0.26        71\n",
      "\n",
      "    accuracy                           0.46       128\n",
      "   macro avg       0.49      0.50      0.42       128\n",
      "weighted avg       0.50      0.46      0.40       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_balanced_data = get_sample_balanced_data(submission_question_data, sample_type='min',\n",
    "                                                demo_vars=demo_vars)\n",
    "valid_demo_vars = ['norm_gender', 'norm_location']\n",
    "device_num = 1\n",
    "txt_var = 'title'\n",
    "train_test_all_demos(sample_balanced_data, valid_demo_vars, device_num=device_num, txt_var=txt_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=672/5095 paired data\n",
      "demo=norm_gender has N=672\n",
      "N=30/5095 paired data\n",
      "demo=norm_age has N=30\n",
      "N=1946/5095 paired data\n",
      "demo=norm_location has N=1946\n",
      "testing demo var = norm_gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.699170</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.701641</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.45      0.46        55\n",
      "           1       0.58      0.59      0.58        70\n",
      "\n",
      "    accuracy                           0.53       125\n",
      "   macro avg       0.52      0.52      0.52       125\n",
      "weighted avg       0.53      0.53      0.53       125\n",
      "\n",
      "testing demo var = norm_location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>0.694767</td>\n",
       "      <td>0.481172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691501</td>\n",
       "      <td>0.523013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.689704</td>\n",
       "      <td>0.543933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.688267</td>\n",
       "      <td>0.535565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.67      0.60       123\n",
      "           1       0.53      0.39      0.45       116\n",
      "\n",
      "    accuracy                           0.54       239\n",
      "   macro avg       0.53      0.53      0.52       239\n",
      "weighted avg       0.53      0.54      0.53       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## same thing but with max-sampled data\n",
    "sample_balanced_data = get_sample_balanced_data(submission_question_data, sample_type='max',\n",
    "                                                demo_vars=demo_vars)\n",
    "valid_demo_vars = ['norm_gender', 'norm_location']\n",
    "device_num = 1\n",
    "txt_var = 'title'\n",
    "# fix training args for smaller data?? no too much overfitting\n",
    "# training_args = {\n",
    "#     'per_device_train_batch_size' : 4, \n",
    "#     'learning_rate' : 1e-4, \n",
    "#     'warmup_steps' : 50\n",
    "# }\n",
    "training_args = None\n",
    "train_test_all_demos(sample_balanced_data, valid_demo_vars, \n",
    "                     device_num=device_num, txt_var=txt_var, \n",
    "                     training_args=training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Too much overfitting here. It's harder to predict which group of people will ask questions about a particular post, at least according to our current set-up.\n",
    "\n",
    "TODO: re-collect comment data, save, re-run analysis on all comments (i.e. not just questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction\n",
    "1. Can we predict which words will be mentioned in questions based on the post text? \n",
    "2. Can we do this for specific reader groups?\n",
    "\n",
    "Here the accuracy metric is F1: precision + recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## extract keywords\n",
    "import pandas as pd\n",
    "full_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "print(full_submission_question_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78108/78108 [00:11<00:00, 7026.92it/s]\n",
      "100%|██████████| 78108/78108 [00:05<00:00, 14988.50it/s]\n",
      "100%|██████████| 78108/78108 [00:37<00:00, 2064.70it/s]\n",
      "100%|██████████| 78108/78108 [00:17<00:00, 4472.76it/s]\n",
      "100%|██████████| 78108/78108 [00:03<00:00, 25469.70it/s]\n",
      "100%|██████████| 78108/78108 [00:28<00:00, 2782.67it/s]\n"
     ]
    }
   ],
   "source": [
    "## tokenize, convert to lemmas\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# get stops and stop lemmas!\n",
    "punct = list(',!/&|-:\\'\"()[]\\\\/’?.') + ['...']\n",
    "stops = stopwords.words('english')\n",
    "stops += punct\n",
    "stops += list(set(map(stemmer.stem, stops)))\n",
    "stops = set(stops)\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_tokens' : full_submission_question_data.loc[:, 'title'].progress_apply(lambda x: tokenizer.tokenize(x)),\n",
    "    'question_tokens' : full_submission_question_data.loc[:, 'reply_question'].progress_apply(lambda x: tokenizer.tokenize(x)),\n",
    "})\n",
    "# stem words\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_tokens_clean' : full_submission_question_data.loc[:, 'title_tokens'].progress_apply(lambda x: list(map(lambda y: stemmer.stem(y), x))),\n",
    "    'question_tokens_clean' : full_submission_question_data.loc[:, 'question_tokens'].progress_apply(lambda x: list(map(lambda y: stemmer.stem(y), x))),\n",
    "})\n",
    "## get overlap\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_question_overlap' : full_submission_question_data.progress_apply(lambda x: (set(x.loc['title_tokens_clean']) & set(x.loc['question_tokens_clean'])) - stops, axis=1)\n",
    "})\n",
    "## match overlap tokens with original post tokens\n",
    "full_submission_question_data = full_submission_question_data.assign(**{\n",
    "    'title_question_overlap_clean' : full_submission_question_data.progress_apply(lambda x: [t1 for t1, t2 in zip(x.loc['title_tokens'], x.loc['title_tokens_clean']) if t2 in x.loc['title_question_overlap']], axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>reply_question</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>title_question_overlap_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-country study suggests that the psychological burden of COVID-19 has led to increased political unrest</td>\n",
       "      <td>[Multi-country, study, suggests, that, the, psychological, burden, of, COVID, -, 19, has, led, to, increased, political, unrest]</td>\n",
       "      <td>Are we really trying to blame covid for why my political leadership is incompetent and worthless?</td>\n",
       "      <td>[Are, we, really, trying, to, blame, covid, for, why, my, political, leadership, is, incompetent, and, worthless, ?]</td>\n",
       "      <td>[COVID, political]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multi-country study suggests that the psychological burden of COVID-19 has led to increased political unrest</td>\n",
       "      <td>[Multi-country, study, suggests, that, the, psychological, burden, of, COVID, -, 19, has, led, to, increased, political, unrest]</td>\n",
       "      <td>You mean covid unveils a psychological burden of perpetual political unrest in USA?</td>\n",
       "      <td>[You, mean, covid, unveils, a, psychological, burden, of, perpetual, political, unrest, in, USA, ?]</td>\n",
       "      <td>[psychological, burden, COVID, political, unrest]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-country study suggests that the psychological burden of COVID-19 has led to increased political unrest</td>\n",
       "      <td>[Multi-country, study, suggests, that, the, psychological, burden, of, COVID, -, 19, has, led, to, increased, political, unrest]</td>\n",
       "      <td>Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?</td>\n",
       "      <td>[Psychological, burden, of, the, virus, itself, ,, or, the, various, lockdown, measures, that, forced, people, to, isolate, ?]</td>\n",
       "      <td>[psychological, burden]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cannibal toads eat so many of their young, they're speeding up evolution</td>\n",
       "      <td>[Cannibal, toads, eat, so, many, of, their, young, ,, they're, speeding, up, evolution]</td>\n",
       "      <td>Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?</td>\n",
       "      <td>[Im, slow, ,, but, i, believe, the, abstract, reads, that, the, native, species, are, evolving, to, become, more, cannibalistic, themselves, eating, more, of, the, young, of, the, invaders, ?]</td>\n",
       "      <td>[eat, young]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eviction moratoriums reduced COVID transmission: a difference-in-differences analysis finds that individuals in US states that lifted eviction moratoria were about twice as likely to get COVID in the next 12 weeks compared to residents of states that did not lift moratoria (n=509,694)</td>\n",
       "      <td>[Eviction, moratoriums, reduced, COVID, transmission, :, a, difference-in-differences, analysis, finds, that, individuals, in, US, states, that, lifted, eviction, moratoria, were, about, twice, as, likely, to, get, COVID, in, the, next, 12, weeks, compared, to, residents, of, states, that, did, not, lift, moratoria, (, n, =, 509,694, )]</td>\n",
       "      <td>How would these evictions **double** the Covid rate in an **area**?</td>\n",
       "      <td>[How, would, these, evictions, *, *, double, *, *, the, Covid, rate, in, an, *, *, area, *, *, ?]</td>\n",
       "      <td>[Eviction, COVID, eviction, COVID]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust</td>\n",
       "      <td>[Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]</td>\n",
       "      <td>Wait, who was saying there would be a pandemic baby boom and why?</td>\n",
       "      <td>[Wait, ,, who, was, saying, there, would, be, a, pandemic, baby, boom, and, why, ?]</td>\n",
       "      <td>[pandemic, baby, boom, pandemic, baby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust</td>\n",
       "      <td>[Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]</td>\n",
       "      <td>The baby boom thing was just a joke right?</td>\n",
       "      <td>[The, baby, boom, thing, was, just, a, joke, right, ?]</td>\n",
       "      <td>[baby, boom, baby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust</td>\n",
       "      <td>[Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]</td>\n",
       "      <td>Hmmm existential dread and a fear of hospitals didn’t create a baby boom?</td>\n",
       "      <td>[Hmmm, existential, dread, and, a, fear, of, hospitals, didn, ’, t, create, a, baby, boom, ?]</td>\n",
       "      <td>[baby, boom, baby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust</td>\n",
       "      <td>[Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]</td>\n",
       "      <td>Why would anyone have a baby now or in the future?</td>\n",
       "      <td>[Why, would, anyone, have, a, baby, now, or, in, the, future, ?]</td>\n",
       "      <td>[baby, baby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Toxic ‘forever chemicals’ contaminate indoor air at worrying levels, study finds: Food and water were thought to be the main ways humans are exposed to PFAS, but study points to risk of breathing them in.</td>\n",
       "      <td>[Toxic, ‘, forever, chemicals, ’, contaminate, indoor, air, at, worrying, levels, ,, study, finds, :, Food, and, water, were, thought, to, be, the, main, ways, humans, are, exposed, to, PFAS, ,, but, study, points, to, risk, of, breathing, them, in, .]</td>\n",
       "      <td>What is the best way to measure the air in your house?</td>\n",
       "      <td>[What, is, the, best, way, to, measure, the, air, in, your, house, ?]</td>\n",
       "      <td>[air, ways]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                            title  \\\n",
       "0                                                                                                                                                                                    Multi-country study suggests that the psychological burden of COVID-19 has led to increased political unrest   \n",
       "1                                                                                                                                                                                    Multi-country study suggests that the psychological burden of COVID-19 has led to increased political unrest   \n",
       "2                                                                                                                                                                                    Multi-country study suggests that the psychological burden of COVID-19 has led to increased political unrest   \n",
       "3                                                                                                                                                                                                                        Cannibal toads eat so many of their young, they're speeding up evolution   \n",
       "5   Eviction moratoriums reduced COVID transmission: a difference-in-differences analysis finds that individuals in US states that lifted eviction moratoria were about twice as likely to get COVID in the next 12 weeks compared to residents of states that did not lift moratoria (n=509,694)   \n",
       "6                                                                                                                               Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust   \n",
       "8                                                                                                                               Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust   \n",
       "10                                                                                                                              Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust   \n",
       "12                                                                                                                              Observational data from some high-income countries suggest that early speculations about a pandemic baby boom were wrong, supporting instead a pandemic baby bust   \n",
       "13                                                                                   Toxic ‘forever chemicals’ contaminate indoor air at worrying levels, study finds: Food and water were thought to be the main ways humans are exposed to PFAS, but study points to risk of breathing them in.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                          title_tokens  \\\n",
       "0                                                                                                                                                                                                                     [Multi-country, study, suggests, that, the, psychological, burden, of, COVID, -, 19, has, led, to, increased, political, unrest]   \n",
       "1                                                                                                                                                                                                                     [Multi-country, study, suggests, that, the, psychological, burden, of, COVID, -, 19, has, led, to, increased, political, unrest]   \n",
       "2                                                                                                                                                                                                                     [Multi-country, study, suggests, that, the, psychological, burden, of, COVID, -, 19, has, led, to, increased, political, unrest]   \n",
       "3                                                                                                                                                                                                                                                              [Cannibal, toads, eat, so, many, of, their, young, ,, they're, speeding, up, evolution]   \n",
       "5   [Eviction, moratoriums, reduced, COVID, transmission, :, a, difference-in-differences, analysis, finds, that, individuals, in, US, states, that, lifted, eviction, moratoria, were, about, twice, as, likely, to, get, COVID, in, the, next, 12, weeks, compared, to, residents, of, states, that, did, not, lift, moratoria, (, n, =, 509,694, )]   \n",
       "6                                                                                                                                                          [Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]   \n",
       "8                                                                                                                                                          [Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]   \n",
       "10                                                                                                                                                         [Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]   \n",
       "12                                                                                                                                                         [Observational, data, from, some, high-income, countries, suggest, that, early, speculations, about, a, pandemic, baby, boom, were, wrong, ,, supporting, instead, a, pandemic, baby, bust]   \n",
       "13                                                                                        [Toxic, ‘, forever, chemicals, ’, contaminate, indoor, air, at, worrying, levels, ,, study, finds, :, Food, and, water, were, thought, to, be, the, main, ways, humans, are, exposed, to, PFAS, ,, but, study, points, to, risk, of, breathing, them, in, .]   \n",
       "\n",
       "                                                                                                                                                      reply_question  \\\n",
       "0                                                                  Are we really trying to blame covid for why my political leadership is incompetent and worthless?   \n",
       "1                                                                                You mean covid unveils a psychological burden of perpetual political unrest in USA?   \n",
       "2                                                          Psychological burden of the virus itself, or the various lockdown measures that forced people to isolate?   \n",
       "3   Im slow, but i believe the abstract reads that the native species are evolving to become more cannibalistic themselves eating more of the young of the invaders?   \n",
       "5                                                                                                How would these evictions **double** the Covid rate in an **area**?   \n",
       "6                                                                                                  Wait, who was saying there would be a pandemic baby boom and why?   \n",
       "8                                                                                                                         The baby boom thing was just a joke right?   \n",
       "10                                                                                         Hmmm existential dread and a fear of hospitals didn’t create a baby boom?   \n",
       "12                                                                                                                Why would anyone have a baby now or in the future?   \n",
       "13                                                                                                            What is the best way to measure the air in your house?   \n",
       "\n",
       "                                                                                                                                                                                     question_tokens  \\\n",
       "0                                                                               [Are, we, really, trying, to, blame, covid, for, why, my, political, leadership, is, incompetent, and, worthless, ?]   \n",
       "1                                                                                                [You, mean, covid, unveils, a, psychological, burden, of, perpetual, political, unrest, in, USA, ?]   \n",
       "2                                                                     [Psychological, burden, of, the, virus, itself, ,, or, the, various, lockdown, measures, that, forced, people, to, isolate, ?]   \n",
       "3   [Im, slow, ,, but, i, believe, the, abstract, reads, that, the, native, species, are, evolving, to, become, more, cannibalistic, themselves, eating, more, of, the, young, of, the, invaders, ?]   \n",
       "5                                                                                                  [How, would, these, evictions, *, *, double, *, *, the, Covid, rate, in, an, *, *, area, *, *, ?]   \n",
       "6                                                                                                                [Wait, ,, who, was, saying, there, would, be, a, pandemic, baby, boom, and, why, ?]   \n",
       "8                                                                                                                                             [The, baby, boom, thing, was, just, a, joke, right, ?]   \n",
       "10                                                                                                     [Hmmm, existential, dread, and, a, fear, of, hospitals, didn, ’, t, create, a, baby, boom, ?]   \n",
       "12                                                                                                                                  [Why, would, anyone, have, a, baby, now, or, in, the, future, ?]   \n",
       "13                                                                                                                             [What, is, the, best, way, to, measure, the, air, in, your, house, ?]   \n",
       "\n",
       "                         title_question_overlap_clean  \n",
       "0                                  [COVID, political]  \n",
       "1   [psychological, burden, COVID, political, unrest]  \n",
       "2                             [psychological, burden]  \n",
       "3                                        [eat, young]  \n",
       "5                  [Eviction, COVID, eviction, COVID]  \n",
       "6              [pandemic, baby, boom, pandemic, baby]  \n",
       "8                                  [baby, boom, baby]  \n",
       "10                                 [baby, boom, baby]  \n",
       "12                                       [baby, baby]  \n",
       "13                                        [air, ways]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "overlap_thresh = 1\n",
    "display(full_submission_question_data[full_submission_question_data.loc[:, 'title_question_overlap'].apply(lambda x: len(x) >= overlap_thresh)].head(10).loc[:, ['title', 'title_tokens', 'reply_question', 'question_tokens', 'title_question_overlap_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set this up as a prediction problem:\n",
    "\n",
    "keywords ~ post text (+ author group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! To get the loss function working right, we need to do something like token classification but with the duplicates zero-ed out:\n",
    "\n",
    "X=[this, is, a, test, sentence, about, a, test]\n",
    "\n",
    "I=[1, 2, 3, 4, 5, 6, 3, 4]\n",
    "\n",
    "Y=[0, 0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "P=[0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "Q=[0, 0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "loss(Y,Q) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def update_pred_scores(pred_scores, labels, inputs):\n",
    "    # remove scores for duplicate input\n",
    "    # for 1-label only keep max score; for 0-label, only keep min score\n",
    "    norm_pred_scores = torch.softmax(pred_scores, 1)\n",
    "    print(f'norm scores = {norm_pred_scores}')\n",
    "    drop_idx = []\n",
    "    for input_i in list(set(inputs)):\n",
    "        idx_i = np.where(inputs==input_i)[0]\n",
    "        if(len(idx_i) > 1):\n",
    "            labels_i = labels[idx_i]\n",
    "            # get relevant idx in order of pred score\n",
    "            high_score_idx_i = [x for x in norm_pred_scores[:, 1].argsort(descending=True).tolist() if x in idx_i]\n",
    "            # 1-label: remove scores below max-score\n",
    "            if(any(labels_i==1)):\n",
    "                drop_idx.extend(high_score_idx_i[1:])\n",
    "            # 0-label: remove scores above min-score\n",
    "            else:\n",
    "                drop_idx.extend(high_score_idx_i[:-1])\n",
    "    print(f'drop idx = {drop_idx}')\n",
    "    valid_idx = [i for i in range(pred_scores.shape[0]) if i not in drop_idx]\n",
    "    print(f'valid idx = {valid_idx}')\n",
    "    dedup_pred_scores = pred_scores[valid_idx, :]\n",
    "    dedup_labels = labels[valid_idx]\n",
    "    ## TODO: if this throws error in loss because of reindex, we will re-assign scores\n",
    "    return dedup_pred_scores, dedup_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm scores = tensor([[0.3543, 0.6457],\n",
      "        [0.4750, 0.5250],\n",
      "        [0.4502, 0.5498],\n",
      "        [0.3100, 0.6900]])\n",
      "drop idx = [3, 2]\n",
      "valid idx = [0, 1]\n",
      "(tensor([[-0.1000,  0.5000],\n",
      "        [-0.4000, -0.3000]]), array([1, 0]))\n",
      "norm scores = tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]], grad_fn=<SoftmaxBackward>)\n",
      "drop idx = [1, 2]\n",
      "valid idx = [0, 3]\n",
      "[1 0]\n",
      "tensor(307.4842, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_scores = torch.Tensor([\n",
    "    [-0.1, 0.5], \n",
    "    [-0.4, -0.3],\n",
    "    [-0.1, 0.1],\n",
    "    [-0.9, -0.1]\n",
    "])\n",
    "\n",
    "inputs = np.array([\n",
    "    123,\n",
    "    456,\n",
    "    123,\n",
    "    456\n",
    "])\n",
    "labels = np.array([\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "])\n",
    "print(update_pred_scores(pred_scores, labels, inputs))\n",
    "## test loss\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch import LongTensor\n",
    "ll = Linear(1,2)\n",
    "inputs = np.array([\n",
    "    123,\n",
    "    456,\n",
    "    123,\n",
    "    456\n",
    "])\n",
    "x = torch.Tensor(inputs).unsqueeze(1)\n",
    "y = ll(x)\n",
    "\n",
    "labels = np.array([\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "])\n",
    "y_clean, labels_clean = update_pred_scores(y, labels, inputs)\n",
    "print(labels_clean)\n",
    "loss_func = CrossEntropyLoss()\n",
    "loss = loss_func(y_clean, LongTensor(labels_clean))\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, List\n",
    "from transformers.trainer_utils import EvalLoopOutput, denumpify_detensorize, EvalPrediction\n",
    "from transformers.file_utils import is_torch_tpu_available\n",
    "from transformers.trainer_pt_utils import find_batch_size, nested_concat, nested_numpify, IterableDataset, IterableDatasetShard\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "if is_torch_tpu_available():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "def keyword_score(pred, labels):\n",
    "    pred = set(pred)\n",
    "    labels = set(labels)\n",
    "    TP = pred & labels\n",
    "    FP = pred - labels\n",
    "    FN = labels - pred\n",
    "    prec = TP / (TP + FP)\n",
    "    rec = TP / (TP + FN)\n",
    "    F1 = (prec + rec) / 2.\n",
    "    return prec, rec, F1\n",
    "\n",
    "def update_duplicate_labels(pred_scores, labels, inputs, ignore_val=-100):\n",
    "    # remove scores for duplicate input\n",
    "    # for 1-label only keep max score; for 0-label, only keep min score\n",
    "    norm_pred_scores = torch.softmax(pred_scores, axis=1)\n",
    "    print(f'norm scores = {norm_pred_scores}')\n",
    "    drop_idx = []\n",
    "    for input_i in list(set(inputs)):\n",
    "        idx_i = np.where(inputs==input_i)[0]\n",
    "        if(len(idx_i) > 1):\n",
    "            labels_i = labels[idx_i]\n",
    "            # get relevant idx in order of pred score\n",
    "            high_score_idx_i = [x for x in norm_pred_scores[:, 1].argsort(descending=True).tolist() if x in idx_i]\n",
    "            # 1-label: remove scores below max-score\n",
    "            if(any(labels_i==1)):\n",
    "                drop_idx.extend(high_score_idx_i[1:])\n",
    "            # 0-label: remove scores above min-score\n",
    "            else:\n",
    "                drop_idx.extend(high_score_idx_i[:-1])\n",
    "#     print(f'drop idx = {drop_idx}')\n",
    "    fix_labels = np.array(labels)\n",
    "    fix_labels[drop_idx] = ignore_val\n",
    "#     print(f'valid idx = {valid_idx}')\n",
    "    ## TODO: if this throws error in loss because of reindex, we will re-assign scores\n",
    "    return fix_labels\n",
    "class KeywordLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # override https://github.com/huggingface/transformers/blob/bef1e3e4a00bd0863f804ba0a4e05dc77676a341/src/transformers/trainer.py#compute_loss\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        # tmp debugging\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            input_ids = inputs['input_ids']\n",
    "            labels = inputs['labels']\n",
    "            # update labels\n",
    "            clean_labels = []\n",
    "            for idx_j in range(len(input_ids)):\n",
    "                outputs_j = outputs[idx_j]\n",
    "                pred_scores_j = outputs.logits[idx_j, :, :]\n",
    "                input_j = input_ids[idx_j].tolist()\n",
    "                labels_j = labels[idx_j].tolist()\n",
    "                # assign -100 to duplicate labels\n",
    "                labels_j = update_duplicate_labels(pred_scores_j, labels_j, input_j, ignore_val=-100)\n",
    "                clean_labels.append(labels_j)\n",
    "            labels = torch.cat(clean_labels)\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        # get the ids for words that model output\n",
    "        if(return_outputs):\n",
    "            return loss, outputs\n",
    "        else:\n",
    "            return loss\n",
    "    # custom eval function\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        description: str,\n",
    "        prediction_loss_only: Optional[bool] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> EvalLoopOutput:\n",
    "        \"\"\"\n",
    "        Prediction/evaluation loop, shared by :obj:`Trainer.evaluate()` and :obj:`Trainer.predict()`.\n",
    "        Works both with or without labels.\n",
    "        \"\"\"\n",
    "        args = self.args\n",
    "\n",
    "        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "\n",
    "        # if eval is called w/o train init deepspeed here\n",
    "        if args.deepspeed and not self.deepspeed:\n",
    "\n",
    "            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
    "            # from the checkpoint eventually\n",
    "            deepspeed_engine, _, _ = deepspeed_init(\n",
    "                self, num_training_steps=0, resume_from_checkpoint=None, inference=True\n",
    "            )\n",
    "            self.model = deepspeed_engine.module\n",
    "            self.model_wrapped = deepspeed_engine\n",
    "            self.deepspeed = deepspeed_engine\n",
    "\n",
    "        model = self._wrap_model(self.model, training=False)\n",
    "\n",
    "        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n",
    "        # while ``train`` is running, cast it to the right dtype first and then put on device\n",
    "        if not self.is_in_train:\n",
    "            if args.fp16_full_eval:\n",
    "                model = model.to(dtype=torch.float16, device=args.device)\n",
    "            elif args.bf16_full_eval:\n",
    "                model = model.to(dtype=torch.bfloat16, device=args.device)\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "\n",
    "        logger.info(f\"***** Running {description} *****\")\n",
    "        if isinstance(dataloader.dataset, collections.abc.Sized):\n",
    "            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n",
    "        else:\n",
    "            logger.info(\"  Num examples: Unknown\")\n",
    "        logger.info(f\"  Batch size = {batch_size}\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        self.callback_handler.eval_dataloader = dataloader\n",
    "        # Do this before wrapping.\n",
    "        eval_dataset = dataloader.dataset\n",
    "\n",
    "        if is_torch_tpu_available():\n",
    "            dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n",
    "\n",
    "        if args.past_index >= 0:\n",
    "            self._past = None\n",
    "\n",
    "        # Initialize containers\n",
    "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
    "        losses_host = None\n",
    "        preds_host = None\n",
    "        labels_host = None\n",
    "        # losses/preds/labels on CPU (final containers)\n",
    "        all_losses = None\n",
    "        all_preds = None\n",
    "        all_labels = None\n",
    "        # Will be useful when we have an iterable dataset so don't know its length.\n",
    "        # need all inputs for evaluation\n",
    "        all_inputs = []\n",
    "\n",
    "        observed_num_examples = 0\n",
    "        # Main evaluation loop\n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            # Update the observed num examples\n",
    "            observed_batch_size = find_batch_size(inputs)\n",
    "            if observed_batch_size is not None:\n",
    "                observed_num_examples += observed_batch_size\n",
    "                # For batch samplers, batch_size is not known by the dataloader in advance.\n",
    "                if batch_size is None:\n",
    "                    batch_size = observed_batch_size\n",
    "\n",
    "            # Prediction step\n",
    "            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "\n",
    "            # Update containers on host\n",
    "            if loss is not None:\n",
    "                losses = self._nested_gather(loss.repeat(batch_size))\n",
    "                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
    "            if logits is not None:\n",
    "                logits = self._pad_across_processes(logits)\n",
    "                logits = self._nested_gather(logits)\n",
    "                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
    "            if labels is not None:\n",
    "                labels = self._pad_across_processes(labels)\n",
    "                labels = self._nested_gather(labels)\n",
    "                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
    "            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
    "\n",
    "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
    "            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
    "                if losses_host is not None:\n",
    "                    losses = nested_numpify(losses_host)\n",
    "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "                if preds_host is not None:\n",
    "                    logits = nested_numpify(preds_host)\n",
    "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "                if labels_host is not None:\n",
    "                    labels = nested_numpify(labels_host)\n",
    "                    all_labels = (\n",
    "                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "                    )\n",
    "\n",
    "                # Set back to None to begin a new accumulation\n",
    "                losses_host, preds_host, labels_host = None, None, None\n",
    "            all_inputs.append(inputs)\n",
    "        if args.past_index and hasattr(self, \"_past\"):\n",
    "            # Clean the state at the end of the evaluation loop\n",
    "            delattr(self, \"_past\")\n",
    "\n",
    "        # Gather all remaining tensors and put them back on the CPU\n",
    "        if losses_host is not None:\n",
    "            losses = nested_numpify(losses_host)\n",
    "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "        if preds_host is not None:\n",
    "            logits = nested_numpify(preds_host)\n",
    "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "        if labels_host is not None:\n",
    "            labels = nested_numpify(labels_host)\n",
    "            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "\n",
    "        # Number of samples\n",
    "        if not isinstance(eval_dataset, IterableDataset):\n",
    "            num_samples = len(eval_dataset)\n",
    "        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n",
    "        # methods. Therefore we need to make sure it also has the attribute.\n",
    "        elif isinstance(eval_dataset, IterableDatasetShard) and hasattr(eval_dataset, \"num_examples\"):\n",
    "            num_samples = eval_dataset.num_examples\n",
    "        else:\n",
    "            num_samples = observed_num_examples\n",
    "\n",
    "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
    "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
    "        if all_losses is not None:\n",
    "            all_losses = all_losses[:num_samples]\n",
    "        if all_preds is not None:\n",
    "            all_preds = nested_truncate(all_preds, num_samples)\n",
    "        if all_labels is not None:\n",
    "            all_labels = nested_truncate(all_labels, num_samples)\n",
    "\n",
    "        # Metrics!\n",
    "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
    "            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels), all_inputs)\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
    "        metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "        if all_losses is not None:\n",
    "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels, inputs = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        pred = [i for i,o in zip(inputs, predictions) if o==1]\n",
    "        prec, rec, F1 = keyword_score(pred, labels)\n",
    "        return {\n",
    "            'precision' : prec,\n",
    "            'rec' : rec,\n",
    "            'F1' : F1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# tokenize, etc.\n",
    "def convert_keyword_to_output_label(input_tokens, keywords):\n",
    "    output_labels = pd.Series(input_tokens).isin(keywords).astype(int).values\n",
    "    return output_labels\n",
    "\n",
    "def build_dataset(tokenizer, inputs, outputs, max_length=100):\n",
    "    input_token_data = tokenizer(inputs, padding=True, max_length=max_length)\n",
    "    input_tokens = [tokenizer.convert_ids_to_tokens(x) for x in input_token_data['input_ids']]\n",
    "    output_ids = list(map(lambda x: convert_keyword_to_output_label(x[0], x[1]), zip(input_tokens, outputs)))\n",
    "    data_dict = {\n",
    "    'input_ids' : input_token_data['input_ids'],\n",
    "    'attention_mask' : input_token_data['attention_mask'],\n",
    "    'labels' : output_ids,\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1142, 1110,  170, 2774, 5650,  102,    0,    0,    0],\n",
      "        [ 101, 1142, 1110,  170, 2963, 5650, 1164,  170, 2963,  102],\n",
      "        [ 101,  146, 1138,  170, 2963, 1303,  102,    0,    0,    0]])\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "## create baby datasets\n",
    "from transformers import DistilBertTokenizer\n",
    "train_inputs = [\n",
    "    'this is a test sentence',\n",
    "    'this is a baby sentence about a baby',\n",
    "    'I have a baby here',\n",
    "]\n",
    "train_keywords = [\n",
    "    ['sentence'],\n",
    "    ['sentence', 'baby'],\n",
    "    ['baby'],\n",
    "]\n",
    "val_inputs = [\n",
    "    'I have a test example here'\n",
    "]\n",
    "val_keywords = [\n",
    "    ['example']\n",
    "]\n",
    "model_name = 'distilbert-base-cased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "train_dataset = build_dataset(tokenizer, train_inputs, train_keywords)\n",
    "val_dataset = build_dataset(tokenizer, val_inputs, val_keywords)\n",
    "print(train_dataset['input_ids'])\n",
    "print(train_dataset['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load model fml\n",
    "from transformers import DistilBertForTokenClassification\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_name).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir='runs/', num_train_epochs=100, save_total_limit=1)\n",
    "trainer = KeywordLossTrainer(model=model, train_dataset=train_dataset, eval_dataset=val_dataset, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:15, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have a test example here']\n",
      "[['example']]\n",
      "{'input_ids': tensor([ 101,  146, 1138,  170, 2774, 1859, 1303,  102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print(val_inputs)\n",
    "print(val_keywords)\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[9.9995e-01, 4.5393e-05],\n",
      "         [9.9995e-01, 5.4430e-05],\n",
      "         [9.9995e-01, 5.2777e-05],\n",
      "         [9.9997e-01, 3.4538e-05],\n",
      "         [9.9907e-01, 9.2932e-04],\n",
      "         [7.7754e-03, 9.9222e-01],\n",
      "         [9.9986e-01, 1.4153e-04],\n",
      "         [9.9996e-01, 4.4286e-05]]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "## does the trained model accurately extract the right keyword?\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    val_output = trainer.model(**{k : v.to(trainer.model.device) for k,v in val_dataset[[0]].items()})\n",
    "print(torch.softmax(val_output.logits, axis=2))\n",
    "print(val_output.logits.argmax(2))\n",
    "print(val_dataset['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have a very bad demo for keyword extraction in the case of nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## same thing but with actual data lol\n",
    "# # reload submissions, questions, etc\n",
    "# ## extract keywords\n",
    "# import pandas as pd\n",
    "# from importlib import reload\n",
    "# import demo_keyword_extractor\n",
    "# reload(demo_keyword_extractor)\n",
    "# from demo_keyword_extractor import extract_keywords_from_data\n",
    "# from transformers import DistilBertTokenizer\n",
    "# # full_submission_question_data = pd.read_csv('science_submission_question_data.gz', sep='\\t', compression='gzip')\n",
    "# # print(full_submission_question_data.shape[0])\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# ## add new tokens e.g. \"COVID\" missing from tokenizer vocab\n",
    "# ## get word counts\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# from nltk.tokenize.casual import TweetTokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# stops = list(stopwords.words('english'))\n",
    "# stops += list(map(lambda x: x.replace(\"'\", ''), stops))\n",
    "# stops = set(stops)\n",
    "# pre_tokenizer = TweetTokenizer()\n",
    "# # question_tokens = full_submission_question_data.loc[:, 'reply_question'].progress_apply(lambda x: pre_tokenizer.tokenize(x.lower())).values\n",
    "# question_word_counts = pd.Series([y for x in question_tokens for y in x if y not in stops and \"'\" not in y]).value_counts().sort_values(ascending=False)\n",
    "# top_k = 5000\n",
    "# extra_vocab = set(question_word_counts.index[:top_k]) - set(tokenizer.vocab.keys())\n",
    "# print(f'{len(extra_vocab)} extra tokens to add')\n",
    "# tokenizer.add_tokens(list(extra_vocab))\n",
    "# full_submission_question_data = extract_keywords_from_data(full_submission_question_data, tokenizer=tokenizer)\n",
    "# ## remove bad tokens from keywords\n",
    "# full_submission_question_data = full_submission_question_data.assign(**{\n",
    "#     'title_question_overlap_clean' : full_submission_question_data.loc[:, 'title_question_overlap_clean'].apply(lambda x: list(filter(lambda y: not y.startswith('#') and y not in stops, x)))\n",
    "# })\n",
    "## save for posterity\n",
    "title_keyword_data = full_submission_question_data.loc[:, ['id_submission', 'id_comment', 'title', 'reply_question', 'title_tokens', 'title_tokens_clean', 'question_tokens', 'question_tokens_clean', 'title_question_overlap_clean']]\n",
    "title_keyword_data.to_csv('science_submission_question_keyword_data.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_submission</th>\n",
       "      <th>id_comment</th>\n",
       "      <th>title</th>\n",
       "      <th>reply_question</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_tokens_clean</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>question_tokens_clean</th>\n",
       "      <th>title_question_overlap_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7861</th>\n",
       "      <td>ofnr4a</td>\n",
       "      <td>h4fvj4g</td>\n",
       "      <td>The impact of loneliness in old age on life and health expectancy. Findings show that people aged 60, who perceive themselves to be sometimes lonely or mostly lonely, can expect to live three to five years less, on average, compared to peers who perceive themselves as never lonely.</td>\n",
       "      <td>I mean, does someone who is lonely want to live a longer lonely life?</td>\n",
       "      <td>[the, impact, of, loneliness, in, old, age, on, life, and, health, expectancy, ., findings, show, that, people, aged, 60, ,, who, per, ##ce, ive, themselves, to, be, sometimes, lonely, or, mo, ##s, tl, y, lonely, ,, can, expect, to, l, ive, three, to, f, ive, yea, rs, less, ,, on, average, ,, compared, to, peers, who, per, ##ce, ive, themselves, as, never, lonely, .]</td>\n",
       "      <td>[the, impact, of, loneli, in, old, age, on, life, and, health, expect, ., find, show, that, peopl, age, 60, ,, who, per, ##ce, ive, themselv, to, be, sometim, lone, or, mo, ##, tl, y, lone, ,, can, expect, to, l, ive, three, to, f, ive, yea, rs, less, ,, on, averag, ,, compar, to, peer, who, per, ##ce, ive, themselv, as, never, lone, .]</td>\n",
       "      <td>[i, mean, ,, doe, someon, who, is, lone, want, to, l, ive, a, longer, lone, life, ?]</td>\n",
       "      <td>[i, mean, ,, doe, someon, who, is, lone, want, to, l, ive, a, longer, lone, life, ?]</td>\n",
       "      <td>[life, ive, lonely, lonely, l, ive, ive, ive, lonely]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>p49gis</td>\n",
       "      <td>h8zbnnc</td>\n",
       "      <td>Making women feel comfortable on mostly male work teams may be easier than previously thought. A new psychology study finds that just one man can have a big impact: When one male coworker expresses support of gender equality in a male-dominated workplace, women feel more comfortable at work.</td>\n",
       "      <td>wait...being nice to people makes them feel more comfortable?</td>\n",
       "      <td>[making, women, feel, comfortable, on, mo, ##s, tl, y, male, work, teams, may, be, easier, than, previously, thought, ., a, new, psychology, study, finds, that, just, one, man, can, have, a, big, impact, :, when, one, male, cow, ##or, ##ker, expresses, support, of, gender, equality, in, a, male, -, dominated, workplace, ,, women, feel, more, comfortable, at, work, .]</td>\n",
       "      <td>[make, women, feel, comfort, on, mo, ##, tl, y, male, work, team, may, be, easier, than, previous, thought, ., a, new, psycholog, studi, find, that, just, one, man, can, have, a, big, impact, :, when, one, male, cow, ##or, ##ker, express, support, of, gender, equal, in, a, male, -, domin, workplac, ,, women, feel, more, comfort, at, work, .]</td>\n",
       "      <td>[wait, .., ., be, nice, to, peopl, make, them, feel, more, comfort, ?]</td>\n",
       "      <td>[wait, .., ., be, nice, to, peopl, make, them, feel, more, comfort, ?]</td>\n",
       "      <td>[making, feel, comfortable, feel, comfortable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73267</th>\n",
       "      <td>ewdu7x</td>\n",
       "      <td>fg24t5s</td>\n",
       "      <td>Fatal car accidents in the United States spike by 6% during the workweek following the “spring forward” to daylight saving time, resulting in about 28 additional deaths each year, according to new University of Colorado Boulder research.</td>\n",
       "      <td>Is 28 additional death over a week's worth of accidents in the whole nation statistically significant to confirm that it is related to the DST change?</td>\n",
       "      <td>[fatal, car, accidents, in, the, united, states, spike, by, 6, %, during, the, work, ##week, following, the, “, spring, forward, ”, to, daylight, saving, time, ,, resulting, in, about, 28, additional, deaths, each, yea, r, ,, according, to, new, un, ive, rs, ##ity, of, colorado, boulder, research, .]</td>\n",
       "      <td>[fatal, car, accid, in, the, unit, state, spike, by, 6, %, dure, the, work, ##week, follow, the, “, spring, forward, ”, to, daylight, save, time, ,, result, in, about, 28, addit, death, each, yea, r, ,, accord, to, new, un, ive, rs, ##iti, of, colorado, boulder, research, .]</td>\n",
       "      <td>[is, 28, addit, death, over, a, week, ', s, worth, of, accid, in, the, whole, nation, statist, alli, sign, ##if, ##i, cant, to, confirm, that, it, is, relat, to, the, ds, ##t, chang, ?]</td>\n",
       "      <td>[is, 28, addit, death, over, a, week, ', s, worth, of, accid, in, the, whole, nation, statist, alli, sign, ##if, ##i, cant, to, confirm, that, it, is, relat, to, the, ds, ##t, chang, ?]</td>\n",
       "      <td>[accidents, 28, additional, deaths]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69994</th>\n",
       "      <td>f9bjil</td>\n",
       "      <td>fis07sd</td>\n",
       "      <td>Planned Parenthood clinic closures increased maternal mortality rates.</td>\n",
       "      <td>So how many more do we need to open to completely eliminate maternal mortality rates?</td>\n",
       "      <td>[planned, parent, ##hood, clinic, closure, ##s, increased, maternal, mortality, rates, .]</td>\n",
       "      <td>[plan, parent, ##hood, clinic, closur, ##, increas, matern, mortal, rate, .]</td>\n",
       "      <td>[so, how, mani, more, do, we, need, to, open, to, complet, elimin, matern, mortal, rate, ?]</td>\n",
       "      <td>[so, how, mani, more, do, we, need, to, open, to, complet, elimin, matern, mortal, rate, ?]</td>\n",
       "      <td>[maternal, mortality, rates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51271</th>\n",
       "      <td>i5prcy</td>\n",
       "      <td>g0sjwsp</td>\n",
       "      <td>Successful Elimination of Covid-19 Transmission in New Zealand - Rapid, science-based risk assessment linked to early, decisive government action was critical, and implementing interventions at various levels was effective, finds new paper in NEJM (7 August 2020).</td>\n",
       "      <td>Soo.. how is the pay for pharmacists in NZ?</td>\n",
       "      <td>[successful, elimination, of, cov, id, -, 19, transmission, in, new, zealand, -, rapid, ,, science, -, based, risk, asses, sm, ##ent, linked, to, early, ,, dec, ##is, ive, government, action, was, critical, ,, and, implementing, interventions, at, various, levels, was, effect, ive, ,, finds, new, paper, in, ne, ##jm, (, 7, august, 2020, ), .]</td>\n",
       "      <td>[success, elimin, of, cov, id, -, 19, transmiss, in, new, zealand, -, rapid, ,, scienc, -, base, risk, ass, sm, ##ent, link, to, earli, ,, dec, ##i, ive, govern, action, wa, critic, ,, and, implement, intervent, at, variou, level, wa, effect, ive, ,, find, new, paper, in, ne, ##jm, (, 7, august, 2020, ), .]</td>\n",
       "      <td>[soo, .., how, is, the, pay, for, pharma, ci, ##t, in, nz, ?]</td>\n",
       "      <td>[soo, .., how, is, the, pay, for, pharma, ci, ##t, in, nz, ?]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47185</th>\n",
       "      <td>iqdiie</td>\n",
       "      <td>g4se6xb</td>\n",
       "      <td>Researchers put people aged over 65 with some cognitive function decline into two groups who spent six months making lifestyle changes in diet, exercise and brain training. Those given extra support were found to have a lower risk of Alzheimer's disease and improved cognitive abilities.</td>\n",
       "      <td>I thought they proved that Alzheimers had to do with a virus like Herpes?</td>\n",
       "      <td>[researchers, put, people, aged, over, 65, with, some, co, ##gni, ##t, ive, function, decline, into, two, groups, who, spent, six, months, making, lifestyle, changes, in, diet, ,, exercise, and, brain, training, ., those, g, ive, n, extra, support, were, found, to, have, a, lower, risk, of, alzheimer, ', s, disease, and, improved, co, ##gni, ##t, ive, abilities, .]</td>\n",
       "      <td>[research, put, peopl, age, over, 65, with, some, co, ##gni, ##t, ive, function, declin, into, two, group, who, spent, six, month, make, lifestyl, chang, in, diet, ,, exercis, and, brain, train, ., those, g, ive, n, extra, support, were, found, to, have, a, lower, risk, of, alzheim, ', s, diseas, and, improv, co, ##gni, ##t, ive, abil, .]</td>\n",
       "      <td>[i, thought, they, prove, that, alzheim, had, to, do, with, a, viru, like, herp, ?]</td>\n",
       "      <td>[i, thought, they, prove, that, alzheim, had, to, do, with, a, viru, like, herp, ?]</td>\n",
       "      <td>[alzheimer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50413</th>\n",
       "      <td>ib67wp</td>\n",
       "      <td>g1umjqo</td>\n",
       "      <td>Two Supreme Court rulings to loosen campaign finance restrictions (Citizens United v. FEC, SpeechNow.org v. FEC) increased the electoral success of Republican candidates. The pro-Republican effect of Citizens United is stronger in states where labor unions are relatively weaker.</td>\n",
       "      <td>Why not just own the fact that the sub is now also political, like the rest of Reddit?</td>\n",
       "      <td>[two, su, ##p, rem, e, court, ruling, ##s, to, loosen, campaign, finance, restrictions, (, citizens, united, v, ., fe, ##c, ,, speech, ##now, ., org, v, ., fe, ##c, ), increased, the, electoral, success, of, republican, candidates, ., the, pro, -, republican, effect, of, citizens, united, is, stronger, in, states, where, labor, unions, are, re, ##lat, ive, l, ##y, weaker, .]</td>\n",
       "      <td>[two, su, ##p, rem, e, court, rule, ##, to, loosen, campaign, financ, restrict, (, citizen, unit, v, ., fe, ##c, ,, speech, ##now, ., org, v, ., fe, ##c, ), increas, the, elector, success, of, republican, candid, ., the, pro, -, republican, effect, of, citizen, unit, is, stronger, in, state, where, labor, union, are, re, ##lat, ive, l, ##i, weaker, .]</td>\n",
       "      <td>[whi, not, just, own, the, fact, that, the, sub, is, now, also, polit, ,, like, the, rest, of, reddit, ?]</td>\n",
       "      <td>[whi, not, just, own, the, fact, that, the, sub, is, now, also, polit, ,, like, the, rest, of, reddit, ?]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22200</th>\n",
       "      <td>m6zi2s</td>\n",
       "      <td>gr8u333</td>\n",
       "      <td>Singaporean scientists develop device to 'communicate' with plants using electrical signals. As a proof-of concept, they attached a Venus flytrap to a robotic arm and, through a smartphone, stimulated its leaf to pick up a piece of wire, demonstrating the potential of plant-based robotic systems.</td>\n",
       "      <td>Cool, but now we are going to force plants into labor too?</td>\n",
       "      <td>[singapore, ##an, scientists, develop, device, to, ', communicate, ', with, plants, using, electrical, signals, ., as, a, proof, -, of, concept, ,, they, attached, a, venus, fly, ##tra, ##p, to, a, robotic, arm, and, ,, through, a, smartphone, ,, stimulated, its, leaf, to, pick, up, a, piece, of, wire, ,, demonstrating, the, potential, of, plant, -, based, robotic, systems, .]</td>\n",
       "      <td>[singapor, ##an, scientist, develop, devic, to, ', commun, ', with, plant, use, electr, signal, ., as, a, proof, -, of, concept, ,, they, attach, a, venu, fli, ##tra, ##p, to, a, robot, arm, and, ,, through, a, smartphon, ,, stimul, it, leaf, to, pick, up, a, piec, of, wire, ,, demonstr, the, potenti, of, plant, -, base, robot, system, .]</td>\n",
       "      <td>[cool, ,, but, now, we, are, go, to, forc, plant, into, labor, too, ?]</td>\n",
       "      <td>[cool, ,, but, now, we, are, go, to, forc, plant, into, labor, too, ?]</td>\n",
       "      <td>[plants, plant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40440</th>\n",
       "      <td>jud6z7</td>\n",
       "      <td>gcc7dh4</td>\n",
       "      <td>Conservatives and liberals differ on COVID-19 because conservatives tend to attribute negative outcomes to purposeful actions by threats high in agency. If health officials talked about the virus as a palpable enemy that is seeking to attack humans, they may get greater buy-in from conservatives.</td>\n",
       "      <td>Now, how do you explain this to a conservative without them punching you and abandoning science even more than 'all the way'?</td>\n",
       "      <td>[con, ##ser, ##vat, ive, s, and, liberals, differ, on, cov, id, -, 19, because, con, ##ser, ##vat, ive, s, tend, to, attribute, n, eg, at, ive, outcomes, to, purpose, ##ful, actions, by, threats, high, in, agency, ., if, health, officials, talked, about, the, virus, as, a, pal, ##pa, ##ble, enemy, that, is, seeking, to, attack, humans, ,, they, may, get, greater, buy, -, in, from, con, ##ser, ##vat, ive, s, .]</td>\n",
       "      <td>[con, ##ser, ##vat, ive, s, and, liber, differ, on, cov, id, -, 19, becaus, con, ##ser, ##vat, ive, s, tend, to, attribut, n, eg, at, ive, outcom, to, purpos, ##ful, action, by, threat, high, in, agenc, ., if, health, offici, talk, about, the, viru, as, a, pal, ##pa, ##ble, enemi, that, is, seek, to, attack, human, ,, they, may, get, greater, buy, -, in, from, con, ##ser, ##vat, ive, s, .]</td>\n",
       "      <td>[now, ,, how, do, you, explain, thi, to, a, con, ##ser, ##vat, ive, without, them, punch, you, and, abandon, scienc, even, more, than, ', all, the, way, ', ?]</td>\n",
       "      <td>[now, ,, how, do, you, explain, thi, to, a, con, ##ser, ##vat, ive, without, them, punch, you, and, abandon, scienc, even, more, than, ', all, the, way, ', ?]</td>\n",
       "      <td>[con, ive, con, ive, ive, con, ive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62878</th>\n",
       "      <td>gbi0ug</td>\n",
       "      <td>fp6iopj</td>\n",
       "      <td>Reddit Discussion Series: We’re geneticists studying animal models to help us solve biological problems related to human disease. Let’s discuss!</td>\n",
       "      <td>How did you guys end up in your respective fields?</td>\n",
       "      <td>[reddit, discussion, series, :, we, ’, re, genetic, ##ists, studying, animal, models, to, help, us, solve, biological, problems, related, to, human, disease, ., let, ’, s, discuss, !]</td>\n",
       "      <td>[reddit, discuss, seri, :, we, ’, re, genet, ##ist, studi, anim, model, to, help, us, solv, biolog, problem, relat, to, human, diseas, ., let, ’, s, discuss, !]</td>\n",
       "      <td>[how, did, you, guy, end, up, in, your, respect, ive, field, ?]</td>\n",
       "      <td>[how, did, you, guy, end, up, in, your, respect, ive, field, ?]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_submission id_comment  \\\n",
       "7861         ofnr4a    h4fvj4g   \n",
       "2523         p49gis    h8zbnnc   \n",
       "73267        ewdu7x    fg24t5s   \n",
       "69994        f9bjil    fis07sd   \n",
       "51271        i5prcy    g0sjwsp   \n",
       "...             ...        ...   \n",
       "47185        iqdiie    g4se6xb   \n",
       "50413        ib67wp    g1umjqo   \n",
       "22200        m6zi2s    gr8u333   \n",
       "40440        jud6z7    gcc7dh4   \n",
       "62878        gbi0ug    fp6iopj   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           title  \\\n",
       "7861                  The impact of loneliness in old age on life and health expectancy. Findings show that people aged 60, who perceive themselves to be sometimes lonely or mostly lonely, can expect to live three to five years less, on average, compared to peers who perceive themselves as never lonely.   \n",
       "2523        Making women feel comfortable on mostly male work teams may be easier than previously thought. A new psychology study finds that just one man can have a big impact: When one male coworker expresses support of gender equality in a male-dominated workplace, women feel more comfortable at work.   \n",
       "73267                                                              Fatal car accidents in the United States spike by 6% during the workweek following the “spring forward” to daylight saving time, resulting in about 28 additional deaths each year, according to new University of Colorado Boulder research.   \n",
       "69994                                                                                                                                                                                                                                     Planned Parenthood clinic closures increased maternal mortality rates.   \n",
       "51271                                   Successful Elimination of Covid-19 Transmission in New Zealand - Rapid, science-based risk assessment linked to early, decisive government action was critical, and implementing interventions at various levels was effective, finds new paper in NEJM (7 August 2020).   \n",
       "...                                                                                                                                                                                                                                                                                                          ...   \n",
       "47185            Researchers put people aged over 65 with some cognitive function decline into two groups who spent six months making lifestyle changes in diet, exercise and brain training. Those given extra support were found to have a lower risk of Alzheimer's disease and improved cognitive abilities.   \n",
       "50413                    Two Supreme Court rulings to loosen campaign finance restrictions (Citizens United v. FEC, SpeechNow.org v. FEC) increased the electoral success of Republican candidates. The pro-Republican effect of Citizens United is stronger in states where labor unions are relatively weaker.   \n",
       "22200  Singaporean scientists develop device to 'communicate' with plants using electrical signals. As a proof-of concept, they attached a Venus flytrap to a robotic arm and, through a smartphone, stimulated its leaf to pick up a piece of wire, demonstrating the potential of plant-based robotic systems.   \n",
       "40440  Conservatives and liberals differ on COVID-19 because conservatives tend to attribute negative outcomes to purposeful actions by threats high in agency. If health officials talked about the virus as a palpable enemy that is seeking to attack humans, they may get greater buy-in from conservatives.   \n",
       "62878                                                                                                                                                           Reddit Discussion Series: We’re geneticists studying animal models to help us solve biological problems related to human disease. Let’s discuss!   \n",
       "\n",
       "                                                                                                                                               reply_question  \\\n",
       "7861                                                                                    I mean, does someone who is lonely want to live a longer lonely life?   \n",
       "2523                                                                                            wait...being nice to people makes them feel more comfortable?   \n",
       "73267  Is 28 additional death over a week's worth of accidents in the whole nation statistically significant to confirm that it is related to the DST change?   \n",
       "69994                                                                   So how many more do we need to open to completely eliminate maternal mortality rates?   \n",
       "51271                                                                                                             Soo.. how is the pay for pharmacists in NZ?   \n",
       "...                                                                                                                                                       ...   \n",
       "47185                                                                               I thought they proved that Alzheimers had to do with a virus like Herpes?   \n",
       "50413                                                                  Why not just own the fact that the sub is now also political, like the rest of Reddit?   \n",
       "22200                                                                                              Cool, but now we are going to force plants into labor too?   \n",
       "40440                           Now, how do you explain this to a conservative without them punching you and abandoning science even more than 'all the way'?   \n",
       "62878                                                                                                      How did you guys end up in your respective fields?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                        title_tokens  \\\n",
       "7861                                               [the, impact, of, loneliness, in, old, age, on, life, and, health, expectancy, ., findings, show, that, people, aged, 60, ,, who, per, ##ce, ive, themselves, to, be, sometimes, lonely, or, mo, ##s, tl, y, lonely, ,, can, expect, to, l, ive, three, to, f, ive, yea, rs, less, ,, on, average, ,, compared, to, peers, who, per, ##ce, ive, themselves, as, never, lonely, .]   \n",
       "2523                                               [making, women, feel, comfortable, on, mo, ##s, tl, y, male, work, teams, may, be, easier, than, previously, thought, ., a, new, psychology, study, finds, that, just, one, man, can, have, a, big, impact, :, when, one, male, cow, ##or, ##ker, expresses, support, of, gender, equality, in, a, male, -, dominated, workplace, ,, women, feel, more, comfortable, at, work, .]   \n",
       "73267                                                                                                                  [fatal, car, accidents, in, the, united, states, spike, by, 6, %, during, the, work, ##week, following, the, “, spring, forward, ”, to, daylight, saving, time, ,, resulting, in, about, 28, additional, deaths, each, yea, r, ,, according, to, new, un, ive, rs, ##ity, of, colorado, boulder, research, .]   \n",
       "69994                                                                                                                                                                                                                                                                                                                                      [planned, parent, ##hood, clinic, closure, ##s, increased, maternal, mortality, rates, .]   \n",
       "51271                                                                       [successful, elimination, of, cov, id, -, 19, transmission, in, new, zealand, -, rapid, ,, science, -, based, risk, asses, sm, ##ent, linked, to, early, ,, dec, ##is, ive, government, action, was, critical, ,, and, implementing, interventions, at, various, levels, was, effect, ive, ,, finds, new, paper, in, ne, ##jm, (, 7, august, 2020, ), .]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                              ...   \n",
       "47185                                                [researchers, put, people, aged, over, 65, with, some, co, ##gni, ##t, ive, function, decline, into, two, groups, who, spent, six, months, making, lifestyle, changes, in, diet, ,, exercise, and, brain, training, ., those, g, ive, n, extra, support, were, found, to, have, a, lower, risk, of, alzheimer, ', s, disease, and, improved, co, ##gni, ##t, ive, abilities, .]   \n",
       "50413                                      [two, su, ##p, rem, e, court, ruling, ##s, to, loosen, campaign, finance, restrictions, (, citizens, united, v, ., fe, ##c, ,, speech, ##now, ., org, v, ., fe, ##c, ), increased, the, electoral, success, of, republican, candidates, ., the, pro, -, republican, effect, of, citizens, united, is, stronger, in, states, where, labor, unions, are, re, ##lat, ive, l, ##y, weaker, .]   \n",
       "22200                                    [singapore, ##an, scientists, develop, device, to, ', communicate, ', with, plants, using, electrical, signals, ., as, a, proof, -, of, concept, ,, they, attached, a, venus, fly, ##tra, ##p, to, a, robotic, arm, and, ,, through, a, smartphone, ,, stimulated, its, leaf, to, pick, up, a, piece, of, wire, ,, demonstrating, the, potential, of, plant, -, based, robotic, systems, .]   \n",
       "40440  [con, ##ser, ##vat, ive, s, and, liberals, differ, on, cov, id, -, 19, because, con, ##ser, ##vat, ive, s, tend, to, attribute, n, eg, at, ive, outcomes, to, purpose, ##ful, actions, by, threats, high, in, agency, ., if, health, officials, talked, about, the, virus, as, a, pal, ##pa, ##ble, enemy, that, is, seeking, to, attack, humans, ,, they, may, get, greater, buy, -, in, from, con, ##ser, ##vat, ive, s, .]   \n",
       "62878                                                                                                                                                                                                                                        [reddit, discussion, series, :, we, ’, re, genetic, ##ists, studying, animal, models, to, help, us, solve, biological, problems, related, to, human, disease, ., let, ’, s, discuss, !]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                             title_tokens_clean  \\\n",
       "7861                                                         [the, impact, of, loneli, in, old, age, on, life, and, health, expect, ., find, show, that, peopl, age, 60, ,, who, per, ##ce, ive, themselv, to, be, sometim, lone, or, mo, ##, tl, y, lone, ,, can, expect, to, l, ive, three, to, f, ive, yea, rs, less, ,, on, averag, ,, compar, to, peer, who, per, ##ce, ive, themselv, as, never, lone, .]   \n",
       "2523                                                    [make, women, feel, comfort, on, mo, ##, tl, y, male, work, team, may, be, easier, than, previous, thought, ., a, new, psycholog, studi, find, that, just, one, man, can, have, a, big, impact, :, when, one, male, cow, ##or, ##ker, express, support, of, gender, equal, in, a, male, -, domin, workplac, ,, women, feel, more, comfort, at, work, .]   \n",
       "73267                                                                                                                       [fatal, car, accid, in, the, unit, state, spike, by, 6, %, dure, the, work, ##week, follow, the, “, spring, forward, ”, to, daylight, save, time, ,, result, in, about, 28, addit, death, each, yea, r, ,, accord, to, new, un, ive, rs, ##iti, of, colorado, boulder, research, .]   \n",
       "69994                                                                                                                                                                                                                                                                                                                              [plan, parent, ##hood, clinic, closur, ##, increas, matern, mortal, rate, .]   \n",
       "51271                                                                                      [success, elimin, of, cov, id, -, 19, transmiss, in, new, zealand, -, rapid, ,, scienc, -, base, risk, ass, sm, ##ent, link, to, earli, ,, dec, ##i, ive, govern, action, wa, critic, ,, and, implement, intervent, at, variou, level, wa, effect, ive, ,, find, new, paper, in, ne, ##jm, (, 7, august, 2020, ), .]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                         ...   \n",
       "47185                                                      [research, put, peopl, age, over, 65, with, some, co, ##gni, ##t, ive, function, declin, into, two, group, who, spent, six, month, make, lifestyl, chang, in, diet, ,, exercis, and, brain, train, ., those, g, ive, n, extra, support, were, found, to, have, a, lower, risk, of, alzheim, ', s, diseas, and, improv, co, ##gni, ##t, ive, abil, .]   \n",
       "50413                                         [two, su, ##p, rem, e, court, rule, ##, to, loosen, campaign, financ, restrict, (, citizen, unit, v, ., fe, ##c, ,, speech, ##now, ., org, v, ., fe, ##c, ), increas, the, elector, success, of, republican, candid, ., the, pro, -, republican, effect, of, citizen, unit, is, stronger, in, state, where, labor, union, are, re, ##lat, ive, l, ##i, weaker, .]   \n",
       "22200                                                      [singapor, ##an, scientist, develop, devic, to, ', commun, ', with, plant, use, electr, signal, ., as, a, proof, -, of, concept, ,, they, attach, a, venu, fli, ##tra, ##p, to, a, robot, arm, and, ,, through, a, smartphon, ,, stimul, it, leaf, to, pick, up, a, piec, of, wire, ,, demonstr, the, potenti, of, plant, -, base, robot, system, .]   \n",
       "40440  [con, ##ser, ##vat, ive, s, and, liber, differ, on, cov, id, -, 19, becaus, con, ##ser, ##vat, ive, s, tend, to, attribut, n, eg, at, ive, outcom, to, purpos, ##ful, action, by, threat, high, in, agenc, ., if, health, offici, talk, about, the, viru, as, a, pal, ##pa, ##ble, enemi, that, is, seek, to, attack, human, ,, they, may, get, greater, buy, -, in, from, con, ##ser, ##vat, ive, s, .]   \n",
       "62878                                                                                                                                                                                                                                          [reddit, discuss, seri, :, we, ’, re, genet, ##ist, studi, anim, model, to, help, us, solv, biolog, problem, relat, to, human, diseas, ., let, ’, s, discuss, !]   \n",
       "\n",
       "                                                                                                                                                                                 question_tokens  \\\n",
       "7861                                                                                                        [i, mean, ,, doe, someon, who, is, lone, want, to, l, ive, a, longer, lone, life, ?]   \n",
       "2523                                                                                                                      [wait, .., ., be, nice, to, peopl, make, them, feel, more, comfort, ?]   \n",
       "73267  [is, 28, addit, death, over, a, week, ', s, worth, of, accid, in, the, whole, nation, statist, alli, sign, ##if, ##i, cant, to, confirm, that, it, is, relat, to, the, ds, ##t, chang, ?]   \n",
       "69994                                                                                                [so, how, mani, more, do, we, need, to, open, to, complet, elimin, matern, mortal, rate, ?]   \n",
       "51271                                                                                                                              [soo, .., how, is, the, pay, for, pharma, ci, ##t, in, nz, ?]   \n",
       "...                                                                                                                                                                                          ...   \n",
       "47185                                                                                                        [i, thought, they, prove, that, alzheim, had, to, do, with, a, viru, like, herp, ?]   \n",
       "50413                                                                                  [whi, not, just, own, the, fact, that, the, sub, is, now, also, polit, ,, like, the, rest, of, reddit, ?]   \n",
       "22200                                                                                                                     [cool, ,, but, now, we, are, go, to, forc, plant, into, labor, too, ?]   \n",
       "40440                             [now, ,, how, do, you, explain, thi, to, a, con, ##ser, ##vat, ive, without, them, punch, you, and, abandon, scienc, even, more, than, ', all, the, way, ', ?]   \n",
       "62878                                                                                                                            [how, did, you, guy, end, up, in, your, respect, ive, field, ?]   \n",
       "\n",
       "                                                                                                                                                                           question_tokens_clean  \\\n",
       "7861                                                                                                        [i, mean, ,, doe, someon, who, is, lone, want, to, l, ive, a, longer, lone, life, ?]   \n",
       "2523                                                                                                                      [wait, .., ., be, nice, to, peopl, make, them, feel, more, comfort, ?]   \n",
       "73267  [is, 28, addit, death, over, a, week, ', s, worth, of, accid, in, the, whole, nation, statist, alli, sign, ##if, ##i, cant, to, confirm, that, it, is, relat, to, the, ds, ##t, chang, ?]   \n",
       "69994                                                                                                [so, how, mani, more, do, we, need, to, open, to, complet, elimin, matern, mortal, rate, ?]   \n",
       "51271                                                                                                                              [soo, .., how, is, the, pay, for, pharma, ci, ##t, in, nz, ?]   \n",
       "...                                                                                                                                                                                          ...   \n",
       "47185                                                                                                        [i, thought, they, prove, that, alzheim, had, to, do, with, a, viru, like, herp, ?]   \n",
       "50413                                                                                  [whi, not, just, own, the, fact, that, the, sub, is, now, also, polit, ,, like, the, rest, of, reddit, ?]   \n",
       "22200                                                                                                                     [cool, ,, but, now, we, are, go, to, forc, plant, into, labor, too, ?]   \n",
       "40440                             [now, ,, how, do, you, explain, thi, to, a, con, ##ser, ##vat, ive, without, them, punch, you, and, abandon, scienc, even, more, than, ', all, the, way, ', ?]   \n",
       "62878                                                                                                                            [how, did, you, guy, end, up, in, your, respect, ive, field, ?]   \n",
       "\n",
       "                                title_question_overlap_clean  \n",
       "7861   [life, ive, lonely, lonely, l, ive, ive, ive, lonely]  \n",
       "2523          [making, feel, comfortable, feel, comfortable]  \n",
       "73267                    [accidents, 28, additional, deaths]  \n",
       "69994                           [maternal, mortality, rates]  \n",
       "51271                                                     []  \n",
       "...                                                      ...  \n",
       "47185                                            [alzheimer]  \n",
       "50413                                                     []  \n",
       "22200                                        [plants, plant]  \n",
       "40440                    [con, ive, con, ive, ive, con, ive]  \n",
       "62878                                                     []  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(title_keyword_data.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload etc\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "list_cols = ['title_tokens', 'title_tokens_clean', 'question_tokens', 'question_tokens_clean', 'title_question_overlap_clean']\n",
    "title_keyword_data = pd.read_csv('science_submission_question_keyword_data.gz', sep='\\t', compression='gzip',\n",
    "                                 converters={c : literal_eval for c in list_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many keywords per question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "keyword_counts = title_keyword_data.loc[:, 'title_question_overlap_clean'].apply(len).values\n",
    "plt.hist(keyword_counts, bins=keyword_counts.max())\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half the questions have 0 keywords in the question, but we have a long tail of high-overlap questions. These questions could be copying text verbatim but TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into training/val\n",
    "from importlib import reload\n",
    "import demo_keyword_extractor\n",
    "reload(demo_keyword_extractor)\n",
    "from demo_keyword_extractor import build_dataset\n",
    "from transformers import DistilBertTokenizer\n",
    "train_pct = 0.8\n",
    "val_pct = 0.1\n",
    "N_data = title_keyword_data.shape[0]\n",
    "title_keyword_data = title_keyword_data.sample(N_data, replace=False, random_state=123)\n",
    "train_data = title_keyword_data[:int(N_data*train_pct)]\n",
    "val_data = title_keyword_data[int(N_data*train_pct):int(N_data*(train_pct+val_pct))]\n",
    "test_data = title_keyword_data[int(N_data*(train_pct+val_pct)):]\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "max_length = 512\n",
    "train_dataset = build_dataset(tokenizer, \n",
    "                              train_data.loc[:, 'title'].values.tolist(), \n",
    "                              train_data.loc[:, 'title_question_overlap_clean'].values.tolist(),\n",
    "                              max_length=max_length,\n",
    "                             )\n",
    "val_dataset = build_dataset(tokenizer, \n",
    "                            val_data.loc[:, 'title'].values.tolist(), \n",
    "                            val_data.loc[:, 'title_question_overlap_clean'].values.tolist(),\n",
    "                            max_length=max_length,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62486\n",
      "7811\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de903b118343bcbd64614d5eff77bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c25c8743cc4fefb01bdd6e5f5d3f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from demo_keyword_extractor import KeywordLossTrainer\n",
    "from importlib import reload\n",
    "import transformers\n",
    "reload(transformers)\n",
    "from transformers import DistilBertForTokenClassification\n",
    "model_name = 'distilbert-base-uncased'\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_name, cache_dir='../../data/model_cache/')\n",
    "training_args = TrainingArguments(output_dir='runs/', num_train_epochs=5, save_total_limit=1)\n",
    "trainer = KeywordLossTrainer(model=model, train_dataset=train_dataset, eval_dataset=val_dataset, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 62486\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39055' max='39055' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39055/39055 1:59:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-500\n",
      "Configuration saved in runs/checkpoint-500/config.json\n",
      "Model weights saved in runs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to runs/checkpoint-1000\n",
      "Configuration saved in runs/checkpoint-1000/config.json\n",
      "Model weights saved in runs/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-1500\n",
      "Configuration saved in runs/checkpoint-1500/config.json\n",
      "Model weights saved in runs/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-2000\n",
      "Configuration saved in runs/checkpoint-2000/config.json\n",
      "Model weights saved in runs/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-2500\n",
      "Configuration saved in runs/checkpoint-2500/config.json\n",
      "Model weights saved in runs/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-3000\n",
      "Configuration saved in runs/checkpoint-3000/config.json\n",
      "Model weights saved in runs/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-3500\n",
      "Configuration saved in runs/checkpoint-3500/config.json\n",
      "Model weights saved in runs/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-4000\n",
      "Configuration saved in runs/checkpoint-4000/config.json\n",
      "Model weights saved in runs/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-6000\n",
      "Configuration saved in runs/checkpoint-6000/config.json\n",
      "Model weights saved in runs/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-6500\n",
      "Configuration saved in runs/checkpoint-6500/config.json\n",
      "Model weights saved in runs/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-7000\n",
      "Configuration saved in runs/checkpoint-7000/config.json\n",
      "Model weights saved in runs/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-7500\n",
      "Configuration saved in runs/checkpoint-7500/config.json\n",
      "Model weights saved in runs/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-8000\n",
      "Configuration saved in runs/checkpoint-8000/config.json\n",
      "Model weights saved in runs/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-8500\n",
      "Configuration saved in runs/checkpoint-8500/config.json\n",
      "Model weights saved in runs/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-9000\n",
      "Configuration saved in runs/checkpoint-9000/config.json\n",
      "Model weights saved in runs/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-9500\n",
      "Configuration saved in runs/checkpoint-9500/config.json\n",
      "Model weights saved in runs/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-10000\n",
      "Configuration saved in runs/checkpoint-10000/config.json\n",
      "Model weights saved in runs/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-10500\n",
      "Configuration saved in runs/checkpoint-10500/config.json\n",
      "Model weights saved in runs/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-11000\n",
      "Configuration saved in runs/checkpoint-11000/config.json\n",
      "Model weights saved in runs/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-11500\n",
      "Configuration saved in runs/checkpoint-11500/config.json\n",
      "Model weights saved in runs/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-12000\n",
      "Configuration saved in runs/checkpoint-12000/config.json\n",
      "Model weights saved in runs/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-12500\n",
      "Configuration saved in runs/checkpoint-12500/config.json\n",
      "Model weights saved in runs/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-13000\n",
      "Configuration saved in runs/checkpoint-13000/config.json\n",
      "Model weights saved in runs/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-13500\n",
      "Configuration saved in runs/checkpoint-13500/config.json\n",
      "Model weights saved in runs/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-14000\n",
      "Configuration saved in runs/checkpoint-14000/config.json\n",
      "Model weights saved in runs/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-14500\n",
      "Configuration saved in runs/checkpoint-14500/config.json\n",
      "Model weights saved in runs/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-15000\n",
      "Configuration saved in runs/checkpoint-15000/config.json\n",
      "Model weights saved in runs/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-15500\n",
      "Configuration saved in runs/checkpoint-15500/config.json\n",
      "Model weights saved in runs/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-16000\n",
      "Configuration saved in runs/checkpoint-16000/config.json\n",
      "Model weights saved in runs/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-16500\n",
      "Configuration saved in runs/checkpoint-16500/config.json\n",
      "Model weights saved in runs/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-17000\n",
      "Configuration saved in runs/checkpoint-17000/config.json\n",
      "Model weights saved in runs/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-17500\n",
      "Configuration saved in runs/checkpoint-17500/config.json\n",
      "Model weights saved in runs/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-18000\n",
      "Configuration saved in runs/checkpoint-18000/config.json\n",
      "Model weights saved in runs/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-18500\n",
      "Configuration saved in runs/checkpoint-18500/config.json\n",
      "Model weights saved in runs/checkpoint-18500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [runs/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-19000\n",
      "Configuration saved in runs/checkpoint-19000/config.json\n",
      "Model weights saved in runs/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-19500\n",
      "Configuration saved in runs/checkpoint-19500/config.json\n",
      "Model weights saved in runs/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-20000\n",
      "Configuration saved in runs/checkpoint-20000/config.json\n",
      "Model weights saved in runs/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-20500\n",
      "Configuration saved in runs/checkpoint-20500/config.json\n",
      "Model weights saved in runs/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-21000\n",
      "Configuration saved in runs/checkpoint-21000/config.json\n",
      "Model weights saved in runs/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-21500\n",
      "Configuration saved in runs/checkpoint-21500/config.json\n",
      "Model weights saved in runs/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-22000\n",
      "Configuration saved in runs/checkpoint-22000/config.json\n",
      "Model weights saved in runs/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-22500\n",
      "Configuration saved in runs/checkpoint-22500/config.json\n",
      "Model weights saved in runs/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-23000\n",
      "Configuration saved in runs/checkpoint-23000/config.json\n",
      "Model weights saved in runs/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-23500\n",
      "Configuration saved in runs/checkpoint-23500/config.json\n",
      "Model weights saved in runs/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-24000\n",
      "Configuration saved in runs/checkpoint-24000/config.json\n",
      "Model weights saved in runs/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-24500\n",
      "Configuration saved in runs/checkpoint-24500/config.json\n",
      "Model weights saved in runs/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-25000\n",
      "Configuration saved in runs/checkpoint-25000/config.json\n",
      "Model weights saved in runs/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-25500\n",
      "Configuration saved in runs/checkpoint-25500/config.json\n",
      "Model weights saved in runs/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-26000\n",
      "Configuration saved in runs/checkpoint-26000/config.json\n",
      "Model weights saved in runs/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-26500\n",
      "Configuration saved in runs/checkpoint-26500/config.json\n",
      "Model weights saved in runs/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-27000\n",
      "Configuration saved in runs/checkpoint-27000/config.json\n",
      "Model weights saved in runs/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-27500\n",
      "Configuration saved in runs/checkpoint-27500/config.json\n",
      "Model weights saved in runs/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-28000\n",
      "Configuration saved in runs/checkpoint-28000/config.json\n",
      "Model weights saved in runs/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-28500\n",
      "Configuration saved in runs/checkpoint-28500/config.json\n",
      "Model weights saved in runs/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-29000\n",
      "Configuration saved in runs/checkpoint-29000/config.json\n",
      "Model weights saved in runs/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-29500\n",
      "Configuration saved in runs/checkpoint-29500/config.json\n",
      "Model weights saved in runs/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-30000\n",
      "Configuration saved in runs/checkpoint-30000/config.json\n",
      "Model weights saved in runs/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-30500\n",
      "Configuration saved in runs/checkpoint-30500/config.json\n",
      "Model weights saved in runs/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-31000\n",
      "Configuration saved in runs/checkpoint-31000/config.json\n",
      "Model weights saved in runs/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-31500\n",
      "Configuration saved in runs/checkpoint-31500/config.json\n",
      "Model weights saved in runs/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-32000\n",
      "Configuration saved in runs/checkpoint-32000/config.json\n",
      "Model weights saved in runs/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-32500\n",
      "Configuration saved in runs/checkpoint-32500/config.json\n",
      "Model weights saved in runs/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-33000\n",
      "Configuration saved in runs/checkpoint-33000/config.json\n",
      "Model weights saved in runs/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-33500\n",
      "Configuration saved in runs/checkpoint-33500/config.json\n",
      "Model weights saved in runs/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-34000\n",
      "Configuration saved in runs/checkpoint-34000/config.json\n",
      "Model weights saved in runs/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-34500\n",
      "Configuration saved in runs/checkpoint-34500/config.json\n",
      "Model weights saved in runs/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-35000\n",
      "Configuration saved in runs/checkpoint-35000/config.json\n",
      "Model weights saved in runs/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-34500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-35500\n",
      "Configuration saved in runs/checkpoint-35500/config.json\n",
      "Model weights saved in runs/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-36000\n",
      "Configuration saved in runs/checkpoint-36000/config.json\n",
      "Model weights saved in runs/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-36500\n",
      "Configuration saved in runs/checkpoint-36500/config.json\n",
      "Model weights saved in runs/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-37000\n",
      "Configuration saved in runs/checkpoint-37000/config.json\n",
      "Model weights saved in runs/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-37500\n",
      "Configuration saved in runs/checkpoint-37500/config.json\n",
      "Model weights saved in runs/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-38000\n",
      "Configuration saved in runs/checkpoint-38000/config.json\n",
      "Model weights saved in runs/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-38500\n",
      "Configuration saved in runs/checkpoint-38500/config.json\n",
      "Model weights saved in runs/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to runs/checkpoint-39000\n",
      "Configuration saved in runs/checkpoint-39000/config.json\n",
      "Model weights saved in runs/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [runs/checkpoint-38500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## train!\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='977' max='977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [977/977 00:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.10143487900495529,\n",
       " 'eval_runtime': 58.5582,\n",
       " 'eval_samples_per_second': 133.389,\n",
       " 'eval_steps_per_second': 16.684,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this model do with the test data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file runs/checkpoint-39000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file runs/checkpoint-39000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at runs/checkpoint-39000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## reload model\n",
    "final_model = DistilBertForTokenClassification.from_pretrained('runs/checkpoint-39000/')\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_dataset(tokenizer, \n",
    "                            test_data.loc[:, 'title'].values.tolist(), \n",
    "                            test_data.loc[:, 'title_question_overlap_clean'].values.tolist(),\n",
    "                            max_length=max_length,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7811 [02:38<?, ?it/s] 7.99it/s]\n",
      "  0%|          | 0/7811 [01:12<?, ?it/s]\n",
      "100%|██████████| 7811/7811 [16:24<00:00,  7.93it/s]\n"
     ]
    }
   ],
   "source": [
    "## run prediction\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = [final_model(**{k : v.to(final_model.device).unsqueeze(1) for k,v in x.items()}) for x in tqdm(test_dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the overall performance score for the output data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import demo_keyword_extractor\n",
    "reload(demo_keyword_extractor)\n",
    "from demo_keyword_extractor import keyword_score\n",
    "def compute_metrics(p, null_ids={101}):\n",
    "    predictions, labels, inputs = p\n",
    "    # nan values don't matter because argmax converts to 0\n",
    "    # fix nan vals in prediction => set to 0\n",
    "#     nan_idx = np.where(torch.isnan(predictions).min(1)[0])\n",
    "#     predictions[torch.isnan(predictions)]\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    input_pred = [int(i) for i,o in zip(inputs, predictions) if o==1 and int(i) not in null_ids]\n",
    "    input_labels = [int(i) for i,l in zip(inputs, labels) if l==1]\n",
    "#     print(input_pred, input_labels)\n",
    "    prec, rec, F1 = keyword_score(input_pred, input_labels)\n",
    "    return {\n",
    "        'prec' : prec,\n",
    "        'rec' : rec,\n",
    "        'F1' : F1,\n",
    "        'pred_ids' : input_pred,\n",
    "        'label_ids' : input_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1502/7811 [00:20<01:26, 72.75it/s]"
     ]
    }
   ],
   "source": [
    "test_data_metrics = []\n",
    "for pred_i, test_data_i in tqdm(zip(test_output, test_dataset), total=len(test_dataset)):\n",
    "    test_data_metric = compute_metrics((pred_i.logits.squeeze(1), test_data_i['labels'], test_data_i['input_ids']))\n",
    "    test_data_metrics.append(test_data_metric)\n",
    "test_data_metrics = pd.DataFrame(test_data_metrics)\n",
    "test_data_metrics.index = test_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the overall metrics (all data, only data with at least 1 label)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec    0.032369\n",
      "rec     0.076026\n",
      "F1      0.054197\n",
      "dtype: float64\n",
      "N=4312 data with at least one label\n",
      "prec    0.058634\n",
      "rec     0.137717\n",
      "F1      0.098176\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "metric_cols = ['prec', 'rec', 'F1']\n",
    "print(test_data_metrics.loc[:, metric_cols].mean(axis=0))\n",
    "test_data_with_labels_metrics = test_data_metrics[test_data_metrics.loc[:, 'label_ids'].apply(lambda x: len(x) > 0)]\n",
    "print(f'N={test_data_with_labels_metrics.shape[0]} data with at least one label')\n",
    "print(test_data_with_labels_metrics.loc[:, metric_cols].mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does slightly better when predicting on posts with at least one label.\n",
    "\n",
    "Let's look at a few of those as examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_submission</th>\n",
       "      <th>id_comment</th>\n",
       "      <th>title</th>\n",
       "      <th>reply_question</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_tokens_clean</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>question_tokens_clean</th>\n",
       "      <th>title_question_overlap_clean</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>F1</th>\n",
       "      <th>pred_ids</th>\n",
       "      <th>label_ids</th>\n",
       "      <th>pred_tokens</th>\n",
       "      <th>label_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>gehvui</td>\n",
       "      <td>fpon6na</td>\n",
       "      <td>Why do viruses often come from bats? A discussion with your friendly neighborhood virologist</td>\n",
       "      <td>Is there even solid evidence that fever actually does anything?</td>\n",
       "      <td>[why, do, viruses, often, come, from, bats, ?, a, discussion, with, your, friendly, neighborhood, vi, ##rol, ##ogist]</td>\n",
       "      <td>[whi, do, virus, often, come, from, bat, ?, a, discuss, with, your, friendli, neighborhood, vi, ##rol, ##ogist]</td>\n",
       "      <td>[is, there, even, solid, evid, that, fever, actual, doe, anyth, ?]</td>\n",
       "      <td>[is, there, even, solid, evid, that, fever, actual, doe, anyth, ?]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[1029, 2007, 6819]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[?, with, vi]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9007</th>\n",
       "      <td>ijkedb</td>\n",
       "      <td>g3g4f0x</td>\n",
       "      <td>For the first time, scientists just replicated pressures found on white dwarf stars in a lab on earth.</td>\n",
       "      <td>Will it be possible to create neutronium or quarks or a small black hole?</td>\n",
       "      <td>[for, the, first, time, ,, scientists, just, replicate, ##d, pressures, found, on, white, dwarf, stars, in, a, lab, on, earth, .]</td>\n",
       "      <td>[for, the, first, time, ,, scientist, just, replic, ##d, pressur, found, on, white, dwarf, star, in, a, lab, on, earth, .]</td>\n",
       "      <td>[will, it, be, possibl, to, creat, neutron, ##ium, or, qu, ##ark, ##, or, a, small, black, hole, ?]</td>\n",
       "      <td>[will, it, be, possibl, to, creat, neutron, ##ium, or, qu, ##ark, ##, or, a, small, black, hole, ?]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[2034, 2051, 2006, 2006]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[first, time, on, on]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71656</th>\n",
       "      <td>jm1vii</td>\n",
       "      <td>gatd830</td>\n",
       "      <td>Researchers have showed for the first time that the body produces more stress hormones when people are repeatedly interrupted at work. Should this stress become chronic, it can lead to states of e...</td>\n",
       "      <td>Should this stress becomes chronic, it  can lead to states of exhaustion that have a negative impact on public  health and carry a significant economic cost\"?</td>\n",
       "      <td>[researchers, have, showed, for, the, first, time, that, the, body, produces, more, stress, hormones, when, people, are, repeatedly, interrupted, at, work, ., should, this, stress, become, chronic...</td>\n",
       "      <td>[research, have, show, for, the, first, time, that, the, bodi, produc, more, stress, hormon, when, peopl, are, repeatedli, interrupt, at, work, ., should, thi, stress, becom, chronic, ,, it, can, ...</td>\n",
       "      <td>[should, thi, stress, becom, chronic, ,, it, can, lead, to, state, of, exhaust, that, have, a, n, eg, at, ive, impact, on, public, health, and, carri, a, sign, ##if, ##i, cant, econom, cost, \", ?]</td>\n",
       "      <td>[should, thi, stress, becom, chronic, ,, it, can, lead, to, state, of, exhaust, that, have, a, n, eg, at, ive, impact, on, public, health, and, carri, a, sign, ##if, ##i, cant, econom, cost, \", ?]</td>\n",
       "      <td>[stress, stress, become, chronic, lead, states, exhaustion, n, eg, ive, impact, public, health, carry, sign, cant, economic, cost]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>[2034, 2051, 2323, 15575, 2006]</td>\n",
       "      <td>[6911, 6911, 2468, 11888, 2599, 2163, 15575, 4254, 2270, 2740, 4287, 3171, 3465]</td>\n",
       "      <td>[first, time, should, exhaustion, on]</td>\n",
       "      <td>[stress, stress, become, chronic, lead, states, exhaustion, impact, public, health, carry, economic, cost]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25274</th>\n",
       "      <td>kklv00</td>\n",
       "      <td>gh5vqm3</td>\n",
       "      <td>Clearing land to feed a growing human population will threaten thousands of species</td>\n",
       "      <td>How economically viable is aquaponics, vertical farming, and   vegetables grown in artificial environments (with artificial lights)?</td>\n",
       "      <td>[clearing, land, to, feed, a, growing, human, population, will, threaten, thousands, of, species]</td>\n",
       "      <td>[clear, land, to, feed, a, grow, human, popul, will, threaten, thousand, of, speci]</td>\n",
       "      <td>[how, econom, viabl, is, aqua, ##pon, ##ic, ,, vertic, farm, ,, and, v, eg, eta, ##ble, grown, in, artifici, environ, (, with, artifici, light, ), ?]</td>\n",
       "      <td>[how, econom, viabl, is, aqua, ##pon, ##ic, ,, vertic, farm, ,, and, v, eg, eta, ##ble, grown, in, artifici, environ, (, with, artifici, light, ), ?]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[2455, 2097]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[land, will]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26637</th>\n",
       "      <td>ibicdy</td>\n",
       "      <td>g1xu6y4</td>\n",
       "      <td>Scientists found the secret behind the unpleasant smell of BO: an enzyme called C-T lyase, found in bacteria that live in armpits. They feed on sweat &amp;amp; convert it smelly thioalcohols. C-T lyas...</td>\n",
       "      <td>If this is gene-related, and I’ve seen that certain segments of populations in Asia don’t have this gene, so their sweat doesn’t stink, could it be possible to make one’s sweat not smell using gen...</td>\n",
       "      <td>[scientists, found, the, secret, behind, the, unpleasant, smell, of, bo, :, an, enzyme, called, c, -, t, l, ##yas, ##e, ,, found, in, bacteria, that, l, ive, in, arm, ##pit, ##s, ., they, feed, on...</td>\n",
       "      <td>[scientist, found, the, secret, behind, the, unpleas, smell, of, bo, :, an, enzym, call, c, -, t, l, ##ya, ##e, ,, found, in, bacteria, that, l, ive, in, arm, ##pit, ##, ., they, feed, on, sweat, ...</td>\n",
       "      <td>[if, thi, is, gene, -, relat, ,, and, i, ’, ve, seen, that, certain, s, eg, men, ##t, of, popul, in, asia, don, ’, t, have, thi, gene, ,, so, their, sweat, doesn, ’, t, stink, ,, could, it, be, po...</td>\n",
       "      <td>[if, thi, is, gene, -, relat, ,, and, i, ’, ve, seen, that, certain, s, eg, men, ##t, of, popul, in, asia, don, ’, t, have, thi, gene, ,, so, their, sweat, doesn, ’, t, stink, ,, could, it, be, po...</td>\n",
       "      <td>[smell, bacteria, sweat, smell, gene, smell, bacteria, stink]</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>[2369, 16010, 5437, 1039, 1011, 16303, 10327, 2006, 23713, 10463, 5437, 3597, 1039, 1011, 16303, 2011, 1011, 5437, 10327, 27136]</td>\n",
       "      <td>[5437, 10327, 7518, 5437, 4962, 5437, 10327, 27136]</td>\n",
       "      <td>[behind, unpleasant, smell, c, -, ##yas, bacteria, on, amp, convert, smell, ##co, c, -, ##yas, by, -, smell, bacteria, stink]</td>\n",
       "      <td>[smell, bacteria, sweat, smell, gene, smell, bacteria, stink]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_submission id_comment  \\\n",
       "17989        gehvui    fpon6na   \n",
       "9007         ijkedb    g3g4f0x   \n",
       "71656        jm1vii    gatd830   \n",
       "25274        kklv00    gh5vqm3   \n",
       "26637        ibicdy    g1xu6y4   \n",
       "\n",
       "                                                                                                                                                                                                         title  \\\n",
       "17989                                                                                                             Why do viruses often come from bats? A discussion with your friendly neighborhood virologist   \n",
       "9007                                                                                                    For the first time, scientists just replicated pressures found on white dwarf stars in a lab on earth.   \n",
       "71656  Researchers have showed for the first time that the body produces more stress hormones when people are repeatedly interrupted at work. Should this stress become chronic, it can lead to states of e...   \n",
       "25274                                                                                                                      Clearing land to feed a growing human population will threaten thousands of species   \n",
       "26637  Scientists found the secret behind the unpleasant smell of BO: an enzyme called C-T lyase, found in bacteria that live in armpits. They feed on sweat &amp; convert it smelly thioalcohols. C-T lyas...   \n",
       "\n",
       "                                                                                                                                                                                                reply_question  \\\n",
       "17989                                                                                                                                          Is there even solid evidence that fever actually does anything?   \n",
       "9007                                                                                                                                 Will it be possible to create neutronium or quarks or a small black hole?   \n",
       "71656                                           Should this stress becomes chronic, it  can lead to states of exhaustion that have a negative impact on public  health and carry a significant economic cost\"?   \n",
       "25274                                                                     How economically viable is aquaponics, vertical farming, and   vegetables grown in artificial environments (with artificial lights)?   \n",
       "26637  If this is gene-related, and I’ve seen that certain segments of populations in Asia don’t have this gene, so their sweat doesn’t stink, could it be possible to make one’s sweat not smell using gen...   \n",
       "\n",
       "                                                                                                                                                                                                  title_tokens  \\\n",
       "17989                                                                                    [why, do, viruses, often, come, from, bats, ?, a, discussion, with, your, friendly, neighborhood, vi, ##rol, ##ogist]   \n",
       "9007                                                                         [for, the, first, time, ,, scientists, just, replicate, ##d, pressures, found, on, white, dwarf, stars, in, a, lab, on, earth, .]   \n",
       "71656  [researchers, have, showed, for, the, first, time, that, the, body, produces, more, stress, hormones, when, people, are, repeatedly, interrupted, at, work, ., should, this, stress, become, chronic...   \n",
       "25274                                                                                                        [clearing, land, to, feed, a, growing, human, population, will, threaten, thousands, of, species]   \n",
       "26637  [scientists, found, the, secret, behind, the, unpleasant, smell, of, bo, :, an, enzyme, called, c, -, t, l, ##yas, ##e, ,, found, in, bacteria, that, l, ive, in, arm, ##pit, ##s, ., they, feed, on...   \n",
       "\n",
       "                                                                                                                                                                                            title_tokens_clean  \\\n",
       "17989                                                                                          [whi, do, virus, often, come, from, bat, ?, a, discuss, with, your, friendli, neighborhood, vi, ##rol, ##ogist]   \n",
       "9007                                                                                [for, the, first, time, ,, scientist, just, replic, ##d, pressur, found, on, white, dwarf, star, in, a, lab, on, earth, .]   \n",
       "71656  [research, have, show, for, the, first, time, that, the, bodi, produc, more, stress, hormon, when, peopl, are, repeatedli, interrupt, at, work, ., should, thi, stress, becom, chronic, ,, it, can, ...   \n",
       "25274                                                                                                                      [clear, land, to, feed, a, grow, human, popul, will, threaten, thousand, of, speci]   \n",
       "26637  [scientist, found, the, secret, behind, the, unpleas, smell, of, bo, :, an, enzym, call, c, -, t, l, ##ya, ##e, ,, found, in, bacteria, that, l, ive, in, arm, ##pit, ##, ., they, feed, on, sweat, ...   \n",
       "\n",
       "                                                                                                                                                                                               question_tokens  \\\n",
       "17989                                                                                                                                       [is, there, even, solid, evid, that, fever, actual, doe, anyth, ?]   \n",
       "9007                                                                                                       [will, it, be, possibl, to, creat, neutron, ##ium, or, qu, ##ark, ##, or, a, small, black, hole, ?]   \n",
       "71656     [should, thi, stress, becom, chronic, ,, it, can, lead, to, state, of, exhaust, that, have, a, n, eg, at, ive, impact, on, public, health, and, carri, a, sign, ##if, ##i, cant, econom, cost, \", ?]   \n",
       "25274                                                    [how, econom, viabl, is, aqua, ##pon, ##ic, ,, vertic, farm, ,, and, v, eg, eta, ##ble, grown, in, artifici, environ, (, with, artifici, light, ), ?]   \n",
       "26637  [if, thi, is, gene, -, relat, ,, and, i, ’, ve, seen, that, certain, s, eg, men, ##t, of, popul, in, asia, don, ’, t, have, thi, gene, ,, so, their, sweat, doesn, ’, t, stink, ,, could, it, be, po...   \n",
       "\n",
       "                                                                                                                                                                                         question_tokens_clean  \\\n",
       "17989                                                                                                                                       [is, there, even, solid, evid, that, fever, actual, doe, anyth, ?]   \n",
       "9007                                                                                                       [will, it, be, possibl, to, creat, neutron, ##ium, or, qu, ##ark, ##, or, a, small, black, hole, ?]   \n",
       "71656     [should, thi, stress, becom, chronic, ,, it, can, lead, to, state, of, exhaust, that, have, a, n, eg, at, ive, impact, on, public, health, and, carri, a, sign, ##if, ##i, cant, econom, cost, \", ?]   \n",
       "25274                                                    [how, econom, viabl, is, aqua, ##pon, ##ic, ,, vertic, farm, ,, and, v, eg, eta, ##ble, grown, in, artifici, environ, (, with, artifici, light, ), ?]   \n",
       "26637  [if, thi, is, gene, -, relat, ,, and, i, ’, ve, seen, that, certain, s, eg, men, ##t, of, popul, in, asia, don, ’, t, have, thi, gene, ,, so, their, sweat, doesn, ’, t, stink, ,, could, it, be, po...   \n",
       "\n",
       "                                                                                                             title_question_overlap_clean  \\\n",
       "17989                                                                                                                                  []   \n",
       "9007                                                                                                                                   []   \n",
       "71656  [stress, stress, become, chronic, lead, states, exhaustion, n, eg, ive, impact, public, health, carry, sign, cant, economic, cost]   \n",
       "25274                                                                                                                                  []   \n",
       "26637                                                                       [smell, bacteria, sweat, smell, gene, smell, bacteria, stink]   \n",
       "\n",
       "           prec       rec        F1  \\\n",
       "17989  0.000000  0.000000  0.000000   \n",
       "9007   0.000000  0.000000  0.000000   \n",
       "71656  0.200000  0.083333  0.141667   \n",
       "25274  0.000000  0.000000  0.000000   \n",
       "26637  0.230769  0.600000  0.415385   \n",
       "\n",
       "                                                                                                                               pred_ids  \\\n",
       "17989                                                                                                                [1029, 2007, 6819]   \n",
       "9007                                                                                                           [2034, 2051, 2006, 2006]   \n",
       "71656                                                                                                   [2034, 2051, 2323, 15575, 2006]   \n",
       "25274                                                                                                                      [2455, 2097]   \n",
       "26637  [2369, 16010, 5437, 1039, 1011, 16303, 10327, 2006, 23713, 10463, 5437, 3597, 1039, 1011, 16303, 2011, 1011, 5437, 10327, 27136]   \n",
       "\n",
       "                                                                              label_ids  \\\n",
       "17989                                                                                []   \n",
       "9007                                                                                 []   \n",
       "71656  [6911, 6911, 2468, 11888, 2599, 2163, 15575, 4254, 2270, 2740, 4287, 3171, 3465]   \n",
       "25274                                                                                []   \n",
       "26637                               [5437, 10327, 7518, 5437, 4962, 5437, 10327, 27136]   \n",
       "\n",
       "                                                                                                                         pred_tokens  \\\n",
       "17989                                                                                                                  [?, with, vi]   \n",
       "9007                                                                                                           [first, time, on, on]   \n",
       "71656                                                                                          [first, time, should, exhaustion, on]   \n",
       "25274                                                                                                                   [land, will]   \n",
       "26637  [behind, unpleasant, smell, c, -, ##yas, bacteria, on, amp, convert, smell, ##co, c, -, ##yas, by, -, smell, bacteria, stink]   \n",
       "\n",
       "                                                                                                     label_tokens  \n",
       "17989                                                                                                          []  \n",
       "9007                                                                                                           []  \n",
       "71656  [stress, stress, become, chronic, lead, states, exhaustion, impact, public, health, carry, economic, cost]  \n",
       "25274                                                                                                          []  \n",
       "26637                                               [smell, bacteria, sweat, smell, gene, smell, bacteria, stink]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## recombine with original data\n",
    "test_pred_data = pd.concat([\n",
    "    test_data,\n",
    "    test_data_metrics.loc[:, ['prec', 'rec', 'F1', 'pred_ids', 'label_ids']],\n",
    "], axis=1)\n",
    "null_ids = {101}\n",
    "test_pred_data = test_pred_data.assign(**{\n",
    "    'pred_tokens' : test_pred_data.loc[:, 'pred_ids'].apply(lambda x: tokenizer.convert_ids_to_tokens(list(filter(lambda y: y not in null_ids, x)))),\n",
    "    'label_tokens' : test_pred_data.loc[:, 'label_ids'].apply(lambda x: tokenizer.convert_ids_to_tokens(list(filter(lambda y: y not in null_ids, x)))),\n",
    "})\n",
    "display(test_pred_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct preds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>reply_question</th>\n",
       "      <th>pred_tokens</th>\n",
       "      <th>label_tokens</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69312</th>\n",
       "      <td>Blue jeans are a significant source of microfiber pollution in oceans and lakes. One pair of jeans can release over 50,000 microfibers per wash.</td>\n",
       "      <td>If people don't care what's on their plate, why should they care about what's far away in some lake?</td>\n",
       "      <td>[lakes]</td>\n",
       "      <td>[lakes]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72209</th>\n",
       "      <td>New dinosaur species found in Australia was as long as a basketball court</td>\n",
       "      <td>A lot of dinsaurs were that long, why is that the distinguishing fact?</td>\n",
       "      <td>[long]</td>\n",
       "      <td>[long]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60037</th>\n",
       "      <td>Chest beats as an honest signal of body size in male mountain gorillas</td>\n",
       "      <td>Could gorillas not just evolve to larger chests without being better fighters and otherwise better mates?</td>\n",
       "      <td>[chest, gorilla]</td>\n",
       "      <td>[chest, gorilla]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71172</th>\n",
       "      <td>Pets More Effective for Grief Support than Humans, Study Finds</td>\n",
       "      <td>What is effective for grieving the lost of a pet?</td>\n",
       "      <td>[pets]</td>\n",
       "      <td>[pets]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34788</th>\n",
       "      <td>Fatal school shootings linked to increased antidepressant use among youths</td>\n",
       "      <td>Is this the same study group that found divorce in America rose after the growth of margarine use in the USA , just askin?</td>\n",
       "      <td>[use]</td>\n",
       "      <td>[use]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20414</th>\n",
       "      <td>People’s attachment to the wilderness is linked to the fulfillment of basic psychological needs, study finds</td>\n",
       "      <td>Did a woods support your needs for food, clothing and shelter?</td>\n",
       "      <td>[needs]</td>\n",
       "      <td>[needs]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34715</th>\n",
       "      <td>Electric catfish cannot be shocked and scientists don’t know why</td>\n",
       "      <td>Could be dumb or mentioned already, but you think the defense is in place so they don’t shock their own accidentally?</td>\n",
       "      <td>[shocked]</td>\n",
       "      <td>[shocked]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55606</th>\n",
       "      <td>New battery electrolyte developed at Stanford may boost the performance of electric vehicles</td>\n",
       "      <td>So what is the reason this tech won’t just follow similar developments and never reach mass production?</td>\n",
       "      <td>[developed]</td>\n",
       "      <td>[developed]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17385</th>\n",
       "      <td>Humans judge faces in incomplete photographs as physically more attractive</td>\n",
       "      <td>So would hiding half my face behind long hair make me seem more attractive then?</td>\n",
       "      <td>[faces]</td>\n",
       "      <td>[faces]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67695</th>\n",
       "      <td>Vikings not typically blonde according to Genomes</td>\n",
       "      <td>What's the deal with the anti-white-washing of the vikings?</td>\n",
       "      <td>[vikings]</td>\n",
       "      <td>[vikings]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63373</th>\n",
       "      <td>New type of atomic clock keeps time even more precisely</td>\n",
       "      <td>At some point, don’t difficult-to-account-for relativistic corrections in time-keeping of relevant periods overtake the internal error of the clock?</td>\n",
       "      <td>[keeps, time]</td>\n",
       "      <td>[clock, keeps, time]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73404</th>\n",
       "      <td>The Earth has lost a quarter of its water</td>\n",
       "      <td>So the Earth was 70% water and lost 25% of that?</td>\n",
       "      <td>[lost, water]</td>\n",
       "      <td>[earth, lost, water]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76368</th>\n",
       "      <td>Thirdhand Smoke: Study Finds Smokers’ Clothes are Carrying Cigarette Chemicals Indoors</td>\n",
       "      <td>We needed to be told that when your clothes smell like cigarettes, it’s because there’s chemicals on you from the cigarette?</td>\n",
       "      <td>[clothes, chemicals]</td>\n",
       "      <td>[clothes, cigarette, chemicals]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>U.S. adults look to scientific organizations like the Centers for Disease Control and Prevention, rather than their president, to lead the country’s response to the coronavirus pandemic</td>\n",
       "      <td>And is there a way we can do something like that, by injection, inside, or, or, almost a cleaning?</td>\n",
       "      <td>[like, rather]</td>\n",
       "      <td>[like]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41148</th>\n",
       "      <td>Taking MDMA could help to treat alcoholism</td>\n",
       "      <td>Anybody have any idea on taking mdma and how it interacts with the liver?</td>\n",
       "      <td>[taking, help]</td>\n",
       "      <td>[taking]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43115</th>\n",
       "      <td>Study Confirms Climate Models are Getting Future Warming Projections Right</td>\n",
       "      <td>So a study confirmed that other studies are correct?</td>\n",
       "      <td>[confirms]</td>\n",
       "      <td>[study, confirms]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28700</th>\n",
       "      <td>Increase in immigration has little impact on the wages of US citizens</td>\n",
       "      <td>As for paperwork- maybe just get rid of paperwork and allow anyone come to US or in any other country?</td>\n",
       "      <td>[on, us]</td>\n",
       "      <td>[us]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>People’s attachment to the wilderness is linked to the fulfillment of basic psychological needs, study finds</td>\n",
       "      <td>What does it mean for people who don’t really feel the need or want to be surrounded by nature ?</td>\n",
       "      <td>[needs]</td>\n",
       "      <td>[people, needs]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35276</th>\n",
       "      <td>China Officially Recommends Vitamin C as Coronavirus Treatment</td>\n",
       "      <td>I thought vitamin C was proven to do nothing to our immune system and then Airborne got sued for it?</td>\n",
       "      <td>[recommends, c]</td>\n",
       "      <td>[c]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>A human monoclonal antibody blocking SARS-CoV-2 infection</td>\n",
       "      <td>A follow up question to this: Do we have any information on just how much of these antibodies would be required to be administered to have an actual impact?</td>\n",
       "      <td>[antibody, -, -]</td>\n",
       "      <td>[antibody]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           title  \\\n",
       "69312                                           Blue jeans are a significant source of microfiber pollution in oceans and lakes. One pair of jeans can release over 50,000 microfibers per wash.   \n",
       "72209                                                                                                                  New dinosaur species found in Australia was as long as a basketball court   \n",
       "60037                                                                                                                     Chest beats as an honest signal of body size in male mountain gorillas   \n",
       "71172                                                                                                                             Pets More Effective for Grief Support than Humans, Study Finds   \n",
       "34788                                                                                                                 Fatal school shootings linked to increased antidepressant use among youths   \n",
       "20414                                                                               People’s attachment to the wilderness is linked to the fulfillment of basic psychological needs, study finds   \n",
       "34715                                                                                                                           Electric catfish cannot be shocked and scientists don’t know why   \n",
       "55606                                                                                               New battery electrolyte developed at Stanford may boost the performance of electric vehicles   \n",
       "17385                                                                                                                 Humans judge faces in incomplete photographs as physically more attractive   \n",
       "67695                                                                                                                                          Vikings not typically blonde according to Genomes   \n",
       "63373                                                                                                                                    New type of atomic clock keeps time even more precisely   \n",
       "73404                                                                                                                                                  The Earth has lost a quarter of its water   \n",
       "76368                                                                                                     Thirdhand Smoke: Study Finds Smokers’ Clothes are Carrying Cigarette Chemicals Indoors   \n",
       "1155   U.S. adults look to scientific organizations like the Centers for Disease Control and Prevention, rather than their president, to lead the country’s response to the coronavirus pandemic   \n",
       "41148                                                                                                                                                 Taking MDMA could help to treat alcoholism   \n",
       "43115                                                                                                                 Study Confirms Climate Models are Getting Future Warming Projections Right   \n",
       "28700                                                                                                                      Increase in immigration has little impact on the wages of US citizens   \n",
       "8672                                                                                People’s attachment to the wilderness is linked to the fulfillment of basic psychological needs, study finds   \n",
       "35276                                                                                                                             China Officially Recommends Vitamin C as Coronavirus Treatment   \n",
       "5816                                                                                                                                   A human monoclonal antibody blocking SARS-CoV-2 infection   \n",
       "\n",
       "                                                                                                                                                     reply_question  \\\n",
       "69312                                                          If people don't care what's on their plate, why should they care about what's far away in some lake?   \n",
       "72209                                                                                        A lot of dinsaurs were that long, why is that the distinguishing fact?   \n",
       "60037                                                     Could gorillas not just evolve to larger chests without being better fighters and otherwise better mates?   \n",
       "71172                                                                                                             What is effective for grieving the lost of a pet?   \n",
       "34788                                    Is this the same study group that found divorce in America rose after the growth of margarine use in the USA , just askin?   \n",
       "20414                                                                                                Did a woods support your needs for food, clothing and shelter?   \n",
       "34715                                         Could be dumb or mentioned already, but you think the defense is in place so they don’t shock their own accidentally?   \n",
       "55606                                                       So what is the reason this tech won’t just follow similar developments and never reach mass production?   \n",
       "17385                                                                              So would hiding half my face behind long hair make me seem more attractive then?   \n",
       "67695                                                                                                   What's the deal with the anti-white-washing of the vikings?   \n",
       "63373          At some point, don’t difficult-to-account-for relativistic corrections in time-keeping of relevant periods overtake the internal error of the clock?   \n",
       "73404                                                                                                              So the Earth was 70% water and lost 25% of that?   \n",
       "76368                                  We needed to be told that when your clothes smell like cigarettes, it’s because there’s chemicals on you from the cigarette?   \n",
       "1155                                                             And is there a way we can do something like that, by injection, inside, or, or, almost a cleaning?   \n",
       "41148                                                                                     Anybody have any idea on taking mdma and how it interacts with the liver?   \n",
       "43115                                                                                                          So a study confirmed that other studies are correct?   \n",
       "28700                                                        As for paperwork- maybe just get rid of paperwork and allow anyone come to US or in any other country?   \n",
       "8672                                                               What does it mean for people who don’t really feel the need or want to be surrounded by nature ?   \n",
       "35276                                                          I thought vitamin C was proven to do nothing to our immune system and then Airborne got sued for it?   \n",
       "5816   A follow up question to this: Do we have any information on just how much of these antibodies would be required to be administered to have an actual impact?   \n",
       "\n",
       "                pred_tokens                     label_tokens  prec       rec  \\\n",
       "69312               [lakes]                          [lakes]   1.0  1.000000   \n",
       "72209                [long]                           [long]   1.0  1.000000   \n",
       "60037      [chest, gorilla]                 [chest, gorilla]   1.0  1.000000   \n",
       "71172                [pets]                           [pets]   1.0  1.000000   \n",
       "34788                 [use]                            [use]   1.0  1.000000   \n",
       "20414               [needs]                          [needs]   1.0  1.000000   \n",
       "34715             [shocked]                        [shocked]   1.0  1.000000   \n",
       "55606           [developed]                      [developed]   1.0  1.000000   \n",
       "17385               [faces]                          [faces]   1.0  1.000000   \n",
       "67695             [vikings]                        [vikings]   1.0  1.000000   \n",
       "63373         [keeps, time]             [clock, keeps, time]   1.0  0.666667   \n",
       "73404         [lost, water]             [earth, lost, water]   1.0  0.666667   \n",
       "76368  [clothes, chemicals]  [clothes, cigarette, chemicals]   1.0  0.666667   \n",
       "1155         [like, rather]                           [like]   0.5  1.000000   \n",
       "41148        [taking, help]                         [taking]   0.5  1.000000   \n",
       "43115            [confirms]                [study, confirms]   1.0  0.500000   \n",
       "28700              [on, us]                             [us]   0.5  1.000000   \n",
       "8672                [needs]                  [people, needs]   1.0  0.500000   \n",
       "35276       [recommends, c]                              [c]   0.5  1.000000   \n",
       "5816       [antibody, -, -]                       [antibody]   0.5  1.000000   \n",
       "\n",
       "             F1  \n",
       "69312  1.000000  \n",
       "72209  1.000000  \n",
       "60037  1.000000  \n",
       "71172  1.000000  \n",
       "34788  1.000000  \n",
       "20414  1.000000  \n",
       "34715  1.000000  \n",
       "55606  1.000000  \n",
       "17385  1.000000  \n",
       "67695  1.000000  \n",
       "63373  0.833333  \n",
       "73404  0.833333  \n",
       "76368  0.833333  \n",
       "1155   0.750000  \n",
       "41148  0.750000  \n",
       "43115  0.750000  \n",
       "28700  0.750000  \n",
       "8672   0.750000  \n",
       "35276  0.750000  \n",
       "5816   0.750000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect preds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>reply_question</th>\n",
       "      <th>pred_tokens</th>\n",
       "      <th>label_tokens</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>Why do viruses often come from bats? A discussion with your friendly neighborhood virologist</td>\n",
       "      <td>Is there even solid evidence that fever actually does anything?</td>\n",
       "      <td>[?, with, vi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34378</th>\n",
       "      <td>Use of genetically modified eggplant in Bangladesh increased the yield by 51%, decreased pesticide use by 37%, increased farmer profits, and reduced reporting of sickness tied to pesticide poisoning. [Evidence from randomized control trials]</td>\n",
       "      <td>What is it doing to the soil long term?</td>\n",
       "      <td>[use, by, use, by, 37, reduced, [, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>Conservatives and liberals differ on COVID-19 because conservatives tend to attribute negative outcomes to purposeful actions by threats high in agency. If health officials talked about the virus as a palpable enemy that is seeking to attack humans, they may get greater buy-in from conservatives.</td>\n",
       "      <td>So... We have to discuss covid with conservatives like we do when we frame the concepts for small children?</td>\n",
       "      <td>[on, ##vid, -, tend, attribute, purpose, actions, by, about, ##pa, buy, -]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73001</th>\n",
       "      <td>DEA moves toward approving more research for Marijuana growers. 36 states now permit marijuana to be used medically, and 17 allow recreational use. Yet researchers who wish to study the drug’s health effects have been limited since 1968 to a single legal supplier of the drug.</td>\n",
       "      <td>Interesting read but is it really appropriate for this sub?</td>\n",
       "      <td>[dea, moves, app, 36, 17, use, wish, since]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7690</th>\n",
       "      <td>Smokers who scored higher on a test of math ability were more likely to say they intended to quit smoking, says an online study of Americans. The reason: They had a better memory for smoking risk stats, which led to perceiving a greater risk from smoking and then a greater intention to quit.</td>\n",
       "      <td>The title implies causation but is that even established in the study itself?</td>\n",
       "      <td>[on, likely, intended, says, ##iving, intention]</td>\n",
       "      <td>[study]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59807</th>\n",
       "      <td>People’s attachment to the wilderness is linked to the fulfillment of basic psychological needs, study finds</td>\n",
       "      <td>I'm amazed how something that basic is... rediscovered?</td>\n",
       "      <td>[needs]</td>\n",
       "      <td>[basic]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60674</th>\n",
       "      <td>About 1 in 3 patients were diagnosed with psychiatric or neurological illnesses in the 6 months following COVID-19 diagnosis (n=236379). An even greater proportion (46%) of patients who were hospitalized received such diagnoses.</td>\n",
       "      <td>What percent of the population has a psychiatric problem?</td>\n",
       "      <td>[about, 1, 3, with, 6, ##vid, -, (, 236, (, ##ses]</td>\n",
       "      <td>[psychiatric]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57890</th>\n",
       "      <td>Studies reveal that location tracking apps do more than just monitor your whereabouts; they collect a lot of sensitive information about the user's residence, habits, interests, demographics, and personality traits</td>\n",
       "      <td>Aside from the breach of privacy, what’s my risk here if I have nothing to hide?</td>\n",
       "      <td>[lot, about, ', personality, traits]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>Blood clotting a significant cause of death in patients with COVID-19. \"COVID-19 is associated with a unique type of blood clotting disorder that is primarily focussed within the lungs &amp;amp; which undoubtedly contributes to the high levels of mortality being seen in patients with COVID-19\".</td>\n",
       "      <td>So what they are saying is, I should continue taking my blood thinners?</td>\n",
       "      <td>[cl, ##otti, ##ng, with, ##vid, -, \", ##vid, -, associated, with, cl, ##otti, ##ng, primarily, amp, contributes, with, ##vid, -, \"]</td>\n",
       "      <td>[blood, blood]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37567</th>\n",
       "      <td>Average male punching power found to be 162% (2.62x) greater than average female punching power; the weakest male in the study still outperformed the strongest female; n=39</td>\n",
       "      <td>and how do they differ between males and females?</td>\n",
       "      <td>[(, ##x, weak, ##formed, strongest]</td>\n",
       "      <td>[male, female, male, female]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>A new study in The Lancet by a team of Yale epidemiologists finds that Medicare for All would save more than 68,000 lives annually as well as $450 billion in cost</td>\n",
       "      <td>All of a sudden these companies are gone... You think this will just pass?</td>\n",
       "      <td>[by, would, 68]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24057</th>\n",
       "      <td>Experiences during the first few years of life can have enduring downstream consequences for people’s romantic relationships 20–30 years later. Hostility by the mother in early childhood was a significant predictor of dehumanization behavior against romantic partners by adults.</td>\n",
       "      <td>Is there anything for the father, especially in relation to how it impacts women?</td>\n",
       "      <td>[first, –, hostility, by, ##ani, by]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77558</th>\n",
       "      <td>The CRISPR-based test—which uses gene-targeting technology and requires no specialized equipment—could help detect COVID-19 infections in about 45 minutes.</td>\n",
       "      <td>Just slap a GFP onto the Cas-9 and remove the endonuclease so when it finds the target sequence it illuminates?</td>\n",
       "      <td>[-, based, —, uses, -, —, help, ##vid, -, about]</td>\n",
       "      <td>[targeting]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24346</th>\n",
       "      <td>Despite increases in gender equality and the normalization of casual sex in many cultures, the belief that women who engage in casual sex have low self-esteem remains widespread. New research examines this stereotype and finds no correlation between a woman’s sexual behavior and her self-esteem.</td>\n",
       "      <td>Is it just me or does this subreddit push propaganda?</td>\n",
       "      <td>[-, examines, stereo, -]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62032</th>\n",
       "      <td>Scientists have developed a way of extracting a richer palette of colors from the available spectrum by harnessing disordered patterns inspired by nature that would typically be seen as black. Controlling light that passes through these disordered surfaces is able to produce vivid colors.</td>\n",
       "      <td>... does it mean we'd be able to see other colors?</td>\n",
       "      <td>[developed, palette, by, harness, by, would, controlling, through, vivid]</td>\n",
       "      <td>[colors, able, colors]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>Supervisors with ‘bottom-line mentality’ driven by profits to the exclusion of caring about other outcomes, like employee well-being, environment or ethics, could be hurting their bottom lines by losing the respect of their employees, who counter by withholding performance, according to a new study.</td>\n",
       "      <td>Everyone knows this but still, someone wanna send this to my boss?</td>\n",
       "      <td>[supervisors, with, ‘, -, by, exclusion, about, like, employee, -, by, by, with]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70172</th>\n",
       "      <td>Researchers show children are silent spreaders of virus that causes COVID-19. The infected children were shown to have a significantly higher level of virus in their airways than hospitalized adults in ICUs for COVID-19 treatment.</td>\n",
       "      <td>Could someone here explain where this idiotic baseless assumption about children not spreading covid came from originally?</td>\n",
       "      <td>[##vid, -, ##us, ##vid, -]</td>\n",
       "      <td>[children, spread, children]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69993</th>\n",
       "      <td>New research shows that when vampire bats feel sick, they socially distance themselves from groupmates in their roost – no public health guidance required. Study was conducted in the wild, tracking bats' social encounters with \"backpack\" computers containing proximity sensors.</td>\n",
       "      <td>Is it altruism, or is it the fact that sick creatures typically are less friendly and don't want to socialize when they feel sick?</td>\n",
       "      <td>[##mates, –, ', encounters, with, \", \", proximity, sensors]</td>\n",
       "      <td>[feel, sick, socially, social]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29331</th>\n",
       "      <td>New form of magnetic brain stimulation eliminates severe depression within days</td>\n",
       "      <td>How can I get my son involved in this?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11127</th>\n",
       "      <td>A SARS-CoV-2 protein interaction map reveals targets for drug repurposing</td>\n",
       "      <td>It seems like this is saying it narrowed a list of drugs that we can try to treat it?</td>\n",
       "      <td>[-, -, interaction, reveals, rep]</td>\n",
       "      <td>[drug]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                              title  \\\n",
       "17989                                                                                                                                                                                                                  Why do viruses often come from bats? A discussion with your friendly neighborhood virologist   \n",
       "34378                                                             Use of genetically modified eggplant in Bangladesh increased the yield by 51%, decreased pesticide use by 37%, increased farmer profits, and reduced reporting of sickness tied to pesticide poisoning. [Evidence from randomized control trials]   \n",
       "1278      Conservatives and liberals differ on COVID-19 because conservatives tend to attribute negative outcomes to purposeful actions by threats high in agency. If health officials talked about the virus as a palpable enemy that is seeking to attack humans, they may get greater buy-in from conservatives.   \n",
       "73001                          DEA moves toward approving more research for Marijuana growers. 36 states now permit marijuana to be used medically, and 17 allow recreational use. Yet researchers who wish to study the drug’s health effects have been limited since 1968 to a single legal supplier of the drug.   \n",
       "7690           Smokers who scored higher on a test of math ability were more likely to say they intended to quit smoking, says an online study of Americans. The reason: They had a better memory for smoking risk stats, which led to perceiving a greater risk from smoking and then a greater intention to quit.   \n",
       "59807                                                                                                                                                                                                  People’s attachment to the wilderness is linked to the fulfillment of basic psychological needs, study finds   \n",
       "60674                                                                          About 1 in 3 patients were diagnosed with psychiatric or neurological illnesses in the 6 months following COVID-19 diagnosis (n=236379). An even greater proportion (46%) of patients who were hospitalized received such diagnoses.   \n",
       "57890                                                                                        Studies reveal that location tracking apps do more than just monitor your whereabouts; they collect a lot of sensitive information about the user's residence, habits, interests, demographics, and personality traits   \n",
       "9261            Blood clotting a significant cause of death in patients with COVID-19. \"COVID-19 is associated with a unique type of blood clotting disorder that is primarily focussed within the lungs &amp; which undoubtedly contributes to the high levels of mortality being seen in patients with COVID-19\".   \n",
       "37567                                                                                                                                  Average male punching power found to be 162% (2.62x) greater than average female punching power; the weakest male in the study still outperformed the strongest female; n=39   \n",
       "76697                                                                                                                                            A new study in The Lancet by a team of Yale epidemiologists finds that Medicare for All would save more than 68,000 lives annually as well as $450 billion in cost   \n",
       "24057                        Experiences during the first few years of life can have enduring downstream consequences for people’s romantic relationships 20–30 years later. Hostility by the mother in early childhood was a significant predictor of dehumanization behavior against romantic partners by adults.   \n",
       "77558                                                                                                                                                   The CRISPR-based test—which uses gene-targeting technology and requires no specialized equipment—could help detect COVID-19 infections in about 45 minutes.   \n",
       "24346      Despite increases in gender equality and the normalization of casual sex in many cultures, the belief that women who engage in casual sex have low self-esteem remains widespread. New research examines this stereotype and finds no correlation between a woman’s sexual behavior and her self-esteem.   \n",
       "62032             Scientists have developed a way of extracting a richer palette of colors from the available spectrum by harnessing disordered patterns inspired by nature that would typically be seen as black. Controlling light that passes through these disordered surfaces is able to produce vivid colors.   \n",
       "1484   Supervisors with ‘bottom-line mentality’ driven by profits to the exclusion of caring about other outcomes, like employee well-being, environment or ethics, could be hurting their bottom lines by losing the respect of their employees, who counter by withholding performance, according to a new study.   \n",
       "70172                                                                        Researchers show children are silent spreaders of virus that causes COVID-19. The infected children were shown to have a significantly higher level of virus in their airways than hospitalized adults in ICUs for COVID-19 treatment.   \n",
       "69993                         New research shows that when vampire bats feel sick, they socially distance themselves from groupmates in their roost – no public health guidance required. Study was conducted in the wild, tracking bats' social encounters with \"backpack\" computers containing proximity sensors.   \n",
       "29331                                                                                                                                                                                                                               New form of magnetic brain stimulation eliminates severe depression within days   \n",
       "11127                                                                                                                                                                                                                                     A SARS-CoV-2 protein interaction map reveals targets for drug repurposing   \n",
       "\n",
       "                                                                                                                           reply_question  \\\n",
       "17989                                                                     Is there even solid evidence that fever actually does anything?   \n",
       "34378                                                                                             What is it doing to the soil long term?   \n",
       "1278                          So... We have to discuss covid with conservatives like we do when we frame the concepts for small children?   \n",
       "73001                                                                         Interesting read but is it really appropriate for this sub?   \n",
       "7690                                                        The title implies causation but is that even established in the study itself?   \n",
       "59807                                                                             I'm amazed how something that basic is... rediscovered?   \n",
       "60674                                                                           What percent of the population has a psychiatric problem?   \n",
       "57890                                                    Aside from the breach of privacy, what’s my risk here if I have nothing to hide?   \n",
       "9261                                                              So what they are saying is, I should continue taking my blood thinners?   \n",
       "37567                                                                                   and how do they differ between males and females?   \n",
       "76697                                                          All of a sudden these companies are gone... You think this will just pass?   \n",
       "24057                                                   Is there anything for the father, especially in relation to how it impacts women?   \n",
       "77558                     Just slap a GFP onto the Cas-9 and remove the endonuclease so when it finds the target sequence it illuminates?   \n",
       "24346                                                                               Is it just me or does this subreddit push propaganda?   \n",
       "62032                                                                                  ... does it mean we'd be able to see other colors?   \n",
       "1484                                                                   Everyone knows this but still, someone wanna send this to my boss?   \n",
       "70172          Could someone here explain where this idiotic baseless assumption about children not spreading covid came from originally?   \n",
       "69993  Is it altruism, or is it the fact that sick creatures typically are less friendly and don't want to socialize when they feel sick?   \n",
       "29331                                                                                              How can I get my son involved in this?   \n",
       "11127                                               It seems like this is saying it narrowed a list of drugs that we can try to treat it?   \n",
       "\n",
       "                                                                                                                               pred_tokens  \\\n",
       "17989                                                                                                                        [?, with, vi]   \n",
       "34378                                                                                                [use, by, use, by, 37, reduced, [, ]]   \n",
       "1278                                                            [on, ##vid, -, tend, attribute, purpose, actions, by, about, ##pa, buy, -]   \n",
       "73001                                                                                          [dea, moves, app, 36, 17, use, wish, since]   \n",
       "7690                                                                                      [on, likely, intended, says, ##iving, intention]   \n",
       "59807                                                                                                                              [needs]   \n",
       "60674                                                                                   [about, 1, 3, with, 6, ##vid, -, (, 236, (, ##ses]   \n",
       "57890                                                                                                 [lot, about, ', personality, traits]   \n",
       "9261   [cl, ##otti, ##ng, with, ##vid, -, \", ##vid, -, associated, with, cl, ##otti, ##ng, primarily, amp, contributes, with, ##vid, -, \"]   \n",
       "37567                                                                                                  [(, ##x, weak, ##formed, strongest]   \n",
       "76697                                                                                                                      [by, would, 68]   \n",
       "24057                                                                                                 [first, –, hostility, by, ##ani, by]   \n",
       "77558                                                                                     [-, based, —, uses, -, —, help, ##vid, -, about]   \n",
       "24346                                                                                                             [-, examines, stereo, -]   \n",
       "62032                                                            [developed, palette, by, harness, by, would, controlling, through, vivid]   \n",
       "1484                                                      [supervisors, with, ‘, -, by, exclusion, about, like, employee, -, by, by, with]   \n",
       "70172                                                                                                           [##vid, -, ##us, ##vid, -]   \n",
       "69993                                                                          [##mates, –, ', encounters, with, \", \", proximity, sensors]   \n",
       "29331                                                                                                                                   []   \n",
       "11127                                                                                                    [-, -, interaction, reveals, rep]   \n",
       "\n",
       "                         label_tokens  prec  rec   F1  \n",
       "17989                              []   0.0  0.0  0.0  \n",
       "34378                              []   0.0  0.0  0.0  \n",
       "1278                               []   0.0  0.0  0.0  \n",
       "73001                              []   0.0  0.0  0.0  \n",
       "7690                          [study]   0.0  0.0  0.0  \n",
       "59807                         [basic]   0.0  0.0  0.0  \n",
       "60674                   [psychiatric]   0.0  0.0  0.0  \n",
       "57890                              []   0.0  0.0  0.0  \n",
       "9261                   [blood, blood]   0.0  0.0  0.0  \n",
       "37567    [male, female, male, female]   0.0  0.0  0.0  \n",
       "76697                              []   0.0  0.0  0.0  \n",
       "24057                              []   0.0  0.0  0.0  \n",
       "77558                     [targeting]   0.0  0.0  0.0  \n",
       "24346                              []   0.0  0.0  0.0  \n",
       "62032          [colors, able, colors]   0.0  0.0  0.0  \n",
       "1484                               []   0.0  0.0  0.0  \n",
       "70172    [children, spread, children]   0.0  0.0  0.0  \n",
       "69993  [feel, sick, socially, social]   0.0  0.0  0.0  \n",
       "29331                              []   0.0  0.0  0.0  \n",
       "11127                          [drug]   0.0  0.0  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "display_cols = ['title', 'reply_question', 'pred_tokens', 'label_tokens', 'prec', 'rec', 'F1']\n",
    "print('correct preds')\n",
    "display(test_pred_data.sort_values('F1', ascending=False).loc[:, display_cols].head(20))\n",
    "print('incorrect preds')\n",
    "display(test_pred_data.sort_values('F1', ascending=True).loc[:, display_cols].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correct predictions:\n",
    "    - Copying individual words from title (`needs`; `long`, `use`)\n",
    "    - Engaging with content ([`chest`, `gorilla`]; `shock`)\n",
    "- Incorrect predictions: \n",
    "    - Focusing on tangential aspects of study ([`weak`, `strongest`] vs. [`male`, `female`]; [`palette`, `harness`] vs. [`colors`])\n",
    "    - Overgeneration ([`supervisors`, `employee`] vs. [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract relevant words: proper nouns, noun chunks, verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question generation\n",
    "1. Can we generate the questions asked in comments based on the post text?\n",
    "2. Can we do this for specific reader groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: taxonomy of clarification questions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

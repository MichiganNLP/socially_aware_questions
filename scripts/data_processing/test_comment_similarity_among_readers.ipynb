{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test comment similarity among readers\n",
    "Do readers of similar groups (location, experience, engagement) tend to react similarity with respect to the same article?\n",
    "\n",
    "We look at this in terms of:\n",
    "\n",
    "- entity mentions\n",
    "- question content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load author comment data extracted from prior comments, e.g. location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>date_day</th>\n",
       "      <th>prior_comment_count_bin</th>\n",
       "      <th>prior_comment_len_bin</th>\n",
       "      <th>location_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1045</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1045</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1045</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1072</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1072</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID   date_day  prior_comment_count_bin  prior_comment_len_bin  \\\n",
       "0    1045 2017-04-09                        1                      0   \n",
       "1    1045 2018-01-17                        0                      0   \n",
       "2    1045 2018-01-24                        0                      0   \n",
       "3    1072 2017-03-13                        1                      1   \n",
       "4    1072 2018-01-03                        0                      1   \n",
       "\n",
       "  location_region  \n",
       "0              US  \n",
       "1              US  \n",
       "2              US  \n",
       "3              US  \n",
       "4              US  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "comment_author_data = pd.read_csv('../../data/nyt_comments/author_comment_social_data.tsv', sep='\\t', index_col=False)\n",
    "date_day_fmt = '%Y-%m-%d'\n",
    "comment_author_data = comment_author_data.assign(**{\n",
    "    'date_day' : comment_author_data.loc[:, 'date_day'].apply(lambda x: datetime.strptime(x, date_day_fmt))\n",
    "})\n",
    "display(comment_author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-61d679fddff7>:17: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  comment_data = pd.concat(comment_data, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>createDate</th>\n",
       "      <th>userID</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58691a5795d0e039260788b9</td>\n",
       "      <td>For all you Americans out there --- still rejo...</td>\n",
       "      <td>1.483426e+09</td>\n",
       "      <td>64679318</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58691a5795d0e039260788b9</td>\n",
       "      <td>Obamas policies may prove to be the least of t...</td>\n",
       "      <td>1.483417e+09</td>\n",
       "      <td>69254188</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58691a5795d0e039260788b9</td>\n",
       "      <td>Democrats are comprised of malcontents who gen...</td>\n",
       "      <td>1.483431e+09</td>\n",
       "      <td>76788711</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58691a5795d0e039260788b9</td>\n",
       "      <td>The picture in this article is the face of con...</td>\n",
       "      <td>1.483419e+09</td>\n",
       "      <td>72718862</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58691a5795d0e039260788b9</td>\n",
       "      <td>Elections have consequences.</td>\n",
       "      <td>1.483417e+09</td>\n",
       "      <td>7529267</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  \\\n",
       "0  58691a5795d0e039260788b9   \n",
       "1  58691a5795d0e039260788b9   \n",
       "2  58691a5795d0e039260788b9   \n",
       "3  58691a5795d0e039260788b9   \n",
       "4  58691a5795d0e039260788b9   \n",
       "\n",
       "                                         commentBody    createDate    userID  \\\n",
       "0  For all you Americans out there --- still rejo...  1.483426e+09  64679318   \n",
       "1  Obamas policies may prove to be the least of t...  1.483417e+09  69254188   \n",
       "2  Democrats are comprised of malcontents who gen...  1.483431e+09  76788711   \n",
       "3  The picture in this article is the face of con...  1.483419e+09  72718862   \n",
       "4                       Elections have consequences.  1.483417e+09   7529267   \n",
       "\n",
       "    date_day  \n",
       "0 2017-01-03  \n",
       "1 2017-01-02  \n",
       "2 2017-01-03  \n",
       "3 2017-01-02  \n",
       "4 2017-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load comments\n",
    "import os\n",
    "month_year_pairs = [\n",
    "        ('Jan', '2017'),\n",
    "        ('Feb', '2017'),\n",
    "        ('March', '2017'),\n",
    "        ('April', '2017'),\n",
    "        ('Jan', '2018'),\n",
    "        ('Feb', '2018'),\n",
    "        ('March', '2018'),\n",
    "        ('April', '2018'),\n",
    "    ]\n",
    "comment_data_dir = '../../data/nyt_comments/'\n",
    "comment_cols = ['articleID', 'commentBody', 'createDate', 'userID']\n",
    "comment_data_files = list(map(lambda x: os.path.join(comment_data_dir, f'Comments{x[0]}{x[1]}.csv'), month_year_pairs))\n",
    "comment_data = list(map(lambda x: pd.read_csv(x, sep=',', index_col=False, usecols=comment_cols), comment_data_files))\n",
    "comment_data = pd.concat(comment_data, axis=0)\n",
    "comment_data = comment_data.assign(**{\n",
    "    'userID' : comment_data.loc[:, 'userID'].astype(int)\n",
    "})\n",
    "# remove null comments\n",
    "comment_data = comment_data[comment_data.loc[:, 'commentBody'].apply(lambda x: type(x) is not float)]\n",
    "# fix date var\n",
    "from data_helpers import round_date_to_day\n",
    "comment_data = comment_data.assign(**{\n",
    "    'date_day' : comment_data.loc[:, 'createDate'].apply(round_date_to_day)\n",
    "})\n",
    "display(comment_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899592 comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>date_day</th>\n",
       "      <th>prior_comment_count_bin</th>\n",
       "      <th>prior_comment_len_bin</th>\n",
       "      <th>location_region</th>\n",
       "      <th>articleID</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>createDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1045</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>58e3609a7c459f24986d83a6</td>\n",
       "      <td>The \"earliest days of the internet\" were not i...</td>\n",
       "      <td>1.491756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1045</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>5a5f71d87c459f29e79b46ad</td>\n",
       "      <td>You left out the part where she wrote an anony...</td>\n",
       "      <td>1.516227e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1045</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>5a68592710f40f00018bd70d</td>\n",
       "      <td>Yes, thank you. Folks need to lighten up. I wo...</td>\n",
       "      <td>1.516816e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1072</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>58c7001c7c459f247805b940</td>\n",
       "      <td>“Try to imagine a … presidential candidate say...</td>\n",
       "      <td>1.489449e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1072</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>5a4e75be7c459f29e79b20c2</td>\n",
       "      <td>Fix the Subways.  That this argument has to be...</td>\n",
       "      <td>1.514982e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID   date_day  prior_comment_count_bin  prior_comment_len_bin  \\\n",
       "0    1045 2017-04-09                        1                      0   \n",
       "1    1045 2018-01-17                        0                      0   \n",
       "2    1045 2018-01-24                        0                      0   \n",
       "3    1072 2017-03-13                        1                      1   \n",
       "4    1072 2018-01-03                        0                      1   \n",
       "\n",
       "  location_region                 articleID  \\\n",
       "0              US  58e3609a7c459f24986d83a6   \n",
       "1              US  5a5f71d87c459f29e79b46ad   \n",
       "2              US  5a68592710f40f00018bd70d   \n",
       "3              US  58c7001c7c459f247805b940   \n",
       "4              US  5a4e75be7c459f29e79b20c2   \n",
       "\n",
       "                                         commentBody    createDate  \n",
       "0  The \"earliest days of the internet\" were not i...  1.491756e+09  \n",
       "1  You left out the part where she wrote an anony...  1.516227e+09  \n",
       "2  Yes, thank you. Folks need to lighten up. I wo...  1.516816e+09  \n",
       "3  “Try to imagine a … presidential candidate say...  1.489449e+09  \n",
       "4  Fix the Subways.  That this argument has to be...  1.514982e+09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## join data\n",
    "combined_comment_author_data = pd.merge(comment_author_data, comment_data, on=['userID', 'date_day'])\n",
    "print(f'{combined_comment_author_data.shape[0]} comments')\n",
    "display(combined_comment_author_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test entity overlap\n",
    "First, let's test the overlap in entities mentioned in the comments, without respect to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-19 23:44:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-12-19 23:44:21 INFO: Use device: gpu\n",
      "2020-12-19 23:44:21 INFO: Loading: tokenize\n",
      "2020-12-19 23:44:24 INFO: Loading: ner\n",
      "2020-12-19 23:44:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "## extract entities with NER\n",
    "import stanza\n",
    "# help(stanza.Pipeline)\n",
    "nlp_pipeline = stanza.Pipeline(lang='en', processors='tokenize,ner', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Donald_Trump', 'the_Atlanta_Falcons']\n"
     ]
    }
   ],
   "source": [
    "def extract_all_named_entities(text, pipeline):\n",
    "    text_doc = pipeline(text)\n",
    "    named_entities = []\n",
    "    for text_sent_i in text_doc.sentences:\n",
    "        NE_tokens_i = []\n",
    "        for token_j in text_sent_i.tokens:\n",
    "            token_j_NE = token_j.ner\n",
    "            if(token_j_NE != 'O'):\n",
    "                token_j_text = token_j.text\n",
    "                if(token_j_NE.startswith('E')):\n",
    "                    NE_tokens_i.append(token_j_text)\n",
    "                    NE_final = '_'.join(NE_tokens_i)\n",
    "                    named_entities.append(NE_final)\n",
    "                    NE_tokens_i = []\n",
    "                else:\n",
    "                    NE_tokens_i.append(token_j_text)\n",
    "    return named_entities\n",
    "# print(nlp_pipeline('this is a test for Donald Trump. he is a person.'))\n",
    "test_text = 'this is a test for Donald Trump. he is a person. the Atlanta Falcons won the game.'\n",
    "x = nlp_pipeline(test_text)\n",
    "# print(x.)\n",
    "print(extract_all_named_entities(test_text, nlp_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample to avoid hurting our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "sample_size = 100000\n",
    "sample_comment_author_data_idx = np.random.choice(combined_comment_author_data.index, sample_size, replace=False)\n",
    "sample_comment_author_data = combined_comment_author_data.loc[sample_comment_author_data_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/tqdm/std.py:706: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      " 10%|▉         | 9526/100000 [10:24<1:24:26, 17.86it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 41%|████      | 40795/100000 [42:21<55:11, 17.88it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 73%|███████▎  | 73190/100000 [1:15:41<19:49, 22.53it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 100000/100000 [1:42:45<00:00, 16.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "sample_comment_author_data = sample_comment_author_data.assign(**{\n",
    "    'named_entities' : sample_comment_author_data.loc[:, 'commentBody'].progress_apply(lambda x: extract_all_named_entities(x, nlp_pipeline))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>date_day</th>\n",
       "      <th>prior_comment_count_bin</th>\n",
       "      <th>prior_comment_len_bin</th>\n",
       "      <th>location_region</th>\n",
       "      <th>articleID</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>createDate</th>\n",
       "      <th>named_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438434</th>\n",
       "      <td>30569608</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>5885282895d0e0392607c266</td>\n",
       "      <td>Just watched Sean Splicer's news briefing. He ...</td>\n",
       "      <td>1.485213e+09</td>\n",
       "      <td>[Sean_Splicer_'s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201060</th>\n",
       "      <td>12698735</td>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>non_US</td>\n",
       "      <td>587e66d195d0e0392607b2ee</td>\n",
       "      <td>Glad to hear I'm not the only one who cries at...</td>\n",
       "      <td>1.484776e+09</td>\n",
       "      <td>[Chef_Howard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66777</th>\n",
       "      <td>2258463</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>5ab6993247de81a901216a6a</td>\n",
       "      <td>Perhaps a better analogy: Hiroshima, destructi...</td>\n",
       "      <td>1.521944e+09</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215272</th>\n",
       "      <td>63735584</td>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>58de28ac7c459f24986d7a00</td>\n",
       "      <td>I am losing weight, slowly, at the age of 64 f...</td>\n",
       "      <td>1.491395e+09</td>\n",
       "      <td>[the_age_of_64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579567</th>\n",
       "      <td>71835948</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>non_US</td>\n",
       "      <td>58962bcf95d0e0392607ea60</td>\n",
       "      <td>How does an \"originalist\" ever get to Brown v....</td>\n",
       "      <td>1.486312e+09</td>\n",
       "      <td>[Brown_v._Board_of_Education]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userID   date_day  prior_comment_count_bin  prior_comment_len_bin  \\\n",
       "438434   30569608 2017-01-23                        0                      0   \n",
       "201060   12698735 2017-01-18                        1                      1   \n",
       "66777     2258463 2018-03-24                        1                      1   \n",
       "1215272  63735584 2017-04-05                        0                      0   \n",
       "1579567  71835948 2017-02-05                        0                      0   \n",
       "\n",
       "        location_region                 articleID  \\\n",
       "438434               US  5885282895d0e0392607c266   \n",
       "201060           non_US  587e66d195d0e0392607b2ee   \n",
       "66777                US  5ab6993247de81a901216a6a   \n",
       "1215272              US  58de28ac7c459f24986d7a00   \n",
       "1579567          non_US  58962bcf95d0e0392607ea60   \n",
       "\n",
       "                                               commentBody    createDate  \\\n",
       "438434   Just watched Sean Splicer's news briefing. He ...  1.485213e+09   \n",
       "201060   Glad to hear I'm not the only one who cries at...  1.484776e+09   \n",
       "66777    Perhaps a better analogy: Hiroshima, destructi...  1.521944e+09   \n",
       "1215272  I am losing weight, slowly, at the age of 64 f...  1.491395e+09   \n",
       "1579567  How does an \"originalist\" ever get to Brown v....  1.486312e+09   \n",
       "\n",
       "                        named_entities  \n",
       "438434               [Sean_Splicer_'s]  \n",
       "201060                   [Chef_Howard]  \n",
       "66777                               []  \n",
       "1215272                [the_age_of_64]  \n",
       "1579567  [Brown_v._Board_of_Education]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sample_comment_author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to file temporarily, then restart kernel to fix memory\n",
    "sample_comment_author_data.to_csv('../../data/nyt_comments/sample_author_comment_entities.gz', sep='\\t', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_comment_author_data = pd.read_csv('../../data/nyt_comments/sample_author_comment_entities.gz', sep='\\t', compression='gzip', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>date_day</th>\n",
       "      <th>prior_comment_count_bin</th>\n",
       "      <th>prior_comment_len_bin</th>\n",
       "      <th>location_region</th>\n",
       "      <th>articleID</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>createDate</th>\n",
       "      <th>named_entities</th>\n",
       "      <th>NE_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30569608</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>5885282895d0e0392607c266</td>\n",
       "      <td>Just watched Sean Splicer's news briefing. He ...</td>\n",
       "      <td>1.485213e+09</td>\n",
       "      <td>[\"Sean_Splicer_'s\"]</td>\n",
       "      <td>[Sean_Splicer_'s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12698735</td>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>non_US</td>\n",
       "      <td>587e66d195d0e0392607b2ee</td>\n",
       "      <td>Glad to hear I'm not the only one who cries at...</td>\n",
       "      <td>1.484776e+09</td>\n",
       "      <td>['Chef_Howard']</td>\n",
       "      <td>[Chef_Howard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2258463</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>5ab6993247de81a901216a6a</td>\n",
       "      <td>Perhaps a better analogy: Hiroshima, destructi...</td>\n",
       "      <td>1.521944e+09</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63735584</td>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>58de28ac7c459f24986d7a00</td>\n",
       "      <td>I am losing weight, slowly, at the age of 64 f...</td>\n",
       "      <td>1.491395e+09</td>\n",
       "      <td>['the_age_of_64']</td>\n",
       "      <td>[the_age_of_64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71835948</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>non_US</td>\n",
       "      <td>58962bcf95d0e0392607ea60</td>\n",
       "      <td>How does an \"originalist\" ever get to Brown v....</td>\n",
       "      <td>1.486312e+09</td>\n",
       "      <td>['Brown_v._Board_of_Education']</td>\n",
       "      <td>[Brown_v._Board_of_Education]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     userID    date_day  prior_comment_count_bin  prior_comment_len_bin  \\\n",
       "0  30569608  2017-01-23                        0                      0   \n",
       "1  12698735  2017-01-18                        1                      1   \n",
       "2   2258463  2018-03-24                        1                      1   \n",
       "3  63735584  2017-04-05                        0                      0   \n",
       "4  71835948  2017-02-05                        0                      0   \n",
       "\n",
       "  location_region                 articleID  \\\n",
       "0              US  5885282895d0e0392607c266   \n",
       "1          non_US  587e66d195d0e0392607b2ee   \n",
       "2              US  5ab6993247de81a901216a6a   \n",
       "3              US  58de28ac7c459f24986d7a00   \n",
       "4          non_US  58962bcf95d0e0392607ea60   \n",
       "\n",
       "                                         commentBody    createDate  \\\n",
       "0  Just watched Sean Splicer's news briefing. He ...  1.485213e+09   \n",
       "1  Glad to hear I'm not the only one who cries at...  1.484776e+09   \n",
       "2  Perhaps a better analogy: Hiroshima, destructi...  1.521944e+09   \n",
       "3  I am losing weight, slowly, at the age of 64 f...  1.491395e+09   \n",
       "4  How does an \"originalist\" ever get to Brown v....  1.486312e+09   \n",
       "\n",
       "                    named_entities                        NE_list  \n",
       "0              [\"Sean_Splicer_'s\"]              [Sean_Splicer_'s]  \n",
       "1                  ['Chef_Howard']                  [Chef_Howard]  \n",
       "2                               []                             []  \n",
       "3                ['the_age_of_64']                [the_age_of_64]  \n",
       "4  ['Brown_v._Board_of_Education']  [Brown_v._Board_of_Education]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "sample_comment_author_data = sample_comment_author_data.assign(**{\n",
    "    'NE_list' : sample_comment_author_data.loc[:, 'named_entities'].apply(literal_eval)\n",
    "})\n",
    "display(sample_comment_author_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## clean up NEs\n",
    "import re\n",
    "POSS_MATCHER = re.compile(\"_'s$\") # possessive\n",
    "ARTICLE_MATCHER = re.compile('^(the|a|an)_') # article\n",
    "def clean_NE(NE):\n",
    "    NE = POSS_MATCHER.sub('', NE)\n",
    "    NE = ARTICLE_MATCHER.sub('', NE)\n",
    "    return NE\n",
    "sample_comment_author_data = sample_comment_author_data.assign(**{\n",
    "    'NE_list' : sample_comment_author_data.loc[:, 'NE_list'].apply(lambda x: list(map(lambda y: clean_NE(y), x)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald_Trump        1518\n",
      "White_House         1106\n",
      "United_States        979\n",
      "Paul_Ryan            469\n",
      "Hillary_Clinton      432\n",
      "North_Korea          406\n",
      "Supreme_Court        402\n",
      "New_York             340\n",
      "Republican_Party     328\n",
      "NY_Times             314\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## count most frequent NEs\n",
    "sample_NEs = []\n",
    "for NE_i in sample_comment_author_data.loc[:, 'NE_list'].values:\n",
    "    sample_NEs.extend(NE_i)\n",
    "sample_NEs = pd.Series(sample_NEs).value_counts()\n",
    "print(sample_NEs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do people in the same reader group tend to mention the same entities in response to the same article?\n",
    "\n",
    "We'll compare the entity overlap among reader groups (aggregate) versus entity overlap among randomly-grouped people.\n",
    "\n",
    "Later: we'll compare at the level of news sections to address topical effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reindex in module pandas.core.series:\n",
      "\n",
      "reindex(self, index=None, **kwargs)\n",
      "    Conform Series to new index with optional filling logic, placing\n",
      "    NA/NaN in locations having no value in the previous index. A new object\n",
      "    is produced unless the new index is equivalent to the current one and\n",
      "    ``copy=False``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    \n",
      "    index : array-like, optional\n",
      "        New labels / index to conform to, should be specified using\n",
      "        keywords. Preferably an Index object to avoid duplicating data\n",
      "    \n",
      "    method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n",
      "        Method to use for filling holes in reindexed DataFrame.\n",
      "        Please note: this is only applicable to DataFrames/Series with a\n",
      "        monotonically increasing/decreasing index.\n",
      "    \n",
      "        * None (default): don't fill gaps\n",
      "        * pad / ffill: propagate last valid observation forward to next\n",
      "          valid\n",
      "        * backfill / bfill: use next valid observation to fill gap\n",
      "        * nearest: use nearest valid observations to fill gap\n",
      "    \n",
      "    copy : bool, default True\n",
      "        Return a new object, even if the passed indexes are the same.\n",
      "    level : int or name\n",
      "        Broadcast across a level, matching Index values on the\n",
      "        passed MultiIndex level.\n",
      "    fill_value : scalar, default np.NaN\n",
      "        Value to use for missing values. Defaults to NaN, but can be any\n",
      "        \"compatible\" value.\n",
      "    limit : int, default None\n",
      "        Maximum number of consecutive elements to forward or backward fill.\n",
      "    tolerance : optional\n",
      "        Maximum distance between original and new labels for inexact\n",
      "        matches. The values of the index at the matching locations most\n",
      "        satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      "    \n",
      "        Tolerance may be a scalar value, which applies the same tolerance\n",
      "        to all values, or list-like, which applies variable tolerance per\n",
      "        element. List-like includes list, tuple, array, Series, and must be\n",
      "        the same size as the index and its dtype must exactly match the\n",
      "        index's type.\n",
      "    \n",
      "        .. versionadded:: 0.21.0 (list-like tolerance)\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series with changed index.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.set_index : Set row labels.\n",
      "    DataFrame.reset_index : Remove row labels or move them to new columns.\n",
      "    DataFrame.reindex_like : Change to same indices as other DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    ``DataFrame.reindex`` supports two calling conventions\n",
      "    \n",
      "    * ``(index=index_labels, columns=column_labels, ...)``\n",
      "    * ``(labels, axis={'index', 'columns'}, ...)``\n",
      "    \n",
      "    We *highly* recommend using keyword arguments to clarify your\n",
      "    intent.\n",
      "    \n",
      "    Create a dataframe with some fictional data.\n",
      "    \n",
      "    >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n",
      "    >>> df = pd.DataFrame({\n",
      "    ...      'http_status': [200,200,404,404,301],\n",
      "    ...      'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n",
      "    ...       index=index)\n",
      "    >>> df\n",
      "               http_status  response_time\n",
      "    Firefox            200           0.04\n",
      "    Chrome             200           0.02\n",
      "    Safari             404           0.07\n",
      "    IE10               404           0.08\n",
      "    Konqueror          301           1.00\n",
      "    \n",
      "    Create a new index and reindex the dataframe. By default\n",
      "    values in the new index that do not have corresponding\n",
      "    records in the dataframe are assigned ``NaN``.\n",
      "    \n",
      "    >>> new_index= ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n",
      "    ...             'Chrome']\n",
      "    >>> df.reindex(new_index)\n",
      "                   http_status  response_time\n",
      "    Safari               404.0           0.07\n",
      "    Iceweasel              NaN            NaN\n",
      "    Comodo Dragon          NaN            NaN\n",
      "    IE10                 404.0           0.08\n",
      "    Chrome               200.0           0.02\n",
      "    \n",
      "    We can fill in the missing values by passing a value to\n",
      "    the keyword ``fill_value``. Because the index is not monotonically\n",
      "    increasing or decreasing, we cannot use arguments to the keyword\n",
      "    ``method`` to fill the ``NaN`` values.\n",
      "    \n",
      "    >>> df.reindex(new_index, fill_value=0)\n",
      "                   http_status  response_time\n",
      "    Safari                 404           0.07\n",
      "    Iceweasel                0           0.00\n",
      "    Comodo Dragon            0           0.00\n",
      "    IE10                   404           0.08\n",
      "    Chrome                 200           0.02\n",
      "    \n",
      "    >>> df.reindex(new_index, fill_value='missing')\n",
      "                  http_status response_time\n",
      "    Safari                404          0.07\n",
      "    Iceweasel         missing       missing\n",
      "    Comodo Dragon     missing       missing\n",
      "    IE10                  404          0.08\n",
      "    Chrome                200          0.02\n",
      "    \n",
      "    We can also reindex the columns.\n",
      "    \n",
      "    >>> df.reindex(columns=['http_status', 'user_agent'])\n",
      "               http_status  user_agent\n",
      "    Firefox            200         NaN\n",
      "    Chrome             200         NaN\n",
      "    Safari             404         NaN\n",
      "    IE10               404         NaN\n",
      "    Konqueror          301         NaN\n",
      "    \n",
      "    Or we can use \"axis-style\" keyword arguments\n",
      "    \n",
      "    >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n",
      "               http_status  user_agent\n",
      "    Firefox            200         NaN\n",
      "    Chrome             200         NaN\n",
      "    Safari             404         NaN\n",
      "    IE10               404         NaN\n",
      "    Konqueror          301         NaN\n",
      "    \n",
      "    To further illustrate the filling functionality in\n",
      "    ``reindex``, we will create a dataframe with a\n",
      "    monotonically increasing index (for example, a sequence\n",
      "    of dates).\n",
      "    \n",
      "    >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n",
      "    >>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n",
      "    ...                    index=date_index)\n",
      "    >>> df2\n",
      "                prices\n",
      "    2010-01-01   100.0\n",
      "    2010-01-02   101.0\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04   100.0\n",
      "    2010-01-05    89.0\n",
      "    2010-01-06    88.0\n",
      "    \n",
      "    Suppose we decide to expand the dataframe to cover a wider\n",
      "    date range.\n",
      "    \n",
      "    >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n",
      "    >>> df2.reindex(date_index2)\n",
      "                prices\n",
      "    2009-12-29     NaN\n",
      "    2009-12-30     NaN\n",
      "    2009-12-31     NaN\n",
      "    2010-01-01   100.0\n",
      "    2010-01-02   101.0\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04   100.0\n",
      "    2010-01-05    89.0\n",
      "    2010-01-06    88.0\n",
      "    2010-01-07     NaN\n",
      "    \n",
      "    The index entries that did not have a value in the original data frame\n",
      "    (for example, '2009-12-29') are by default filled with ``NaN``.\n",
      "    If desired, we can fill in the missing values using one of several\n",
      "    options.\n",
      "    \n",
      "    For example, to back-propagate the last valid value to fill the ``NaN``\n",
      "    values, pass ``bfill`` as an argument to the ``method`` keyword.\n",
      "    \n",
      "    >>> df2.reindex(date_index2, method='bfill')\n",
      "                prices\n",
      "    2009-12-29   100.0\n",
      "    2009-12-30   100.0\n",
      "    2009-12-31   100.0\n",
      "    2010-01-01   100.0\n",
      "    2010-01-02   101.0\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04   100.0\n",
      "    2010-01-05    89.0\n",
      "    2010-01-06    88.0\n",
      "    2010-01-07     NaN\n",
      "    \n",
      "    Please note that the ``NaN`` value present in the original dataframe\n",
      "    (at index value 2010-01-03) will not be filled by any of the\n",
      "    value propagation schemes. This is because filling while reindexing\n",
      "    does not look at dataframe values, but only compares the original and\n",
      "    desired indexes. If you do want to fill in the ``NaN`` values present\n",
      "    in the original dataframe, use the ``fillna()`` method.\n",
      "    \n",
      "    See the :ref:`user guide <basics.reindexing>` for more.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# a = pd.Series([1,0], ['Trump', 'Clinton'])\n",
    "# print(a)\n",
    "# print(a.reindex(['Trump', 'Clinton', 'Sanders']).replace(np.nan, 0.))\n",
    "print(help(pd.Series.reindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "def compute_NE_dist(NE_list_1, NE_list_2):\n",
    "    \"\"\"\n",
    "    Compute distance between NE lists based on\n",
    "    frequency difference.\n",
    "    \"\"\"\n",
    "    NE_counts_1 = pd.Series(list(reduce(lambda x,y: x+y, NE_list_1))).value_counts()\n",
    "    NE_counts_2 = pd.Series(list(reduce(lambda x,y: x+y, NE_list_2))).value_counts()\n",
    "    # default value (no NEs) => -1\n",
    "    if(len(NE_counts_1) == 0 and len(NE_counts_2) == 0):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # combine index\n",
    "        NE_combined = list(set(NE_counts_1.index) | set(NE_counts_2.index))\n",
    "        NE_counts_1 = NE_counts_1.reindex(NE_combined, fill_value=0.)\n",
    "        NE_counts_2 = NE_counts_2.reindex(NE_combined, fill_value=0.)\n",
    "        # normalize\n",
    "        NE_counts_1 = (NE_counts_1 / NE_counts_1.sum()).fillna(0.)\n",
    "        NE_counts_2 = (NE_counts_2 / NE_counts_2.sum()).fillna(0.)\n",
    "#         print(NE_counts_1)\n",
    "#         print(NE_counts_2)\n",
    "        # compute difference\n",
    "#         NE_count_diff = NE_counts_1 - NE_counts_2\n",
    "#         NE_count_diff = (NE_count_diff**2.).sum()**.5\n",
    "        NE_count_diff = cosine(NE_counts_1, NE_counts_2)\n",
    "        return NE_count_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine([0.5, 0.5], [0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing var=prior_comment_count_bin for value=0\n",
      "testing var=prior_comment_count_bin for value=1\n",
      "mean paired diff = 9.976E-01\n",
      "mean unpaired diff = 9.992E-01\n",
      "test stat = 214574.000 (p=2.221E-01)\n",
      "testing var=prior_comment_len_bin for value=0\n",
      "testing var=prior_comment_len_bin for value=1\n",
      "mean paired diff = 9.985E-01\n",
      "mean unpaired diff = 9.985E-01\n",
      "test stat = 246251.000 (p=2.897E-01)\n",
      "testing var=location_region for value=US\n",
      "testing var=location_region for value=non_US\n",
      "mean paired diff = 9.978E-01\n",
      "mean unpaired diff = 9.982E-01\n",
      "test stat = 241871.000 (p=3.963E-01)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "author_group_vars = ['prior_comment_count_bin', 'prior_comment_len_bin', 'location_region']\n",
    "pair_sample_size = 1000\n",
    "for author_group_var_i in author_group_vars:\n",
    "    author_group_vals_i = set(sample_comment_author_data.loc[:, author_group_var_i].unique())\n",
    "    ## compute paired similarity\n",
    "    paired_diffs = []\n",
    "    unpaired_diffs = []\n",
    "    for author_group_val_j, data_j in sample_comment_author_data.groupby(author_group_var_i):\n",
    "        print(f'testing var={author_group_var_i} for value={author_group_val_j}')\n",
    "        # randomly sample N author pairs\n",
    "        authors_j = data_j.loc[:, 'userID'].unique()\n",
    "        for k in range(pair_sample_size):\n",
    "            authors_k = np.random.choice(authors_j, 2, replace=False)\n",
    "            author_1_k, author_2_k = authors_k\n",
    "            data_1_k = data_j[data_j.loc[:, 'userID']==author_1_k]\n",
    "            data_2_k = data_j[data_j.loc[:, 'userID']==author_2_k]\n",
    "            NE_1_k = data_1_k.loc[:, 'NE_list'].values\n",
    "            NE_2_k = data_2_k.loc[:, 'NE_list'].values\n",
    "            NE_count_diff_k = compute_NE_dist(NE_1_k, NE_2_k)\n",
    "#             if(np.isnan(NE_count_diff_k)):\n",
    "#                 print(f'nan value for NEs 1 = {NE_1_k}; NEs 2 = {NE_2_k}')\n",
    "#                 break\n",
    "            paired_diffs.append(NE_count_diff_k)\n",
    "        ## compute randomly-paired similarity\n",
    "        other_authors_j = sample_comment_author_data[~sample_comment_author_data.loc[:, 'userID'].isin(authors_j)].loc[:, 'userID'].unique()\n",
    "        # get pair from both groups, compute difference\n",
    "        np.random.shuffle(authors_j)\n",
    "        np.random.shuffle(other_authors_j)\n",
    "        for group_author_k, other_group_author_k in zip(authors_j[:pair_sample_size], other_authors_j):\n",
    "            data_1_k = data_j[data_j.loc[:, 'userID']==group_author_k]\n",
    "            data_2_k = sample_comment_author_data[sample_comment_author_data.loc[:, 'userID']==other_group_author_k]\n",
    "            NE_1_k = data_1_k.loc[:, 'NE_list'].values\n",
    "            NE_2_k = data_2_k.loc[:, 'NE_list'].values\n",
    "            NE_count_diff_k = compute_NE_dist(NE_1_k, NE_2_k)\n",
    "            unpaired_diffs.append(NE_count_diff_k)\n",
    "    ## compare\n",
    "    paired_diffs = np.array(paired_diffs)\n",
    "    unpaired_diffs = np.array(unpaired_diffs)\n",
    "    paired_diffs = paired_diffs[~np.isnan(paired_diffs)]\n",
    "    unpaired_diffs = unpaired_diffs[~np.isnan(unpaired_diffs)]\n",
    "    print('mean paired diff = %.3E'%(np.mean(paired_diffs)))\n",
    "    print('mean unpaired diff = %.3E'%(np.mean(unpaired_diffs)))\n",
    "    test_stat, p_val = mannwhitneyu(paired_diffs, unpaired_diffs)\n",
    "    print('test stat = %.3f (p=%.3E)'%(test_stat, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great! We see a very small difference in means for prior comment counts and for location, but not significant.\n",
    "\n",
    "This is likely a data sparsity issue, since most of the authors only write one comment.\n",
    "Let's try to scale this up:\n",
    "\n",
    "- for each author group, sample $M_1$ authors and $M_2$ authors, then compute their NE similarity\n",
    "- for each author group, sample $M_1$ authors in-group and $M_2$ authors out-of-group, then compute their NE similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing var=prior_comment_count_bin for value=0\n",
      "testing var=prior_comment_count_bin for value=1\n",
      "median paired diff = 8.099E-01\n",
      "median unpaired diff = 8.148E-01\n",
      "test stat = 1860181.000 (p=6.443E-05)\n",
      "testing var=prior_comment_len_bin for value=0\n",
      "testing var=prior_comment_len_bin for value=1\n",
      "median paired diff = 7.959E-01\n",
      "median unpaired diff = 8.109E-01\n",
      "test stat = 1837581.000 (p=4.345E-06)\n",
      "testing var=location_region for value=US\n",
      "testing var=location_region for value=non_US\n",
      "median paired diff = 7.982E-01\n",
      "median unpaired diff = 8.029E-01\n",
      "test stat = 1922489.000 (p=1.690E-02)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "author_group_vars = ['prior_comment_count_bin', 'prior_comment_len_bin', 'location_region']\n",
    "pair_sample_size = 1000\n",
    "group_sample_size = 200\n",
    "combined_diff_data = []\n",
    "for author_group_var_i in author_group_vars:\n",
    "    author_group_vals_i = set(sample_comment_author_data.loc[:, author_group_var_i].unique())\n",
    "    ## compute paired similarity\n",
    "    paired_diffs = []\n",
    "    unpaired_diffs = []\n",
    "    for author_group_val_j, data_j in sample_comment_author_data.groupby(author_group_var_i):\n",
    "        print(f'testing var={author_group_var_i} for value={author_group_val_j}')\n",
    "        # randomly sample N author pairs\n",
    "        authors_j = data_j.loc[:, 'userID'].unique()\n",
    "        for k in range(pair_sample_size):\n",
    "            authors_k = np.random.choice(authors_j, 2, replace=False)\n",
    "            author_1_k = np.random.choice(authors_j, group_sample_size, replace=False)\n",
    "            author_2_k_pool = list(set(authors_j) - set(author_1_k))\n",
    "            author_2_k = np.random.choice(author_2_k_pool, min(group_sample_size, len(author_2_k_pool)), replace=False)\n",
    "            data_1_k = data_j[data_j.loc[:, 'userID'].isin(author_1_k)]\n",
    "            data_2_k = data_j[data_j.loc[:, 'userID'].isin(author_2_k)]\n",
    "            NE_1_k = data_1_k.loc[:, 'NE_list'].values\n",
    "            NE_2_k = data_2_k.loc[:, 'NE_list'].values\n",
    "            NE_count_diff_k = compute_NE_dist(NE_1_k, NE_2_k)\n",
    "            paired_diffs.append(NE_count_diff_k)\n",
    "        ## compute randomly-paired similarity\n",
    "        other_authors_j = sample_comment_author_data[~sample_comment_author_data.loc[:, 'userID'].isin(authors_j)].loc[:, 'userID'].unique()\n",
    "        for k in range(pair_sample_size):\n",
    "            authors_k = np.random.choice(authors_j, group_sample_size, replace=False)\n",
    "            other_authors_k = np.random.choice(other_authors_j, group_sample_size, replace=False)\n",
    "            data_1_k = data_j[data_j.loc[:, 'userID'].isin(authors_k)]\n",
    "            data_2_k = sample_comment_author_data[sample_comment_author_data.loc[:, 'userID'].isin(other_authors_k)]\n",
    "            NE_1_k = data_1_k.loc[:, 'NE_list'].values\n",
    "            NE_2_k = data_2_k.loc[:, 'NE_list'].values\n",
    "            NE_count_diff_k = compute_NE_dist(NE_1_k, NE_2_k)\n",
    "            unpaired_diffs.append(NE_count_diff_k)\n",
    "    ## compare\n",
    "    paired_diffs = np.array(paired_diffs)\n",
    "    unpaired_diffs = np.array(unpaired_diffs)\n",
    "    paired_diffs = paired_diffs[~np.isnan(paired_diffs)]\n",
    "    unpaired_diffs = unpaired_diffs[~np.isnan(unpaired_diffs)]\n",
    "    print('median paired diff = %.3E'%(np.median(paired_diffs)))\n",
    "    print('median unpaired diff = %.3E'%(np.median(unpaired_diffs)))\n",
    "    test_stat, p_val = mannwhitneyu(paired_diffs, unpaired_diffs)\n",
    "    print('test stat = %.3f (p=%.3E)'%(test_stat, p_val))\n",
    "    diff_data_i = pd.DataFrame(\n",
    "        [paired_diffs, unpaired_diffs, \n",
    "         [author_group_var_i,]*(pair_sample_size*2), \n",
    "         list(range(pair_sample_size*2))],\n",
    "        index=['paired_diff', 'unpaired_diff', 'author_group', 'sample']\n",
    "    ).transpose()\n",
    "    combined_diff_data.append(diff_data_i)\n",
    "combined_diff_data = pd.concat(combined_diff_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff</th>\n",
       "      <th>author_group</th>\n",
       "      <th>diff_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.777923</td>\n",
       "      <td>location_region</td>\n",
       "      <td>intra-group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.791526</td>\n",
       "      <td>location_region</td>\n",
       "      <td>intra-group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814428</td>\n",
       "      <td>location_region</td>\n",
       "      <td>intra-group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.83328</td>\n",
       "      <td>location_region</td>\n",
       "      <td>intra-group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.872686</td>\n",
       "      <td>location_region</td>\n",
       "      <td>intra-group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       diff     author_group    diff_type\n",
       "0  0.777923  location_region  intra-group\n",
       "1  0.791526  location_region  intra-group\n",
       "2  0.814428  location_region  intra-group\n",
       "3   0.83328  location_region  intra-group\n",
       "4  0.872686  location_region  intra-group"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## plot differences\n",
    "# # flatten data for plotting\n",
    "# flat_diff_data = []\n",
    "# for group_i, data_i in combined_diff_data.groupby('author_group'):\n",
    "#     pair_data_i = data_i.loc[:, ['paired_diff', 'author_group']].rename(columns={'paired_diff':'diff'}).assign(**{'diff_type' : 'intra-group'})\n",
    "#     unpair_data_i = data_i.loc[:, ['unpaired_diff', 'author_group']].rename(columns={'unpaired_diff':'diff'}).assign(**{'diff_type' : 'inter-group'})\n",
    "#     flat_diff_data.extend([pair_data_i, unpair_data_i])\n",
    "# flat_diff_data = pd.concat(flat_diff_data, axis=0)\n",
    "display(flat_diff_data.head())\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=flat_diff_data, x='author_group', y='diff', hue='diff_type')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This is more what I expected. The author groups have (slightly) lower intra-group difference than inter-group difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test question similarity\n",
    "Now that we've established that readers in the same group tend to mention similar NEs, can we determine if readers in the same group ask similar questions?\n",
    "\n",
    "For this we need a more complicated approach.\n",
    "Let's look first at the type of question based on `information elicited` (\"what\", \"where\", \"why\", \"how\", \"who\").\n",
    "If necessary, we'll scale up by converting questions to latent semantic space and computing their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

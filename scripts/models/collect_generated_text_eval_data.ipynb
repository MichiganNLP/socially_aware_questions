{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect generated text for evaluation data\n",
    "We need to collect text generated by the model for the following two evaluation tasks:\n",
    "\n",
    "1. Is reader-aware text (1) more relevant and (2) more likely to elicit interesting information from the author, than non-aware text?\n",
    "2. Is it just as easy to differentiate different reader groups in the generated text as it is for the real text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51302, 8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "val_data = torch.load('../../data/reddit_data/combined_data_val_data.pt', map_location='cpu')\n",
    "# convert to data frame because easier\n",
    "import pandas as pd\n",
    "val_data = pd.DataFrame(list(val_data))\n",
    "data_cols = ['article_id', 'author_has_subreddit_embed', 'author_has_text_embed', 'reader_token_str', 'source_text', 'target_text', 'subreddit_embed', 'text_embed']\n",
    "val_data = val_data.loc[:, data_cols]\n",
    "print(val_data.shape)\n",
    "# get metadata\n",
    "import pandas as pd\n",
    "post_metadata = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', sep='\\t', compression='gzip', usecols=['article_id', 'subreddit'])\n",
    "# add subreddit info\n",
    "val_data = pd.merge(val_data, post_metadata, on=['article_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for no-reader and reader-aware generated questions\n",
    "After generating text with `test_question_generation.py`, we can filter for questions generated by no-reader and reader-aware models (additional condition: question should have some reader information attached)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "no_reader_pred_text = list(gzip.open('../../data/reddit_data/text_only_model/test_data_output_text.gz'))\n",
    "reader_aware_pred_text = list(gzip.open('../../data/reddit_data/author_text_data/test_data_output_text.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for real and generated reader-aware questions\n",
    "Let's get real and generated text for reader-aware questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advice\n",
      "15\n",
      "38\n",
      "1\n",
      "AmItheAsshole\n",
      "86\n",
      "246\n",
      "42\n",
      "legaladvice\n",
      "6\n",
      "56\n",
      "1\n",
      "pcmasterrace\n",
      "2\n",
      "13\n",
      "1\n",
      "personalfinance\n",
      "20\n",
      "100\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## for each reader group pair: get 2 questions from same article\n",
    "reader_group_data = [\n",
    "    ('EXPERT', '<EXPERT_PCT_0_AUTHOR>', '<EXPERT_PCT_1_AUTHOR>'),\n",
    "    ('TIME', '<RESPONSE_TIME_0_AUTHOR>', '<RESPONSE_TIME_1_AUTHOR>'),\n",
    "    ('LOC', '<US_AUTHOR>', '<NONUS_AUTHOR>'),\n",
    "]\n",
    "# val_data_article_reader_groups = val_data.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "# article ID | reader group class | question | reader group type | subreddit\n",
    "sample_size = 20\n",
    "for subreddit_i, data_i in val_data.groupby('subreddit'):\n",
    "    print(subreddit_i)\n",
    "    article_reader_groups_i = data_i.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "    for reader_group_type_j, reader_group_1, reader_group_2 in reader_group_data:\n",
    "        articles_ids_j = article_reader_groups_i[article_reader_groups_i.apply(lambda x: reader_group_1 in x and reader_group_2 in x)]\n",
    "        print(len(articles_ids_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! It looks like `pcmasterrace` and `personalfinance`, which we were planning to use in evaluation, don't have great coverage of reader groups.\n",
    "\n",
    "Let's pivot to training data to improve coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('../../data/reddit_data/combined_data_train_data.pt')\n",
    "train_data = pd.DataFrame(list(train_data))\n",
    "data_cols = ['article_id', 'author_has_subreddit_embed', 'author_has_text_embed', 'reader_token_str', 'source_text', 'source_ids_reader_token', 'target_text', 'subreddit_embed', 'text_embed', 'attention_mask']\n",
    "train_data = train_data.loc[:, data_cols]\n",
    "# add subreddit info\n",
    "train_data = pd.merge(train_data, post_metadata, on=['article_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK                         1134850\n",
      "<EXPERT_PCT_0_AUTHOR>        290931\n",
      "<RESPONSE_TIME_0_AUTHOR>     223571\n",
      "<RESPONSE_TIME_1_AUTHOR>      79179\n",
      "<US_AUTHOR>                   29237\n",
      "<NONUS_AUTHOR>                21367\n",
      "<EXPERT_PCT_1_AUTHOR>         11819\n",
      "Name: reader_token_str, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.loc[:, 'reader_token_str'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advice\n",
      "group EXPERT has 66 articles\n",
      "group TIME has 201 articles\n",
      "group LOC has 8 articles\n",
      "AmItheAsshole\n",
      "group EXPERT has 270 articles\n",
      "group TIME has 946 articles\n",
      "group LOC has 122 articles\n",
      "legaladvice\n",
      "group EXPERT has 40 articles\n",
      "group TIME has 199 articles\n",
      "group LOC has 6 articles\n",
      "pcmasterrace\n",
      "group EXPERT has 19 articles\n",
      "group TIME has 59 articles\n",
      "group LOC has 0 articles\n",
      "personalfinance\n",
      "group EXPERT has 75 articles\n",
      "group TIME has 413 articles\n",
      "group LOC has 9 articles\n"
     ]
    }
   ],
   "source": [
    "for subreddit_i, data_i in train_data.groupby('subreddit'):\n",
    "    print(subreddit_i)\n",
    "    article_reader_groups_i = data_i.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "    for reader_group_type_j, reader_group_1, reader_group_2 in reader_group_data:\n",
    "        articles_ids_j = article_reader_groups_i[article_reader_groups_i.apply(lambda x: reader_group_1 in x and reader_group_2 in x)]\n",
    "        print(f'group {reader_group_type_j} has {len(articles_ids_j)} articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better! Now we can sample some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>question_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_2</th>\n",
       "      <th>group_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8kc90i</td>\n",
       "      <td>Advice</td>\n",
       "      <td>First off, I have no problem with weed because...</td>\n",
       "      <td>Is she trying to use the marijuana as a shortc...</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>but is it worth being caught?</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8l91i1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>I work online and last week my boss called and...</td>\n",
       "      <td>Now, how's that cold?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Did you thank him for been a good boss and why...</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8p4uwz</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Was in the midst of a panic attack over some t...</td>\n",
       "      <td>How would you feel if you or your family got a...</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Did any one see that you hit the car?</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8v8mga</td>\n",
       "      <td>Advice</td>\n",
       "      <td>In other words, I don't know who I am. I suck ...</td>\n",
       "      <td>Have you ever tried keeping a diary or journal?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Do you doubt yourself, your choices, etc.?</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8vm0a1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Hey, I’m 19 and a half and I’ve been out of hi...</td>\n",
       "      <td>What problem do you wish was solved in the world?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>It was very different from School or College, ...</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id subreddit                                               post  \\\n",
       "0     8kc90i    Advice  First off, I have no problem with weed because...   \n",
       "1     8l91i1    Advice  I work online and last week my boss called and...   \n",
       "2     8p4uwz    Advice  Was in the midst of a panic attack over some t...   \n",
       "3     8v8mga    Advice  In other words, I don't know who I am. I suck ...   \n",
       "4     8vm0a1    Advice  Hey, I’m 19 and a half and I’ve been out of hi...   \n",
       "\n",
       "                                          question_1                group_1  \\\n",
       "0  Is she trying to use the marijuana as a shortc...  <EXPERT_PCT_0_AUTHOR>   \n",
       "1                              Now, how's that cold?  <EXPERT_PCT_0_AUTHOR>   \n",
       "2  How would you feel if you or your family got a...  <EXPERT_PCT_0_AUTHOR>   \n",
       "3    Have you ever tried keeping a diary or journal?  <EXPERT_PCT_0_AUTHOR>   \n",
       "4  What problem do you wish was solved in the world?  <EXPERT_PCT_0_AUTHOR>   \n",
       "\n",
       "                                          question_2                group_2  \\\n",
       "0                      but is it worth being caught?  <EXPERT_PCT_1_AUTHOR>   \n",
       "1  Did you thank him for been a good boss and why...  <EXPERT_PCT_1_AUTHOR>   \n",
       "2              Did any one see that you hit the car?  <EXPERT_PCT_1_AUTHOR>   \n",
       "3         Do you doubt yourself, your choices, etc.?  <EXPERT_PCT_1_AUTHOR>   \n",
       "4  It was very different from School or College, ...  <EXPERT_PCT_1_AUTHOR>   \n",
       "\n",
       "  group_type  \n",
       "0     EXPERT  \n",
       "1     EXPERT  \n",
       "2     EXPERT  \n",
       "3     EXPERT  \n",
       "4     EXPERT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "sample_size = 100\n",
    "reader_group_sample_question_data = []\n",
    "for subreddit_i, data_i in train_data.groupby('subreddit'):\n",
    "    article_reader_groups_i = data_i.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "    for reader_group_type_j, reader_group_1, reader_group_2 in reader_group_data:\n",
    "        article_ids_j = article_reader_groups_i[article_reader_groups_i.apply(lambda x: reader_group_1 in x and reader_group_2 in x)].index.tolist()\n",
    "        # sample\n",
    "        if(len(article_ids_j) > sample_size):\n",
    "            article_ids_j = np.random.choice(article_ids_j, sample_size, replace=False)\n",
    "        # get paired reader group data for each article\n",
    "        for article_id_k in article_ids_j:\n",
    "            data_k = data_i[(data_i.loc[:, 'article_id']==article_id_k)]\n",
    "            sample_data_k_1 = data_k[data_k.loc[:, 'reader_token_str']==reader_group_1].iloc[0, :]\n",
    "            sample_data_k_2 = data_k[data_k.loc[:, 'reader_token_str']==reader_group_2].iloc[0, :]\n",
    "            post_text_k = data_k.loc[:, 'source_text'].iloc[0]\n",
    "            reader_group_sample_question_data.append([article_id_k, subreddit_i, post_text_k, sample_data_k_1.loc['target_text'], reader_group_1, sample_data_k_2.loc['target_text'], reader_group_2, reader_group_type_j])\n",
    "reader_group_sample_question_data = pd.DataFrame(reader_group_sample_question_data, \n",
    "                                                 columns=['article_id', 'subreddit', 'post', 'question_1', 'group_1', 'question_2', 'group_2', 'group_type'])\n",
    "display(reader_group_sample_question_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now! Let's generate questions for the same data using the reader-aware model, and organize to match the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "from test_question_generation import load_model\n",
    "model_cache_dir = '../../data/model_cache/'\n",
    "model_weight_file = '../../data/reddit_data/author_text_data/question_generation_model/checkpoint-114500/pytorch_model.bin'\n",
    "data_dir = '../../data/reddit_data/author_text_data/'\n",
    "model_type = 'bart_author'\n",
    "model, model_tokenizer = load_model(model_cache_dir, model_weight_file, model_type, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>reader_token_str</th>\n",
       "      <th>source_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1636010</th>\n",
       "      <td>7py853</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(154), tensor(7), tensor(5),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220202</th>\n",
       "      <td>7py853</td>\n",
       "      <td>&lt;US_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(154), tensor(7), tensor(5),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654293</th>\n",
       "      <td>7rkvv3</td>\n",
       "      <td>&lt;US_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(17), tensor(27), tensor(119...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199596</th>\n",
       "      <td>7rkvv3</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(17), tensor(27), tensor(119...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507009</th>\n",
       "      <td>814t9v</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(1141), tensor(6), tensor(81...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id reader_token_str  \\\n",
       "1636010     7py853   <NONUS_AUTHOR>   \n",
       "1220202     7py853      <US_AUTHOR>   \n",
       "1654293     7rkvv3      <US_AUTHOR>   \n",
       "1199596     7rkvv3   <NONUS_AUTHOR>   \n",
       "1507009     814t9v   <NONUS_AUTHOR>   \n",
       "\n",
       "                                                source_ids  \\\n",
       "1636010  [tensor(0), tensor(154), tensor(7), tensor(5),...   \n",
       "1220202  [tensor(0), tensor(154), tensor(7), tensor(5),...   \n",
       "1654293  [tensor(0), tensor(17), tensor(27), tensor(119...   \n",
       "1199596  [tensor(0), tensor(17), tensor(27), tensor(119...   \n",
       "1507009  [tensor(0), tensor(1141), tensor(6), tensor(81...   \n",
       "\n",
       "                                            attention_mask  \n",
       "1636010  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1220202  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1654293  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1199596  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1507009  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## subset data to articles and reader groups mentioned in sample data\n",
    "generation_data = train_data[train_data.loc[:, 'article_id'].isin(reader_group_sample_question_data.loc[:, 'article_id'].unique())].drop_duplicates(['article_id', 'reader_token_str'])\n",
    "article_id_valid_tokens = reader_group_sample_question_data.groupby('article_id').apply(lambda x: x.iloc[0, :].loc[['group_1', 'group_2']].values.tolist())\n",
    "generation_data = generation_data[generation_data.apply(lambda x: x.loc['reader_token_str'] in article_id_valid_tokens.loc[x.loc['article_id']], axis=1)]\n",
    "generation_data.sort_values('article_id', inplace=True)\n",
    "generation_data = generation_data.loc[:, ['article_id', 'reader_token_str', 'source_ids_reader_token', 'attention_mask']]\n",
    "generation_data.rename(columns={'source_ids_reader_token' : 'source_ids'}, inplace=True)\n",
    "generation_data.drop_duplicates(['article_id', 'reader_token_str'], inplace=True)\n",
    "print(generation_data.shape[0])\n",
    "# fix tensor vars\n",
    "generation_data = generation_data.assign(**{\n",
    "    'source_ids' : generation_data.loc[:, 'source_ids'].apply(lambda x: torch.LongTensor(x)),\n",
    "    'attention_mask' : generation_data.loc[:, 'attention_mask'].apply(lambda x: torch.LongTensor(x)),\n",
    "})\n",
    "# convert to list of dicts\n",
    "generation_data_iter = generation_data.apply(lambda x: x.to_dict(), axis=1).values.tolist()\n",
    "display(generation_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1634/1634 [07:34<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "## generate text for all source examples\n",
    "from model_helpers import generate_predictions\n",
    "generation_method = 'beam_search'\n",
    "num_beams = 8\n",
    "pred_text = generate_predictions(model, generation_data_iter, model_tokenizer, generation_method=generation_method, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 generated pairs total\n"
     ]
    }
   ],
   "source": [
    "## re-add to generation data\n",
    "generation_pred_data = generation_data.assign(**{\n",
    "    'pred_text' : pred_text\n",
    "})\n",
    "## reorganize to match original sampled data\n",
    "reader_token_groups = {\n",
    "    'LOC' : ['<US_AUTHOR>', '<NONUS_AUTHOR>'],\n",
    "    'EXPERT' : ['<EXPERT_PCT_0_AUTHOR>', '<EXPERT_PCT_1_AUTHOR>'],\n",
    "    'TIME' : ['<RESPONSE_TIME_0_AUTHOR>', '<RESPONSE_TIME_1_AUTHOR>'],\n",
    "}\n",
    "reader_token_group_lookup = {v1 : k for k,v in reader_token_groups.items() for v1 in v}\n",
    "def flatten_pred_data(data, reader_token_group_lookup):\n",
    "    reader_group_type = reader_token_group_lookup[data.loc[:, 'reader_token_str'].iloc[0]]\n",
    "    data_1 = data.iloc[0, :]\n",
    "    data_2 = data.iloc[1, :]\n",
    "    flat_data = [data_1.loc['pred_text'], data_1.loc['reader_token_str'], \n",
    "                 data_2.loc['pred_text'], data_2.loc['reader_token_str'],\n",
    "                 reader_group_type]\n",
    "    flat_data_cols = ['question_1', 'group_1', 'question_2', 'group_2', 'group_type']\n",
    "    flat_data = pd.Series(flat_data, index=flat_data_cols)\n",
    "    return flat_data\n",
    "per_article_generation_pred_data = generation_pred_data.groupby('article_id').apply(lambda x: flatten_pred_data(x, reader_token_group_lookup)).reset_index()\n",
    "# remove duplicates\n",
    "per_article_generation_pred_data = per_article_generation_pred_data[per_article_generation_pred_data.loc[:, 'question_1']!=per_article_generation_pred_data.loc[:, 'question_2']]\n",
    "## join with metadata\n",
    "per_article_generation_pred_data = pd.merge(per_article_generation_pred_data, reader_group_sample_question_data.loc[:, ['article_id', 'subreddit', 'post']].drop_duplicates('article_id'), on='article_id', how='left')\n",
    "print(f'{per_article_generation_pred_data.shape[0]} generated pairs total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the real and generated data, let's shuffle the groups to prepare for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_questions_by_group(data, num_groups, group_vars=['question', 'group']):\n",
    "    ordered_group_cols = [[f'{var}_{i}' for var in group_vars] for i in range(1, num_groups+1)]\n",
    "    group_cols = list(ordered_group_cols)\n",
    "    np.random.shuffle(group_cols)\n",
    "    flat_group_cols = [y for x in group_cols for y in x]\n",
    "    flat_ordered_group_cols = [y for x in ordered_group_cols for y in x]\n",
    "    group_data = data.loc[flat_group_cols]\n",
    "    group_data.index = flat_ordered_group_cols\n",
    "    data.drop(flat_ordered_group_cols, inplace=True)\n",
    "    data = data.append(group_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7py853</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>Turning to the experts at reddit!  I'm in a ba...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Are you sure it's not a scam and not a identit...</td>\n",
       "      <td>Are you sure it's not a scam?</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814t9v</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>My wife, over a year ago, thought she submitte...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Did she file for deferment?</td>\n",
       "      <td>Did you send her a certified letter or just a ...</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89p14v</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Every Tuesday my neighbours garbage goes flyin...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Are you sure it wasn’t your mom’s personal junk?</td>\n",
       "      <td>How did you get your garbage there?</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8c71q3</td>\n",
       "      <td>Advice</td>\n",
       "      <td>My ex girlfriend is very suicidal. We broke up...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>How long have you been with her?</td>\n",
       "      <td>How long have you been with this person?</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8kc90i</td>\n",
       "      <td>Advice</td>\n",
       "      <td>First off, I have no problem with weed because...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Is she trying to use the marijuana for feeling...</td>\n",
       "      <td>What is her motivation behind it?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id        subreddit  \\\n",
       "0     7py853      legaladvice   \n",
       "1     814t9v  personalfinance   \n",
       "2     89p14v           Advice   \n",
       "3     8c71q3           Advice   \n",
       "4     8kc90i           Advice   \n",
       "\n",
       "                                                post group_type  \\\n",
       "0  Turning to the experts at reddit!  I'm in a ba...        LOC   \n",
       "1  My wife, over a year ago, thought she submitte...        LOC   \n",
       "2  Every Tuesday my neighbours garbage goes flyin...        LOC   \n",
       "3  My ex girlfriend is very suicidal. We broke up...        LOC   \n",
       "4  First off, I have no problem with weed because...     EXPERT   \n",
       "\n",
       "                                          question_1  \\\n",
       "0  Are you sure it's not a scam and not a identit...   \n",
       "1                        Did she file for deferment?   \n",
       "2   Are you sure it wasn’t your mom’s personal junk?   \n",
       "3                   How long have you been with her?   \n",
       "4  Is she trying to use the marijuana for feeling...   \n",
       "\n",
       "                                          question_2 group_type_choices  label  \n",
       "0                      Are you sure it's not a scam?      US vs. non-US     -1  \n",
       "1  Did you send her a certified letter or just a ...      US vs. non-US     -1  \n",
       "2                How did you get your garbage there?      US vs. non-US     -1  \n",
       "3           How long have you been with this person?      US vs. non-US     -1  \n",
       "4                  What is her motivation behind it?  expert vs. novice     -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8kc90i</td>\n",
       "      <td>Advice</td>\n",
       "      <td>First off, I have no problem with weed because...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Is she trying to use the marijuana as a shortc...</td>\n",
       "      <td>but is it worth being caught?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8l91i1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>I work online and last week my boss called and...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Did you thank him for been a good boss and why...</td>\n",
       "      <td>Now, how's that cold?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8p4uwz</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Was in the midst of a panic attack over some t...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>How would you feel if you or your family got a...</td>\n",
       "      <td>Did any one see that you hit the car?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8v8mga</td>\n",
       "      <td>Advice</td>\n",
       "      <td>In other words, I don't know who I am. I suck ...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Do you doubt yourself, your choices, etc.?</td>\n",
       "      <td>Have you ever tried keeping a diary or journal?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8vm0a1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Hey, I’m 19 and a half and I’ve been out of hi...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>What problem do you wish was solved in the world?</td>\n",
       "      <td>It was very different from School or College, ...</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id subreddit                                               post  \\\n",
       "0     8kc90i    Advice  First off, I have no problem with weed because...   \n",
       "1     8l91i1    Advice  I work online and last week my boss called and...   \n",
       "2     8p4uwz    Advice  Was in the midst of a panic attack over some t...   \n",
       "3     8v8mga    Advice  In other words, I don't know who I am. I suck ...   \n",
       "4     8vm0a1    Advice  Hey, I’m 19 and a half and I’ve been out of hi...   \n",
       "\n",
       "  group_type                                         question_1  \\\n",
       "0     EXPERT  Is she trying to use the marijuana as a shortc...   \n",
       "1     EXPERT  Did you thank him for been a good boss and why...   \n",
       "2     EXPERT  How would you feel if you or your family got a...   \n",
       "3     EXPERT         Do you doubt yourself, your choices, etc.?   \n",
       "4     EXPERT  What problem do you wish was solved in the world?   \n",
       "\n",
       "                                          question_2 group_type_choices  label  \n",
       "0                      but is it worth being caught?  expert vs. novice     -1  \n",
       "1                              Now, how's that cold?  expert vs. novice     -1  \n",
       "2              Did any one see that you hit the car?  expert vs. novice     -1  \n",
       "3    Have you ever tried keeping a diary or journal?  expert vs. novice     -1  \n",
       "4  It was very different from School or College, ...  expert vs. novice     -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_groups = 2\n",
    "shuffled_generation_pred_data = per_article_generation_pred_data.apply(lambda x: shuffle_questions_by_group(x, num_groups), axis=1)\n",
    "shuffled_reader_group_sample_question_data = reader_group_sample_question_data.apply(lambda x: shuffle_questions_by_group(x, num_groups), axis=1)\n",
    "# align columns lol\n",
    "shuffled_generation_pred_data = shuffled_generation_pred_data.loc[:, shuffled_reader_group_sample_question_data.columns]\n",
    "## prepare for annotation\n",
    "reader_group_type_choices = pd.DataFrame([\n",
    "    ['EXPERT', 'expert vs. novice'],\n",
    "    ['TIME', 'fast response vs. slow response'],\n",
    "    ['LOC', 'US vs. non-US'],\n",
    "], columns=['group_type', 'group_type_choices'])\n",
    "def prepare_for_annotation(data, reader_group_type_choices):\n",
    "    # drop labels\n",
    "    annotation_data = data.drop(['group_1', 'group_2'], axis=1)\n",
    "    # add choices\n",
    "    annotation_data = pd.merge(annotation_data, reader_group_type_choices, on=['group_type'], how='left')\n",
    "    annotation_data = annotation_data.assign(**{'label' : -1})\n",
    "    return annotation_data\n",
    "annotation_generation_pred_data = prepare_for_annotation(shuffled_generation_pred_data, reader_group_type_choices)\n",
    "annotation_reader_group_sample_question_data = prepare_for_annotation(shuffled_reader_group_sample_question_data, reader_group_type_choices)\n",
    "display(annotation_generation_pred_data.head())\n",
    "display(annotation_reader_group_sample_question_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmItheAsshole      300\n",
       "personalfinance    184\n",
       "Advice             174\n",
       "legaladvice        146\n",
       "pcmasterrace        78\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AmItheAsshole      87\n",
       "personalfinance    59\n",
       "Advice             55\n",
       "legaladvice        41\n",
       "pcmasterrace       18\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check subreddit distribution\n",
    "display(annotation_reader_group_sample_question_data.loc[:, 'subreddit'].value_counts())\n",
    "display(annotation_generation_pred_data.loc[:, 'subreddit'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-124-c260bb5366e4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  annotation_reader_group_sample_question_data.sort_values(['subreddit', 'article_id'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## restrict to same posts\n",
    "annotation_reader_group_sample_question_data = annotation_reader_group_sample_question_data[annotation_reader_group_sample_question_data.loc[:, 'article_id'].isin(annotation_generation_pred_data.loc[:, 'article_id'].unique())]\n",
    "## sort\n",
    "annotation_reader_group_sample_question_data.sort_values(['subreddit', 'article_id'], inplace=True)\n",
    "annotation_generation_pred_data.sort_values(['subreddit', 'article_id'], inplace=True)\n",
    "print(annotation_reader_group_sample_question_data.shape[0])\n",
    "print(annotation_generation_pred_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add real/generated labels\n",
    "shuffled_reader_group_sample_question_data = shuffled_reader_group_sample_question_data.assign(**{\n",
    "    'question_type' : 'real',\n",
    "})\n",
    "shuffled_generation_pred_data = shuffled_generation_pred_data.assign(**{\n",
    "    'question_type' : 'author_token_model',\n",
    "})\n",
    "## combine\n",
    "combined_ground_truth_data = pd.concat([\n",
    "    shuffled_reader_group_sample_question_data,\n",
    "    shuffled_generation_pred_data\n",
    "], axis=0)\n",
    "combined_annotation_data = pd.concat([\n",
    "    annotation_reader_group_sample_question_data,\n",
    "    annotation_generation_pred_data,\n",
    "])\n",
    "## sort\n",
    "combined_annotation_data.sort_values(['article_id', 'subreddit', 'group_type_choices'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ground truth data\n",
    "combined_ground_truth_data.to_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/reader_group_ground_truth_data.tsv', sep='\\t', index=False)\n",
    "# write annotation data\n",
    "combined_annotation_data.to_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/reader_group_annotation_data.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect generated text for evaluation data\n",
    "We need to collect text generated by the model for the following two evaluation tasks:\n",
    "\n",
    "1. Is reader-aware text (1) more relevant and (2) more likely to elicit interesting information from the author, than non-aware text?\n",
    "2. Is it just as easy to differentiate different reader groups in the generated text as it is for the real text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51302, 8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "val_data = torch.load('../../data/reddit_data/combined_data_test_data.pt', map_location='cpu')\n",
    "# convert to data frame because easier\n",
    "import pandas as pd\n",
    "val_data = pd.DataFrame(list(val_data))\n",
    "data_cols = ['article_id', 'author_has_subreddit_embed', 'author_has_text_embed', 'reader_token_str', 'source_text', 'target_text', 'subreddit_embed', 'text_embed']\n",
    "val_data = val_data.loc[:, data_cols]\n",
    "print(val_data.shape)\n",
    "# get metadata\n",
    "import pandas as pd\n",
    "post_metadata = pd.read_csv('../../data/reddit_data/combined_data_question_data.gz', sep='\\t', compression='gzip', usecols=['article_id', 'subreddit'])\n",
    "# add subreddit info\n",
    "val_data = pd.merge(val_data, post_metadata, on=['article_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for no-reader and reader-aware generated questions\n",
    "After generating text with `test_question_generation.py`, we can filter for questions generated by no-reader and reader-aware models (additional condition: question should have some reader information attached)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "no_reader_pred_text = list(map(lambda x: x.strip(), gzip.open('../../data/reddit_data/text_only_model/test_data_output_text.gz', 'rt')))\n",
    "reader_aware_pred_text = list(map(lambda x: x.strip(), gzip.open('../../data/reddit_data/author_text_data/test_data_output_text.gz', 'rt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25495/51302 output that are different across models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>reader_token_str</th>\n",
       "      <th>source_text</th>\n",
       "      <th>text_only_model_pred_text</th>\n",
       "      <th>reader_model_pred_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22989</th>\n",
       "      <td>bc5yeu</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>My sister is 34 and I'm 40, but since our earl...</td>\n",
       "      <td>Why did you even make this post?</td>\n",
       "      <td>Why would you do that to someone you're overre...</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22991</th>\n",
       "      <td>cbjgh5</td>\n",
       "      <td>&lt;RESPONSE_TIME_1_AUTHOR&gt;</td>\n",
       "      <td>Is it considered retaliation on my landlords p...</td>\n",
       "      <td>Do you have a lease or are you month to month?</td>\n",
       "      <td>Whose name is on the lease?</td>\n",
       "      <td>legaladvice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22992</th>\n",
       "      <td>a9wlwo</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Like, I’m talking, age 30, at least 20 yrs of ...</td>\n",
       "      <td>Do you have an adult in your life that you tru...</td>\n",
       "      <td>Do you have an adult in your life that you tru...</td>\n",
       "      <td>Advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22994</th>\n",
       "      <td>98f4o1</td>\n",
       "      <td>&lt;US_AUTHOR&gt;</td>\n",
       "      <td>I’m hoping someone can give me advice on how t...</td>\n",
       "      <td>What is their income?</td>\n",
       "      <td>Do they have home insurance?</td>\n",
       "      <td>personalfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22995</th>\n",
       "      <td>98f4o1</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I’m hoping someone can give me advice on how t...</td>\n",
       "      <td>What is their income?</td>\n",
       "      <td>Do they have home insurance?</td>\n",
       "      <td>personalfinance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id          reader_token_str  \\\n",
       "22989     bc5yeu            <NONUS_AUTHOR>   \n",
       "22991     cbjgh5  <RESPONSE_TIME_1_AUTHOR>   \n",
       "22992     a9wlwo     <EXPERT_PCT_0_AUTHOR>   \n",
       "22994     98f4o1               <US_AUTHOR>   \n",
       "22995     98f4o1     <EXPERT_PCT_0_AUTHOR>   \n",
       "\n",
       "                                             source_text  \\\n",
       "22989  My sister is 34 and I'm 40, but since our earl...   \n",
       "22991  Is it considered retaliation on my landlords p...   \n",
       "22992  Like, I’m talking, age 30, at least 20 yrs of ...   \n",
       "22994  I’m hoping someone can give me advice on how t...   \n",
       "22995  I’m hoping someone can give me advice on how t...   \n",
       "\n",
       "                               text_only_model_pred_text  \\\n",
       "22989                   Why did you even make this post?   \n",
       "22991     Do you have a lease or are you month to month?   \n",
       "22992  Do you have an adult in your life that you tru...   \n",
       "22994                              What is their income?   \n",
       "22995                              What is their income?   \n",
       "\n",
       "                                  reader_model_pred_text        subreddit  \n",
       "22989  Why would you do that to someone you're overre...    AmItheAsshole  \n",
       "22991                        Whose name is on the lease?      legaladvice  \n",
       "22992  Do you have an adult in your life that you tru...           Advice  \n",
       "22994                       Do they have home insurance?  personalfinance  \n",
       "22995                       Do they have home insurance?  personalfinance  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "test_data = torch.load('../../data/reddit_data/combined_data_val_data.pt')\n",
    "# convert to dataframe\n",
    "test_data = test_data.data.to_pandas()\n",
    "test_data = test_data.loc[:, ['article_id', 'reader_token_str', 'source_text']]\n",
    "# add predicted text\n",
    "test_data = test_data.assign(**{\n",
    "    'text_only_model_pred_text' : no_reader_pred_text,\n",
    "    'reader_model_pred_text' : reader_aware_pred_text,\n",
    "})\n",
    "## add subreddit data\n",
    "import pandas as pd\n",
    "submission_data = pd.read_csv('../../data/reddit_data/subreddit_submissions_2018-01_2019-12.gz', sep='\\t', compression='gzip', usecols=['id', 'subreddit'])\n",
    "submission_data.rename(columns={'id' : 'article_id'}, inplace=True)\n",
    "test_data = pd.merge(test_data, submission_data, on='article_id', how='left')\n",
    "## limit to data where output is different!!\n",
    "output_diff_test_data = test_data[test_data.loc[:, 'text_only_model_pred_text']!=test_data.loc[:, 'reader_model_pred_text']]\n",
    "print(f'{output_diff_test_data.shape[0]}/{test_data.shape[0]} output that are different across models')\n",
    "display(output_diff_test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 sample data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>reader_token_str</th>\n",
       "      <th>source_text</th>\n",
       "      <th>text_only_model_pred_text</th>\n",
       "      <th>reader_model_pred_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23949</th>\n",
       "      <td>8phwx3</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I’m 30, have a well-paying job in Silicon Vall...</td>\n",
       "      <td>And how would I feel if I did the same?</td>\n",
       "      <td>Do I like the taste of the relationship?</td>\n",
       "      <td>Advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26453</th>\n",
       "      <td>8lygi5</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I'm on a trip currently, and my boyfriend and ...</td>\n",
       "      <td>Do you have any idea what the issue is?</td>\n",
       "      <td>Do you have any pictures or video of the trip?</td>\n",
       "      <td>Advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37479</th>\n",
       "      <td>8oxseg</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>well there is a female i know, like im not fri...</td>\n",
       "      <td>Did you ever ask her why she dislikes you?</td>\n",
       "      <td>How does she react when you don't invite her?</td>\n",
       "      <td>Advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38656</th>\n",
       "      <td>b0ycks</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>So i have recently started writing a book and ...</td>\n",
       "      <td>What book is it?</td>\n",
       "      <td>Does the book give any information about what ...</td>\n",
       "      <td>Advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42544</th>\n",
       "      <td>d1gl4d</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I’m not sure what exactly I’m asking for by po...</td>\n",
       "      <td>How can people say that a rape victim?</td>\n",
       "      <td>Have you tried talking to a counselor about this?</td>\n",
       "      <td>Advice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id       reader_token_str  \\\n",
       "23949     8phwx3  <EXPERT_PCT_0_AUTHOR>   \n",
       "26453     8lygi5  <EXPERT_PCT_0_AUTHOR>   \n",
       "37479     8oxseg  <EXPERT_PCT_0_AUTHOR>   \n",
       "38656     b0ycks  <EXPERT_PCT_0_AUTHOR>   \n",
       "42544     d1gl4d  <EXPERT_PCT_0_AUTHOR>   \n",
       "\n",
       "                                             source_text  \\\n",
       "23949  I’m 30, have a well-paying job in Silicon Vall...   \n",
       "26453  I'm on a trip currently, and my boyfriend and ...   \n",
       "37479  well there is a female i know, like im not fri...   \n",
       "38656  So i have recently started writing a book and ...   \n",
       "42544  I’m not sure what exactly I’m asking for by po...   \n",
       "\n",
       "                        text_only_model_pred_text  \\\n",
       "23949     And how would I feel if I did the same?   \n",
       "26453     Do you have any idea what the issue is?   \n",
       "37479  Did you ever ask her why she dislikes you?   \n",
       "38656                            What book is it?   \n",
       "42544      How can people say that a rape victim?   \n",
       "\n",
       "                                  reader_model_pred_text subreddit  \n",
       "23949           Do I like the taste of the relationship?    Advice  \n",
       "26453     Do you have any pictures or video of the trip?    Advice  \n",
       "37479      How does she react when you don't invite her?    Advice  \n",
       "38656  Does the book give any information about what ...    Advice  \n",
       "42544  Have you tried talking to a counselor about this?    Advice  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>reader_token_str</th>\n",
       "      <th>post</th>\n",
       "      <th>text_only_model_pred_text</th>\n",
       "      <th>reader_model_pred_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text_1</th>\n",
       "      <th>system_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>system_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23949</th>\n",
       "      <td>8phwx3</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I’m 30, have a well-paying job in Silicon Vall...</td>\n",
       "      <td>And how would I feel if I did the same?</td>\n",
       "      <td>Do I like the taste of the relationship?</td>\n",
       "      <td>Advice</td>\n",
       "      <td>And how would I feel if I did the same?</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>Do I like the taste of the relationship?</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26453</th>\n",
       "      <td>8lygi5</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I'm on a trip currently, and my boyfriend and ...</td>\n",
       "      <td>Do you have any idea what the issue is?</td>\n",
       "      <td>Do you have any pictures or video of the trip?</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Do you have any pictures or video of the trip?</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>Do you have any idea what the issue is?</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37479</th>\n",
       "      <td>8oxseg</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>well there is a female i know, like im not fri...</td>\n",
       "      <td>Did you ever ask her why she dislikes you?</td>\n",
       "      <td>How does she react when you don't invite her?</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Did you ever ask her why she dislikes you?</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>How does she react when you don't invite her?</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38656</th>\n",
       "      <td>b0ycks</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>So i have recently started writing a book and ...</td>\n",
       "      <td>What book is it?</td>\n",
       "      <td>Does the book give any information about what ...</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Does the book give any information about what ...</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>What book is it?</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42544</th>\n",
       "      <td>d1gl4d</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>I’m not sure what exactly I’m asking for by po...</td>\n",
       "      <td>How can people say that a rape victim?</td>\n",
       "      <td>Have you tried talking to a counselor about this?</td>\n",
       "      <td>Advice</td>\n",
       "      <td>How can people say that a rape victim?</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>Have you tried talking to a counselor about this?</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id       reader_token_str  \\\n",
       "23949     8phwx3  <EXPERT_PCT_0_AUTHOR>   \n",
       "26453     8lygi5  <EXPERT_PCT_0_AUTHOR>   \n",
       "37479     8oxseg  <EXPERT_PCT_0_AUTHOR>   \n",
       "38656     b0ycks  <EXPERT_PCT_0_AUTHOR>   \n",
       "42544     d1gl4d  <EXPERT_PCT_0_AUTHOR>   \n",
       "\n",
       "                                                    post  \\\n",
       "23949  I’m 30, have a well-paying job in Silicon Vall...   \n",
       "26453  I'm on a trip currently, and my boyfriend and ...   \n",
       "37479  well there is a female i know, like im not fri...   \n",
       "38656  So i have recently started writing a book and ...   \n",
       "42544  I’m not sure what exactly I’m asking for by po...   \n",
       "\n",
       "                        text_only_model_pred_text  \\\n",
       "23949     And how would I feel if I did the same?   \n",
       "26453     Do you have any idea what the issue is?   \n",
       "37479  Did you ever ask her why she dislikes you?   \n",
       "38656                            What book is it?   \n",
       "42544      How can people say that a rape victim?   \n",
       "\n",
       "                                  reader_model_pred_text subreddit  \\\n",
       "23949           Do I like the taste of the relationship?    Advice   \n",
       "26453     Do you have any pictures or video of the trip?    Advice   \n",
       "37479      How does she react when you don't invite her?    Advice   \n",
       "38656  Does the book give any information about what ...    Advice   \n",
       "42544  Have you tried talking to a counselor about this?    Advice   \n",
       "\n",
       "                                                  text_1  \\\n",
       "23949            And how would I feel if I did the same?   \n",
       "26453     Do you have any pictures or video of the trip?   \n",
       "37479         Did you ever ask her why she dislikes you?   \n",
       "38656  Does the book give any information about what ...   \n",
       "42544             How can people say that a rape victim?   \n",
       "\n",
       "                        system_1  \\\n",
       "23949  text_only_model_pred_text   \n",
       "26453     reader_model_pred_text   \n",
       "37479  text_only_model_pred_text   \n",
       "38656     reader_model_pred_text   \n",
       "42544  text_only_model_pred_text   \n",
       "\n",
       "                                                  text_2  \\\n",
       "23949           Do I like the taste of the relationship?   \n",
       "26453            Do you have any idea what the issue is?   \n",
       "37479      How does she react when you don't invite her?   \n",
       "38656                                   What book is it?   \n",
       "42544  Have you tried talking to a counselor about this?   \n",
       "\n",
       "                        system_2  \n",
       "23949     reader_model_pred_text  \n",
       "26453  text_only_model_pred_text  \n",
       "37479     reader_model_pred_text  \n",
       "38656  text_only_model_pred_text  \n",
       "42544     reader_model_pred_text  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## get sample from different subreddits\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "sample_output_data = []\n",
    "samples_per_reader_group = 5\n",
    "for subreddit_i, data_i in output_diff_test_data.groupby('subreddit'):\n",
    "    # remove UNK readers\n",
    "    data_i = data_i[data_i.loc[:, 'reader_token_str']!='UNK']\n",
    "    for reader_token_j, data_j in data_i.groupby('reader_token_str'):\n",
    "        article_ids_j = np.random.choice(data_j.loc[:, 'article_id'].unique(), samples_per_reader_group, replace=False)\n",
    "        sample_data_j = data_j[data_j.loc[:, 'article_id'].isin(article_ids_j)].drop_duplicates('article_id')\n",
    "        sample_output_data.append(sample_data_j)\n",
    "sample_output_data = pd.concat(sample_output_data, axis=0)\n",
    "print(f'{sample_output_data.shape[0]} sample data')\n",
    "display(sample_output_data.head())\n",
    "# ## save to file\n",
    "# sample_output_data.to_csv('../../data/reddit_data/annotation_data/text_quality_ground_truth_data.tsv', sep='\\t', index=False)\n",
    "## shuffle labels, rewrite as annotator data\n",
    "annotator_sample_data = sample_output_data.copy()\n",
    "annotator_sample_data.rename(columns={'source_text' : 'post'}, inplace=True)\n",
    "pred_text_cols = ['text_only_model_pred_text', 'reader_model_pred_text']\n",
    "N_pred_text = len(pred_text_cols)\n",
    "sample_pred_shuffled_idx = annotator_sample_data.loc[:, pred_text_cols].apply(lambda x: np.random.choice(list(range(N_pred_text)), N_pred_text, replace=False), axis=1)\n",
    "sample_pred_shuffled_text_cols = sample_pred_shuffled_idx.apply(lambda x: [pred_text_cols[idx] for idx in x])\n",
    "for i in range(N_pred_text):\n",
    "    annotator_sample_data = annotator_sample_data.assign(**{\n",
    "        f'question_{i+1}' : list(map(lambda x: annotator_sample_data.iloc[x[0], :].loc[x[1][i]], enumerate(sample_pred_shuffled_text_cols.values))),\n",
    "        f'system_{i+1}' : list(map(lambda x: x[i], sample_pred_shuffled_text_cols))\n",
    "    })\n",
    "display(annotator_sample_data.head())\n",
    "## save ground-truth to file\n",
    "annotator_sample_data.to_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/text_quality_ground_truth_data.tsv', sep='\\t', index=False)\n",
    "## remove labels lol\n",
    "system_cols = [f'system_{i+1}' for i in range(N_pred_text)]\n",
    "unlabeled_sample_data = annotator_sample_data.drop(system_cols + ['reader_token_str'] + pred_text_cols, axis=1)\n",
    "unlabeled_sample_data = unlabeled_sample_data.assign(**{\n",
    "    'text_that_makes_more_sense' : -1,\n",
    "    'text_that_is_more_fluent' : -1,\n",
    "    'text_that_is_more_likely_helpful' : -1,\n",
    "})\n",
    "unlabeled_sample_data.to_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/text_quality_annotation_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for real and generated reader-aware questions\n",
    "Let's get real and generated text for reader-aware questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advice\n",
      "15\n",
      "38\n",
      "1\n",
      "AmItheAsshole\n",
      "86\n",
      "246\n",
      "42\n",
      "legaladvice\n",
      "6\n",
      "56\n",
      "1\n",
      "pcmasterrace\n",
      "2\n",
      "13\n",
      "1\n",
      "personalfinance\n",
      "20\n",
      "100\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## for each reader group pair: get 2 questions from same article\n",
    "reader_group_data = [\n",
    "    ('EXPERT', '<EXPERT_PCT_0_AUTHOR>', '<EXPERT_PCT_1_AUTHOR>'),\n",
    "    ('TIME', '<RESPONSE_TIME_0_AUTHOR>', '<RESPONSE_TIME_1_AUTHOR>'),\n",
    "    ('LOC', '<US_AUTHOR>', '<NONUS_AUTHOR>'),\n",
    "]\n",
    "# val_data_article_reader_groups = val_data.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "# article ID | reader group class | question | reader group type | subreddit\n",
    "sample_size = 20\n",
    "for subreddit_i, data_i in val_data.groupby('subreddit'):\n",
    "    print(subreddit_i)\n",
    "    article_reader_groups_i = data_i.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "    for reader_group_type_j, reader_group_1, reader_group_2 in reader_group_data:\n",
    "        articles_ids_j = article_reader_groups_i[article_reader_groups_i.apply(lambda x: reader_group_1 in x and reader_group_2 in x)]\n",
    "        print(len(articles_ids_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! It looks like `pcmasterrace` and `personalfinance`, which we were planning to use in evaluation, don't have great coverage of reader groups.\n",
    "\n",
    "Let's pivot to training data to improve coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('../../data/reddit_data/combined_data_train_data.pt')\n",
    "train_data = pd.DataFrame(list(train_data))\n",
    "data_cols = ['article_id', 'author_has_subreddit_embed', 'author_has_text_embed', 'reader_token_str', 'source_text', 'source_ids_reader_token', 'target_text', 'subreddit_embed', 'text_embed', 'attention_mask']\n",
    "train_data = train_data.loc[:, data_cols]\n",
    "# add subreddit info\n",
    "train_data = pd.merge(train_data, post_metadata, on=['article_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK                         1134850\n",
      "<EXPERT_PCT_0_AUTHOR>        290931\n",
      "<RESPONSE_TIME_0_AUTHOR>     223571\n",
      "<RESPONSE_TIME_1_AUTHOR>      79179\n",
      "<US_AUTHOR>                   29237\n",
      "<NONUS_AUTHOR>                21367\n",
      "<EXPERT_PCT_1_AUTHOR>         11819\n",
      "Name: reader_token_str, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.loc[:, 'reader_token_str'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advice\n",
      "group EXPERT has 66 articles\n",
      "group TIME has 201 articles\n",
      "group LOC has 8 articles\n",
      "AmItheAsshole\n",
      "group EXPERT has 270 articles\n",
      "group TIME has 946 articles\n",
      "group LOC has 122 articles\n",
      "legaladvice\n",
      "group EXPERT has 40 articles\n",
      "group TIME has 199 articles\n",
      "group LOC has 6 articles\n",
      "pcmasterrace\n",
      "group EXPERT has 19 articles\n",
      "group TIME has 59 articles\n",
      "group LOC has 0 articles\n",
      "personalfinance\n",
      "group EXPERT has 75 articles\n",
      "group TIME has 413 articles\n",
      "group LOC has 9 articles\n"
     ]
    }
   ],
   "source": [
    "for subreddit_i, data_i in train_data.groupby('subreddit'):\n",
    "    print(subreddit_i)\n",
    "    article_reader_groups_i = data_i.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "    for reader_group_type_j, reader_group_1, reader_group_2 in reader_group_data:\n",
    "        articles_ids_j = article_reader_groups_i[article_reader_groups_i.apply(lambda x: reader_group_1 in x and reader_group_2 in x)]\n",
    "        print(f'group {reader_group_type_j} has {len(articles_ids_j)} articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better! Now we can sample some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>question_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_2</th>\n",
       "      <th>group_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8kc90i</td>\n",
       "      <td>Advice</td>\n",
       "      <td>First off, I have no problem with weed because...</td>\n",
       "      <td>Is she trying to use the marijuana as a shortc...</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>but is it worth being caught?</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8l91i1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>I work online and last week my boss called and...</td>\n",
       "      <td>Now, how's that cold?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Did you thank him for been a good boss and why...</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8p4uwz</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Was in the midst of a panic attack over some t...</td>\n",
       "      <td>How would you feel if you or your family got a...</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Did any one see that you hit the car?</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8v8mga</td>\n",
       "      <td>Advice</td>\n",
       "      <td>In other words, I don't know who I am. I suck ...</td>\n",
       "      <td>Have you ever tried keeping a diary or journal?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>Do you doubt yourself, your choices, etc.?</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8vm0a1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Hey, I’m 19 and a half and I’ve been out of hi...</td>\n",
       "      <td>What problem do you wish was solved in the world?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>It was very different from School or College, ...</td>\n",
       "      <td>&lt;EXPERT_PCT_1_AUTHOR&gt;</td>\n",
       "      <td>EXPERT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id subreddit                                               post  \\\n",
       "0     8kc90i    Advice  First off, I have no problem with weed because...   \n",
       "1     8l91i1    Advice  I work online and last week my boss called and...   \n",
       "2     8p4uwz    Advice  Was in the midst of a panic attack over some t...   \n",
       "3     8v8mga    Advice  In other words, I don't know who I am. I suck ...   \n",
       "4     8vm0a1    Advice  Hey, I’m 19 and a half and I’ve been out of hi...   \n",
       "\n",
       "                                          question_1                group_1  \\\n",
       "0  Is she trying to use the marijuana as a shortc...  <EXPERT_PCT_0_AUTHOR>   \n",
       "1                              Now, how's that cold?  <EXPERT_PCT_0_AUTHOR>   \n",
       "2  How would you feel if you or your family got a...  <EXPERT_PCT_0_AUTHOR>   \n",
       "3    Have you ever tried keeping a diary or journal?  <EXPERT_PCT_0_AUTHOR>   \n",
       "4  What problem do you wish was solved in the world?  <EXPERT_PCT_0_AUTHOR>   \n",
       "\n",
       "                                          question_2                group_2  \\\n",
       "0                      but is it worth being caught?  <EXPERT_PCT_1_AUTHOR>   \n",
       "1  Did you thank him for been a good boss and why...  <EXPERT_PCT_1_AUTHOR>   \n",
       "2              Did any one see that you hit the car?  <EXPERT_PCT_1_AUTHOR>   \n",
       "3         Do you doubt yourself, your choices, etc.?  <EXPERT_PCT_1_AUTHOR>   \n",
       "4  It was very different from School or College, ...  <EXPERT_PCT_1_AUTHOR>   \n",
       "\n",
       "  group_type  \n",
       "0     EXPERT  \n",
       "1     EXPERT  \n",
       "2     EXPERT  \n",
       "3     EXPERT  \n",
       "4     EXPERT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "sample_size = 100\n",
    "reader_group_sample_question_data = []\n",
    "for subreddit_i, data_i in train_data.groupby('subreddit'):\n",
    "    article_reader_groups_i = data_i.groupby('article_id').apply(lambda x: set(x.loc[:, 'reader_token_str'].unique()))\n",
    "    for reader_group_type_j, reader_group_1, reader_group_2 in reader_group_data:\n",
    "        article_ids_j = article_reader_groups_i[article_reader_groups_i.apply(lambda x: reader_group_1 in x and reader_group_2 in x)].index.tolist()\n",
    "        # sample\n",
    "        if(len(article_ids_j) > sample_size):\n",
    "            article_ids_j = np.random.choice(article_ids_j, sample_size, replace=False)\n",
    "        # get paired reader group data for each article\n",
    "        for article_id_k in article_ids_j:\n",
    "            data_k = data_i[(data_i.loc[:, 'article_id']==article_id_k)]\n",
    "            sample_data_k_1 = data_k[data_k.loc[:, 'reader_token_str']==reader_group_1].iloc[0, :]\n",
    "            sample_data_k_2 = data_k[data_k.loc[:, 'reader_token_str']==reader_group_2].iloc[0, :]\n",
    "            post_text_k = data_k.loc[:, 'source_text'].iloc[0]\n",
    "            reader_group_sample_question_data.append([article_id_k, subreddit_i, post_text_k, sample_data_k_1.loc['target_text'], reader_group_1, sample_data_k_2.loc['target_text'], reader_group_2, reader_group_type_j])\n",
    "reader_group_sample_question_data = pd.DataFrame(reader_group_sample_question_data, \n",
    "                                                 columns=['article_id', 'subreddit', 'post', 'question_1', 'group_1', 'question_2', 'group_2', 'group_type'])\n",
    "display(reader_group_sample_question_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now! Let's generate questions for the same data using the reader-aware model, and organize to match the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "from test_question_generation import load_model\n",
    "model_cache_dir = '../../data/model_cache/'\n",
    "model_weight_file = '../../data/reddit_data/author_text_data/question_generation_model/checkpoint-114500/pytorch_model.bin'\n",
    "data_dir = '../../data/reddit_data/author_text_data/'\n",
    "model_type = 'bart_author'\n",
    "model, model_tokenizer = load_model(model_cache_dir, model_weight_file, model_type, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>reader_token_str</th>\n",
       "      <th>source_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1636010</th>\n",
       "      <td>7py853</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(154), tensor(7), tensor(5),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220202</th>\n",
       "      <td>7py853</td>\n",
       "      <td>&lt;US_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(154), tensor(7), tensor(5),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654293</th>\n",
       "      <td>7rkvv3</td>\n",
       "      <td>&lt;US_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(17), tensor(27), tensor(119...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199596</th>\n",
       "      <td>7rkvv3</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(17), tensor(27), tensor(119...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507009</th>\n",
       "      <td>814t9v</td>\n",
       "      <td>&lt;NONUS_AUTHOR&gt;</td>\n",
       "      <td>[tensor(0), tensor(1141), tensor(6), tensor(81...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id reader_token_str  \\\n",
       "1636010     7py853   <NONUS_AUTHOR>   \n",
       "1220202     7py853      <US_AUTHOR>   \n",
       "1654293     7rkvv3      <US_AUTHOR>   \n",
       "1199596     7rkvv3   <NONUS_AUTHOR>   \n",
       "1507009     814t9v   <NONUS_AUTHOR>   \n",
       "\n",
       "                                                source_ids  \\\n",
       "1636010  [tensor(0), tensor(154), tensor(7), tensor(5),...   \n",
       "1220202  [tensor(0), tensor(154), tensor(7), tensor(5),...   \n",
       "1654293  [tensor(0), tensor(17), tensor(27), tensor(119...   \n",
       "1199596  [tensor(0), tensor(17), tensor(27), tensor(119...   \n",
       "1507009  [tensor(0), tensor(1141), tensor(6), tensor(81...   \n",
       "\n",
       "                                            attention_mask  \n",
       "1636010  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1220202  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1654293  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1199596  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1507009  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## subset data to articles and reader groups mentioned in sample data\n",
    "generation_data = train_data[train_data.loc[:, 'article_id'].isin(reader_group_sample_question_data.loc[:, 'article_id'].unique())].drop_duplicates(['article_id', 'reader_token_str'])\n",
    "article_id_valid_tokens = reader_group_sample_question_data.groupby('article_id').apply(lambda x: x.iloc[0, :].loc[['group_1', 'group_2']].values.tolist())\n",
    "generation_data = generation_data[generation_data.apply(lambda x: x.loc['reader_token_str'] in article_id_valid_tokens.loc[x.loc['article_id']], axis=1)]\n",
    "generation_data.sort_values('article_id', inplace=True)\n",
    "generation_data = generation_data.loc[:, ['article_id', 'reader_token_str', 'source_ids_reader_token', 'attention_mask']]\n",
    "generation_data.rename(columns={'source_ids_reader_token' : 'source_ids'}, inplace=True)\n",
    "generation_data.drop_duplicates(['article_id', 'reader_token_str'], inplace=True)\n",
    "print(generation_data.shape[0])\n",
    "# fix tensor vars\n",
    "generation_data = generation_data.assign(**{\n",
    "    'source_ids' : generation_data.loc[:, 'source_ids'].apply(lambda x: torch.LongTensor(x)),\n",
    "    'attention_mask' : generation_data.loc[:, 'attention_mask'].apply(lambda x: torch.LongTensor(x)),\n",
    "})\n",
    "# convert to list of dicts\n",
    "generation_data_iter = generation_data.apply(lambda x: x.to_dict(), axis=1).values.tolist()\n",
    "display(generation_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1634/1634 [07:34<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "## generate text for all source examples\n",
    "from model_helpers import generate_predictions\n",
    "generation_method = 'beam_search'\n",
    "num_beams = 8\n",
    "pred_text = generate_predictions(model, generation_data_iter, model_tokenizer, generation_method=generation_method, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 generated pairs total\n"
     ]
    }
   ],
   "source": [
    "## re-add to generation data\n",
    "generation_pred_data = generation_data.assign(**{\n",
    "    'pred_text' : pred_text\n",
    "})\n",
    "## reorganize to match original sampled data\n",
    "reader_token_groups = {\n",
    "    'LOC' : ['<US_AUTHOR>', '<NONUS_AUTHOR>'],\n",
    "    'EXPERT' : ['<EXPERT_PCT_0_AUTHOR>', '<EXPERT_PCT_1_AUTHOR>'],\n",
    "    'TIME' : ['<RESPONSE_TIME_0_AUTHOR>', '<RESPONSE_TIME_1_AUTHOR>'],\n",
    "}\n",
    "reader_token_group_lookup = {v1 : k for k,v in reader_token_groups.items() for v1 in v}\n",
    "def flatten_pred_data(data, reader_token_group_lookup):\n",
    "    reader_group_type = reader_token_group_lookup[data.loc[:, 'reader_token_str'].iloc[0]]\n",
    "    data_1 = data.iloc[0, :]\n",
    "    data_2 = data.iloc[1, :]\n",
    "    flat_data = [data_1.loc['pred_text'], data_1.loc['reader_token_str'], \n",
    "                 data_2.loc['pred_text'], data_2.loc['reader_token_str'],\n",
    "                 reader_group_type]\n",
    "    flat_data_cols = ['question_1', 'group_1', 'question_2', 'group_2', 'group_type']\n",
    "    flat_data = pd.Series(flat_data, index=flat_data_cols)\n",
    "    return flat_data\n",
    "per_article_generation_pred_data = generation_pred_data.groupby('article_id').apply(lambda x: flatten_pred_data(x, reader_token_group_lookup)).reset_index()\n",
    "# remove duplicates\n",
    "per_article_generation_pred_data = per_article_generation_pred_data[per_article_generation_pred_data.loc[:, 'question_1']!=per_article_generation_pred_data.loc[:, 'question_2']]\n",
    "## join with metadata\n",
    "per_article_generation_pred_data = pd.merge(per_article_generation_pred_data, reader_group_sample_question_data.loc[:, ['article_id', 'subreddit', 'post']].drop_duplicates('article_id'), on='article_id', how='left')\n",
    "print(f'{per_article_generation_pred_data.shape[0]} generated pairs total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the real and generated data, let's shuffle the groups to prepare for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_questions_by_group(data, num_groups, group_vars=['question', 'group']):\n",
    "    ordered_group_cols = [[f'{var}_{i}' for var in group_vars] for i in range(1, num_groups+1)]\n",
    "    group_cols = list(ordered_group_cols)\n",
    "    np.random.shuffle(group_cols)\n",
    "    flat_group_cols = [y for x in group_cols for y in x]\n",
    "    flat_ordered_group_cols = [y for x in ordered_group_cols for y in x]\n",
    "    group_data = data.loc[flat_group_cols]\n",
    "    group_data.index = flat_ordered_group_cols\n",
    "    data.drop(flat_ordered_group_cols, inplace=True)\n",
    "    data = data.append(group_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7py853</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>Turning to the experts at reddit!  I'm in a ba...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Are you sure it's not a scam and not a identit...</td>\n",
       "      <td>Are you sure it's not a scam?</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814t9v</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>My wife, over a year ago, thought she submitte...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Did she file for deferment?</td>\n",
       "      <td>Did you send her a certified letter or just a ...</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89p14v</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Every Tuesday my neighbours garbage goes flyin...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Are you sure it wasn’t your mom’s personal junk?</td>\n",
       "      <td>How did you get your garbage there?</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8c71q3</td>\n",
       "      <td>Advice</td>\n",
       "      <td>My ex girlfriend is very suicidal. We broke up...</td>\n",
       "      <td>LOC</td>\n",
       "      <td>How long have you been with her?</td>\n",
       "      <td>How long have you been with this person?</td>\n",
       "      <td>US vs. non-US</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8kc90i</td>\n",
       "      <td>Advice</td>\n",
       "      <td>First off, I have no problem with weed because...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Is she trying to use the marijuana for feeling...</td>\n",
       "      <td>What is her motivation behind it?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id        subreddit  \\\n",
       "0     7py853      legaladvice   \n",
       "1     814t9v  personalfinance   \n",
       "2     89p14v           Advice   \n",
       "3     8c71q3           Advice   \n",
       "4     8kc90i           Advice   \n",
       "\n",
       "                                                post group_type  \\\n",
       "0  Turning to the experts at reddit!  I'm in a ba...        LOC   \n",
       "1  My wife, over a year ago, thought she submitte...        LOC   \n",
       "2  Every Tuesday my neighbours garbage goes flyin...        LOC   \n",
       "3  My ex girlfriend is very suicidal. We broke up...        LOC   \n",
       "4  First off, I have no problem with weed because...     EXPERT   \n",
       "\n",
       "                                          question_1  \\\n",
       "0  Are you sure it's not a scam and not a identit...   \n",
       "1                        Did she file for deferment?   \n",
       "2   Are you sure it wasn’t your mom’s personal junk?   \n",
       "3                   How long have you been with her?   \n",
       "4  Is she trying to use the marijuana for feeling...   \n",
       "\n",
       "                                          question_2 group_type_choices  label  \n",
       "0                      Are you sure it's not a scam?      US vs. non-US     -1  \n",
       "1  Did you send her a certified letter or just a ...      US vs. non-US     -1  \n",
       "2                How did you get your garbage there?      US vs. non-US     -1  \n",
       "3           How long have you been with this person?      US vs. non-US     -1  \n",
       "4                  What is her motivation behind it?  expert vs. novice     -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8kc90i</td>\n",
       "      <td>Advice</td>\n",
       "      <td>First off, I have no problem with weed because...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Is she trying to use the marijuana as a shortc...</td>\n",
       "      <td>but is it worth being caught?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8l91i1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>I work online and last week my boss called and...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Did you thank him for been a good boss and why...</td>\n",
       "      <td>Now, how's that cold?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8p4uwz</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Was in the midst of a panic attack over some t...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>How would you feel if you or your family got a...</td>\n",
       "      <td>Did any one see that you hit the car?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8v8mga</td>\n",
       "      <td>Advice</td>\n",
       "      <td>In other words, I don't know who I am. I suck ...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>Do you doubt yourself, your choices, etc.?</td>\n",
       "      <td>Have you ever tried keeping a diary or journal?</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8vm0a1</td>\n",
       "      <td>Advice</td>\n",
       "      <td>Hey, I’m 19 and a half and I’ve been out of hi...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>What problem do you wish was solved in the world?</td>\n",
       "      <td>It was very different from School or College, ...</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id subreddit                                               post  \\\n",
       "0     8kc90i    Advice  First off, I have no problem with weed because...   \n",
       "1     8l91i1    Advice  I work online and last week my boss called and...   \n",
       "2     8p4uwz    Advice  Was in the midst of a panic attack over some t...   \n",
       "3     8v8mga    Advice  In other words, I don't know who I am. I suck ...   \n",
       "4     8vm0a1    Advice  Hey, I’m 19 and a half and I’ve been out of hi...   \n",
       "\n",
       "  group_type                                         question_1  \\\n",
       "0     EXPERT  Is she trying to use the marijuana as a shortc...   \n",
       "1     EXPERT  Did you thank him for been a good boss and why...   \n",
       "2     EXPERT  How would you feel if you or your family got a...   \n",
       "3     EXPERT         Do you doubt yourself, your choices, etc.?   \n",
       "4     EXPERT  What problem do you wish was solved in the world?   \n",
       "\n",
       "                                          question_2 group_type_choices  label  \n",
       "0                      but is it worth being caught?  expert vs. novice     -1  \n",
       "1                              Now, how's that cold?  expert vs. novice     -1  \n",
       "2              Did any one see that you hit the car?  expert vs. novice     -1  \n",
       "3    Have you ever tried keeping a diary or journal?  expert vs. novice     -1  \n",
       "4  It was very different from School or College, ...  expert vs. novice     -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_groups = 2\n",
    "shuffled_generation_pred_data = per_article_generation_pred_data.apply(lambda x: shuffle_questions_by_group(x, num_groups), axis=1)\n",
    "shuffled_reader_group_sample_question_data = reader_group_sample_question_data.apply(lambda x: shuffle_questions_by_group(x, num_groups), axis=1)\n",
    "# align columns lol\n",
    "shuffled_generation_pred_data = shuffled_generation_pred_data.loc[:, shuffled_reader_group_sample_question_data.columns]\n",
    "## prepare for annotation\n",
    "reader_group_type_choices = pd.DataFrame([\n",
    "    ['EXPERT', 'expert vs. novice'],\n",
    "    ['TIME', 'fast response vs. slow response'],\n",
    "    ['LOC', 'US vs. non-US'],\n",
    "], columns=['group_type', 'group_type_choices'])\n",
    "def prepare_for_annotation(data, reader_group_type_choices):\n",
    "    # drop labels\n",
    "    annotation_data = data.drop(['group_1', 'group_2'], axis=1)\n",
    "    # add choices\n",
    "    annotation_data = pd.merge(annotation_data, reader_group_type_choices, on=['group_type'], how='left')\n",
    "    annotation_data = annotation_data.assign(**{'question_1_group_label' : -1})\n",
    "    return annotation_data\n",
    "annotation_generation_pred_data = prepare_for_annotation(shuffled_generation_pred_data, reader_group_type_choices)\n",
    "annotation_reader_group_sample_question_data = prepare_for_annotation(shuffled_reader_group_sample_question_data, reader_group_type_choices)\n",
    "display(annotation_generation_pred_data.head())\n",
    "display(annotation_reader_group_sample_question_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmItheAsshole      300\n",
       "personalfinance    184\n",
       "Advice             174\n",
       "legaladvice        146\n",
       "pcmasterrace        78\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AmItheAsshole      87\n",
       "personalfinance    59\n",
       "Advice             55\n",
       "legaladvice        41\n",
       "pcmasterrace       18\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check subreddit distribution\n",
    "display(annotation_reader_group_sample_question_data.loc[:, 'subreddit'].value_counts())\n",
    "display(annotation_generation_pred_data.loc[:, 'subreddit'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-124-c260bb5366e4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  annotation_reader_group_sample_question_data.sort_values(['subreddit', 'article_id'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## restrict to same posts\n",
    "annotation_reader_group_sample_question_data = annotation_reader_group_sample_question_data[annotation_reader_group_sample_question_data.loc[:, 'article_id'].isin(annotation_generation_pred_data.loc[:, 'article_id'].unique())]\n",
    "## sort\n",
    "annotation_reader_group_sample_question_data.sort_values(['subreddit', 'article_id'], inplace=True)\n",
    "annotation_generation_pred_data.sort_values(['subreddit', 'article_id'], inplace=True)\n",
    "print(annotation_reader_group_sample_question_data.shape[0])\n",
    "print(annotation_generation_pred_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add real/generated labels\n",
    "shuffled_reader_group_sample_question_data = shuffled_reader_group_sample_question_data.assign(**{\n",
    "    'question_type' : 'real',\n",
    "})\n",
    "shuffled_generation_pred_data = shuffled_generation_pred_data.assign(**{\n",
    "    'question_type' : 'author_token_model',\n",
    "})\n",
    "## combine\n",
    "combined_ground_truth_data = pd.concat([\n",
    "    shuffled_reader_group_sample_question_data,\n",
    "    shuffled_generation_pred_data\n",
    "], axis=0)\n",
    "combined_annotation_data = pd.concat([\n",
    "    annotation_reader_group_sample_question_data,\n",
    "    annotation_generation_pred_data,\n",
    "])\n",
    "## sort\n",
    "combined_annotation_data.sort_values(['article_id', 'subreddit', 'group_type_choices'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ground truth data\n",
    "combined_ground_truth_data.to_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/reader_group_ground_truth_data.tsv', sep='\\t', index=False)\n",
    "# write annotation data\n",
    "combined_annotation_data.to_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/reader_group_annotation_data.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pilot annotations (round 1)\n",
    "Let's test the pilot annotations (from `r/PCMasterRace`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'reader_token_str', 'post', 'text_only_model_pred_text',\n",
      "       'reader_model_pred_text', 'subreddit', 'question_1', 'system_1',\n",
      "       'question_2', 'system_2'],\n",
      "      dtype='object')\n",
      "Index(['article_id', 'subreddit', 'post', 'group_type', 'question_1',\n",
      "       'group_1', 'question_2', 'group_2', 'question_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## load ground-truth data\n",
    "import pandas as pd\n",
    "text_quality_data = pd.read_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/text_quality_ground_truth_data.tsv', sep='\\t', index_col=False)\n",
    "reader_group_data = pd.read_csv('../../data/reddit_data/annotation_data/generated_text_evaluation/reader_group_ground_truth_data.tsv', sep='\\t', index_col=False)\n",
    "# fix reader group columns\n",
    "reader_group_type_lookup = {\n",
    "    '<EXPERT_PCT_0_AUTHOR>' : 'non-expert',\n",
    "    '<EXPERT_PCT_1_AUTHOR>' : 'expert',\n",
    "    '<RESPONSE_TIME_0_AUTHOR>' : 'slow',\n",
    "    '<RESPONSE_TIME_1_AUTHOR>' : 'high',\n",
    "}\n",
    "import re\n",
    "reader_group_cols = list(filter(lambda x: re.match('group_\\d', x), reader_group_data.columns))\n",
    "for reader_group_col in reader_group_cols:\n",
    "    reader_group_data = reader_group_data.assign(**{\n",
    "        reader_group_col : reader_group_data.loc[:, reader_group_col].apply(reader_group_type_lookup.get)\n",
    "    })\n",
    "print(text_quality_data.columns)\n",
    "print(reader_group_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "def collect_organize_annotation_data(data_files):\n",
    "    combined_data = []\n",
    "    data_type = re.search('(.+)(?=_annotation_data)', os.path.basename(data_files[0])).group(0)\n",
    "    if(data_type == 'text_quality'):\n",
    "        annotation_cols = ['text_that_is_more_relevant', 'text_that_is_more_fluent', 'text_that_is_more_likely_helpful']\n",
    "    elif(data_type == 'reader_group'):\n",
    "        annotation_cols = ['question_1_group_label']\n",
    "    for data_file in data_files:\n",
    "        data = pd.read_csv(data_file, sep='\\t', index_col=False)\n",
    "        annotator_id = int(data_file.replace('.tsv', '').split('_')[-1])\n",
    "        data.rename(columns={\n",
    "            col : f'{col}_{annotator_id}'\n",
    "            for col in annotation_cols\n",
    "        }, inplace=True)\n",
    "        combined_data.append(data)\n",
    "    if(data_type == 'text_quality'):\n",
    "        join_cols = ['article_id', 'subreddit']\n",
    "    # for reader group annotation, some posts have multiple question tests (e.g. US/non-US and expert/non-expert)\n",
    "    elif(data_type == 'reader_group'):\n",
    "        join_cols = ['article_id', 'subreddit', 'question_1', 'question_2']\n",
    "    # drop dulicate columns\n",
    "#     for data in combined_data:\n",
    "#         data.drop_duplicates(join_cols, inplace=True)\n",
    "    clean_data = combined_data[0].copy()\n",
    "    label_col_matcher = re.compile('|'.join(list(map(lambda x: f'{x}_\\d', annotation_cols))))\n",
    "    for data in combined_data[1:]:\n",
    "#         print(f'clean data cols {clean_data.columns}')\n",
    "#         print(f'data cols {data.columns}')\n",
    "#         label_cols = list(filter(lambda x: re.match('.+_\\d', x), data.columns))\n",
    "        label_cols = list(filter(lambda x: label_col_matcher.match(x) is not None, data.columns))\n",
    "        join_label_cols = list(set(join_cols)|set(label_cols))\n",
    "        clean_data = pd.merge(clean_data, data.loc[:, join_label_cols], on=join_cols, how='outer')\n",
    "    # remove null labels\n",
    "    clean_label_cols = list(filter(lambda x: re.match('.+_\\d', x), clean_data.columns))\n",
    "    null_label_val = -1\n",
    "    clean_data = clean_data[clean_data.loc[:, clean_label_cols].max(axis=1) != null_label_val]\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 data for text quality\n",
      "36 data for text quality\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>post</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>text_that_is_more_relevant_2</th>\n",
       "      <th>text_that_is_more_fluent_2</th>\n",
       "      <th>text_that_is_more_likely_helpful_2</th>\n",
       "      <th>Comment</th>\n",
       "      <th>text_that_is_more_relevant_1</th>\n",
       "      <th>text_that_is_more_likely_helpful_1</th>\n",
       "      <th>text_that_is_more_fluent_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>az4os4</td>\n",
       "      <td>This is my first build and they are all brand ...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>What are you trying to achieve here?</td>\n",
       "      <td>Do you have a graphics card installed?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>czgw6r</td>\n",
       "      <td>Hi guys, I recently faced a problem on Intel X...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Do you have the same problem in another system?</td>\n",
       "      <td>Do you have an Intel CPU?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aorth3</td>\n",
       "      <td>I seem to have a performance issue with Hitman...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Is it at 1080p or 1440p?</td>\n",
       "      <td>Does it do it at 1080 or 1440p?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>same question</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agykvp</td>\n",
       "      <td>Hello Everbody,   I want to build a new pc for...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>What are you going to be doing with the pc?</td>\n",
       "      <td>Are you okay with that?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arenff</td>\n",
       "      <td>I have one of those Intel Stock coolers, in a ...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>What's your temps like?</td>\n",
       "      <td>Do you have good airflow in your case?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                               post     subreddit  \\\n",
       "0     az4os4  This is my first build and they are all brand ...  pcmasterrace   \n",
       "1     czgw6r  Hi guys, I recently faced a problem on Intel X...  pcmasterrace   \n",
       "2     aorth3  I seem to have a performance issue with Hitman...  pcmasterrace   \n",
       "3     agykvp  Hello Everbody,   I want to build a new pc for...  pcmasterrace   \n",
       "4     arenff  I have one of those Intel Stock coolers, in a ...  pcmasterrace   \n",
       "\n",
       "                                        question_1  \\\n",
       "0             What are you trying to achieve here?   \n",
       "1  Do you have the same problem in another system?   \n",
       "2                         Is it at 1080p or 1440p?   \n",
       "3      What are you going to be doing with the pc?   \n",
       "4                          What's your temps like?   \n",
       "\n",
       "                               question_2  text_that_is_more_relevant_2  \\\n",
       "0  Do you have a graphics card installed?                             2   \n",
       "1               Do you have an Intel CPU?                             2   \n",
       "2         Does it do it at 1080 or 1440p?                             2   \n",
       "3                 Are you okay with that?                             1   \n",
       "4  Do you have good airflow in your case?                             1   \n",
       "\n",
       "   text_that_is_more_fluent_2  text_that_is_more_likely_helpful_2  \\\n",
       "0                           1                                   1   \n",
       "1                           1                                   2   \n",
       "2                           2                                   2   \n",
       "3                           1                                   1   \n",
       "4                           2                                   2   \n",
       "\n",
       "         Comment  text_that_is_more_relevant_1  \\\n",
       "0            NaN                             2   \n",
       "1            NaN                             2   \n",
       "2  same question                             1   \n",
       "3            NaN                             1   \n",
       "4            NaN                             2   \n",
       "\n",
       "   text_that_is_more_likely_helpful_1  text_that_is_more_fluent_1  \n",
       "0                                   1                           2  \n",
       "1                                   2                           2  \n",
       "2                                   1                           1  \n",
       "3                                   1                           1  \n",
       "4                                   2                           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>question_1_group_label_2</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>comments</th>\n",
       "      <th>question_1_group_label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8mz1o1</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>hey i'm in need of some helpi have a beefy pc ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>Is that temp changing at all?</td>\n",
       "      <td>What system is that, what motherboard with whi...</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8mz1o1</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>hey i'm in need of some helpi have a beefy pc ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>What model is your cpu?</td>\n",
       "      <td>What temp is your monitor?</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>slow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>very hard to say for both</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8uajem</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Hello, I was curious as to what kind of stuff ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>Have you tried power cycling the monitor?</td>\n",
       "      <td>What do you mean the RAM was in the wrong way?</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>slow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8uajem</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Hello, I was curious as to what kind of stuff ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>Are you sure it's the RAM and not the motherbo...</td>\n",
       "      <td>What do you mean it's in the wrong way?</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>slow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8v9wk1</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>So I see everyone getting like around 60 fps. ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>Isnt this game a shitty port with 50 different...</td>\n",
       "      <td>are all your drivers up to date?</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id     subreddit                                               post  \\\n",
       "0     8mz1o1  pcmasterrace  hey i'm in need of some helpi have a beefy pc ...   \n",
       "1     8mz1o1  pcmasterrace  hey i'm in need of some helpi have a beefy pc ...   \n",
       "2     8uajem  pcmasterrace  Hello, I was curious as to what kind of stuff ...   \n",
       "3     8uajem  pcmasterrace  Hello, I was curious as to what kind of stuff ...   \n",
       "4     8v9wk1  pcmasterrace  So I see everyone getting like around 60 fps. ...   \n",
       "\n",
       "  group_type                                         question_1  \\\n",
       "0       TIME                      Is that temp changing at all?   \n",
       "1       TIME                            What model is your cpu?   \n",
       "2       TIME          Have you tried power cycling the monitor?   \n",
       "3       TIME  Are you sure it's the RAM and not the motherbo...   \n",
       "4       TIME  Isnt this game a shitty port with 50 different...   \n",
       "\n",
       "                                          question_2  \\\n",
       "0  What system is that, what motherboard with whi...   \n",
       "1                         What temp is your monitor?   \n",
       "2     What do you mean the RAM was in the wrong way?   \n",
       "3            What do you mean it's in the wrong way?   \n",
       "4                   are all your drivers up to date?   \n",
       "\n",
       "                group_type_choices question_1_group_label_2  Unnamed: 8  \\\n",
       "0  fast response vs. slow response                     fast         NaN   \n",
       "1  fast response vs. slow response                     slow         NaN   \n",
       "2  fast response vs. slow response                     slow         NaN   \n",
       "3  fast response vs. slow response                     slow         NaN   \n",
       "4  fast response vs. slow response                     fast         NaN   \n",
       "\n",
       "                    comments question_1_group_label_1  \n",
       "0                        NaN                     fast  \n",
       "1  very hard to say for both                     slow  \n",
       "2                        NaN                     slow  \n",
       "3                        NaN                     slow  \n",
       "4                        NaN                     fast  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = '../../data/reddit_data/annotation_data/generated_text_evaluation'\n",
    "text_quality_annotation_files = list(filter(lambda x: re.match('text_quality_.*pilot_\\d\\.tsv', x) is not None, os.listdir('../../data/reddit_data/annotation_data/generated_text_evaluation/')))\n",
    "reader_group_annotation_files = list(filter(lambda x: re.match('reader_group_.*pilot_\\d\\.tsv', x) is not None, os.listdir('../../data/reddit_data/annotation_data/generated_text_evaluation/')))\n",
    "text_quality_annotation_files = list(map(lambda x: os.path.join(data_dir, x), text_quality_annotation_files))\n",
    "reader_group_annotation_files = list(map(lambda x: os.path.join(data_dir, x), reader_group_annotation_files))\n",
    "text_quality_annotation_data = collect_organize_annotation_data(text_quality_annotation_files)\n",
    "reader_group_annotation_data = collect_organize_annotation_data(reader_group_annotation_files)\n",
    "print(f'{text_quality_annotation_data.shape[0]} data for text quality')\n",
    "print(f'{reader_group_annotation_data.shape[0]} data for text quality')\n",
    "display(text_quality_annotation_data.head())\n",
    "display(reader_group_annotation_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'subreddit', 'post', 'group_type', 'question_1',\n",
       "       'question_2', 'group_type_choices', 'question_1_group_label_2',\n",
       "       'Unnamed: 8', 'comments', 'question_1_group_label_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_group_annotation_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text_only_model_pred_text</th>\n",
       "      <th>reader_model_pred_text</th>\n",
       "      <th>reader_token_str</th>\n",
       "      <th>system_1</th>\n",
       "      <th>system_2</th>\n",
       "      <th>post</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>text_that_is_more_relevant_2</th>\n",
       "      <th>text_that_is_more_fluent_2</th>\n",
       "      <th>text_that_is_more_likely_helpful_2</th>\n",
       "      <th>Comment</th>\n",
       "      <th>text_that_is_more_relevant_1</th>\n",
       "      <th>text_that_is_more_likely_helpful_1</th>\n",
       "      <th>text_that_is_more_fluent_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>az4os4</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Do you have a graphics card installed?</td>\n",
       "      <td>What are you trying to achieve here?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>This is my first build and they are all brand ...</td>\n",
       "      <td>What are you trying to achieve here?</td>\n",
       "      <td>Do you have a graphics card installed?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>czgw6r</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Do you have the same problem in another system?</td>\n",
       "      <td>Do you have an Intel CPU?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>Hi guys, I recently faced a problem on Intel X...</td>\n",
       "      <td>Do you have the same problem in another system?</td>\n",
       "      <td>Do you have an Intel CPU?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aorth3</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Is it at 1080p or 1440p?</td>\n",
       "      <td>Does it do it at 1080 or 1440p?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>I seem to have a performance issue with Hitman...</td>\n",
       "      <td>Is it at 1080p or 1440p?</td>\n",
       "      <td>Does it do it at 1080 or 1440p?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>same question</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agykvp</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Are you okay with that?</td>\n",
       "      <td>What are you going to be doing with the pc?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>Hello Everbody,   I want to build a new pc for...</td>\n",
       "      <td>What are you going to be doing with the pc?</td>\n",
       "      <td>Are you okay with that?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arenff</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>What's your temps like?</td>\n",
       "      <td>Do you have good airflow in your case?</td>\n",
       "      <td>&lt;EXPERT_PCT_0_AUTHOR&gt;</td>\n",
       "      <td>text_only_model_pred_text</td>\n",
       "      <td>reader_model_pred_text</td>\n",
       "      <td>I have one of those Intel Stock coolers, in a ...</td>\n",
       "      <td>What's your temps like?</td>\n",
       "      <td>Do you have good airflow in your case?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id     subreddit                        text_only_model_pred_text  \\\n",
       "0     az4os4  pcmasterrace           Do you have a graphics card installed?   \n",
       "1     czgw6r  pcmasterrace  Do you have the same problem in another system?   \n",
       "2     aorth3  pcmasterrace                         Is it at 1080p or 1440p?   \n",
       "3     agykvp  pcmasterrace                          Are you okay with that?   \n",
       "4     arenff  pcmasterrace                          What's your temps like?   \n",
       "\n",
       "                        reader_model_pred_text       reader_token_str  \\\n",
       "0         What are you trying to achieve here?  <EXPERT_PCT_0_AUTHOR>   \n",
       "1                    Do you have an Intel CPU?  <EXPERT_PCT_0_AUTHOR>   \n",
       "2              Does it do it at 1080 or 1440p?  <EXPERT_PCT_0_AUTHOR>   \n",
       "3  What are you going to be doing with the pc?  <EXPERT_PCT_0_AUTHOR>   \n",
       "4       Do you have good airflow in your case?  <EXPERT_PCT_0_AUTHOR>   \n",
       "\n",
       "                    system_1                   system_2  \\\n",
       "0     reader_model_pred_text  text_only_model_pred_text   \n",
       "1  text_only_model_pred_text     reader_model_pred_text   \n",
       "2  text_only_model_pred_text     reader_model_pred_text   \n",
       "3     reader_model_pred_text  text_only_model_pred_text   \n",
       "4  text_only_model_pred_text     reader_model_pred_text   \n",
       "\n",
       "                                                post  \\\n",
       "0  This is my first build and they are all brand ...   \n",
       "1  Hi guys, I recently faced a problem on Intel X...   \n",
       "2  I seem to have a performance issue with Hitman...   \n",
       "3  Hello Everbody,   I want to build a new pc for...   \n",
       "4  I have one of those Intel Stock coolers, in a ...   \n",
       "\n",
       "                                        question_1  \\\n",
       "0             What are you trying to achieve here?   \n",
       "1  Do you have the same problem in another system?   \n",
       "2                         Is it at 1080p or 1440p?   \n",
       "3      What are you going to be doing with the pc?   \n",
       "4                          What's your temps like?   \n",
       "\n",
       "                               question_2  text_that_is_more_relevant_2  \\\n",
       "0  Do you have a graphics card installed?                             2   \n",
       "1               Do you have an Intel CPU?                             2   \n",
       "2         Does it do it at 1080 or 1440p?                             2   \n",
       "3                 Are you okay with that?                             1   \n",
       "4  Do you have good airflow in your case?                             1   \n",
       "\n",
       "   text_that_is_more_fluent_2  text_that_is_more_likely_helpful_2  \\\n",
       "0                           1                                   1   \n",
       "1                           1                                   2   \n",
       "2                           2                                   2   \n",
       "3                           1                                   1   \n",
       "4                           2                                   2   \n",
       "\n",
       "         Comment  text_that_is_more_relevant_1  \\\n",
       "0            NaN                             2   \n",
       "1            NaN                             2   \n",
       "2  same question                             1   \n",
       "3            NaN                             1   \n",
       "4            NaN                             2   \n",
       "\n",
       "   text_that_is_more_likely_helpful_1  text_that_is_more_fluent_1  \n",
       "0                                   1                           2  \n",
       "1                                   2                           2  \n",
       "2                                   1                           1  \n",
       "3                                   1                           1  \n",
       "4                                   2                           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>group_1</th>\n",
       "      <th>group_2</th>\n",
       "      <th>question_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>question_1_group_label_2</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>comments</th>\n",
       "      <th>question_1_group_label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg2q51</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>expert</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>real</td>\n",
       "      <td>What storage do you have?</td>\n",
       "      <td>This is going to sound ridiculous, but have yo...</td>\n",
       "      <td>So first off, I'm running an i5-7500, GTX1070,...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>novice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clojsj</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>expert</td>\n",
       "      <td>real</td>\n",
       "      <td>Is the graphics card getting enough airflow?</td>\n",
       "      <td>Did you change nvidia driver settings to displ...</td>\n",
       "      <td>I'm running the following and am unhappy with ...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>novice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d8yjen</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>expert</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>real</td>\n",
       "      <td>That may be an option instead of you paying to...</td>\n",
       "      <td>Hungarian here, I'm dropping my tuf505 for ser...</td>\n",
       "      <td>Just a few weeks ago I had an ROG STRIX 1070Ti...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>expert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8mz1o1</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>high</td>\n",
       "      <td>slow</td>\n",
       "      <td>real</td>\n",
       "      <td>Is that temp changing at all?</td>\n",
       "      <td>What system is that, what motherboard with whi...</td>\n",
       "      <td>hey i'm in need of some helpi have a beefy pc ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8uajem</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>slow</td>\n",
       "      <td>high</td>\n",
       "      <td>real</td>\n",
       "      <td>Have you tried power cycling the monitor?</td>\n",
       "      <td>What do you mean the RAM was in the wrong way?</td>\n",
       "      <td>Hello, I was curious as to what kind of stuff ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>slow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id     subreddit     group_1     group_2 question_type  \\\n",
       "0     bg2q51  pcmasterrace      expert  non-expert          real   \n",
       "1     clojsj  pcmasterrace  non-expert      expert          real   \n",
       "2     d8yjen  pcmasterrace      expert  non-expert          real   \n",
       "3     8mz1o1  pcmasterrace        high        slow          real   \n",
       "4     8uajem  pcmasterrace        slow        high          real   \n",
       "\n",
       "                                          question_1  \\\n",
       "0                          What storage do you have?   \n",
       "1       Is the graphics card getting enough airflow?   \n",
       "2  That may be an option instead of you paying to...   \n",
       "3                      Is that temp changing at all?   \n",
       "4          Have you tried power cycling the monitor?   \n",
       "\n",
       "                                          question_2  \\\n",
       "0  This is going to sound ridiculous, but have yo...   \n",
       "1  Did you change nvidia driver settings to displ...   \n",
       "2  Hungarian here, I'm dropping my tuf505 for ser...   \n",
       "3  What system is that, what motherboard with whi...   \n",
       "4     What do you mean the RAM was in the wrong way?   \n",
       "\n",
       "                                                post group_type  \\\n",
       "0  So first off, I'm running an i5-7500, GTX1070,...     EXPERT   \n",
       "1  I'm running the following and am unhappy with ...     EXPERT   \n",
       "2  Just a few weeks ago I had an ROG STRIX 1070Ti...     EXPERT   \n",
       "3  hey i'm in need of some helpi have a beefy pc ...       TIME   \n",
       "4  Hello, I was curious as to what kind of stuff ...       TIME   \n",
       "\n",
       "                group_type_choices question_1_group_label_2  Unnamed: 8  \\\n",
       "0                expert vs. novice                   expert         NaN   \n",
       "1                expert vs. novice               non-expert         NaN   \n",
       "2                expert vs. novice               non-expert         NaN   \n",
       "3  fast response vs. slow response                     fast         NaN   \n",
       "4  fast response vs. slow response                     slow         NaN   \n",
       "\n",
       "  comments question_1_group_label_1  \n",
       "0      NaN                   novice  \n",
       "1      NaN                   novice  \n",
       "2      NaN                   expert  \n",
       "3      NaN                     fast  \n",
       "4      NaN                     slow  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## reshape data\n",
    "text_quality_data_cols = ['article_id', 'subreddit', 'text_only_model_pred_text', 'reader_model_pred_text', 'reader_token_str', 'system_1', 'system_2']\n",
    "clean_text_quality_data = pd.merge(\n",
    "    text_quality_data.loc[:, text_quality_data_cols],\n",
    "    text_quality_annotation_data, on=['article_id', 'subreddit'],\n",
    ")\n",
    "reader_group_data_cols = ['article_id', 'subreddit', 'group_1', 'group_2', 'question_type', 'question_1', 'question_2']\n",
    "clean_reader_group_data = pd.merge(\n",
    "    reader_group_data.loc[:, reader_group_data_cols],\n",
    "    reader_group_annotation_data, on=['article_id', 'subreddit', 'question_1', 'question_2'],\n",
    ")\n",
    "display(clean_text_quality_data.head())\n",
    "display(clean_reader_group_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text quality data, how well do the two systems compare with one another in terms of fluency, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test\n",
    "def get_aggregate_label_counts(data, label_cols=[]):\n",
    "    label_counts = []\n",
    "    for col_i in label_cols:\n",
    "        label_cols_i = list(filter(lambda x: col_i in x, data.columns))\n",
    "        combined_labels_i = []\n",
    "        for label_col_j in label_cols_i:\n",
    "            # get name of system that matches question chosen\n",
    "            labels_j = data.apply(lambda x: x.loc[f'system_{x.loc[label_col_j]}'], axis=1)\n",
    "            combined_labels_i += labels_j.values.tolist()\n",
    "        combined_label_counts_i = pd.Series(combined_labels_i).value_counts()\n",
    "        combined_label_pct_i = combined_label_counts_i / combined_label_counts_i.sum()\n",
    "        combined_label_pct_i.loc['label_type'] = col_i\n",
    "        combined_label_pct_i.loc['N'] = len(combined_labels_i)\n",
    "        # test for significance lol\n",
    "        p_val = binom_test(combined_label_counts_i.values, n=len(combined_labels_i))\n",
    "        combined_label_pct_i.loc['binom_test'] = p_val\n",
    "        label_counts.append(combined_label_pct_i)\n",
    "    label_counts = pd.concat(label_counts, axis=1).transpose()\n",
    "    return label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reader_model_pred_text</th>\n",
       "      <th>text_only_model_pred_text</th>\n",
       "      <th>label_type</th>\n",
       "      <th>N</th>\n",
       "      <th>binom_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>text_that_is_more_relevant</td>\n",
       "      <td>60</td>\n",
       "      <td>0.698883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>text_that_is_more_fluent</td>\n",
       "      <td>60</td>\n",
       "      <td>0.245061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>text_that_is_more_likely_helpful</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0273401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>combined</td>\n",
       "      <td>180</td>\n",
       "      <td>0.020597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reader_model_pred_text text_only_model_pred_text  \\\n",
       "0               0.533333                  0.466667   \n",
       "1               0.583333                  0.416667   \n",
       "2                   0.65                      0.35   \n",
       "0               0.588889                  0.411111   \n",
       "\n",
       "                         label_type    N binom_test  \n",
       "0        text_that_is_more_relevant   60   0.698883  \n",
       "1          text_that_is_more_fluent   60   0.245061  \n",
       "2  text_that_is_more_likely_helpful   60  0.0273401  \n",
       "0                          combined  180   0.020597  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## compute aggregate counts for each system being chosen as \"better\"\n",
    "text_quality_label_cols = ['text_that_is_more_relevant', 'text_that_is_more_fluent', 'text_that_is_more_likely_helpful']\n",
    "text_quality_label_counts = get_aggregate_label_counts(clean_text_quality_data, label_cols=text_quality_label_cols)\n",
    "# get total stats\n",
    "model_cols = ['reader_model_pred_text', 'text_only_model_pred_text']\n",
    "N_total = text_quality_label_counts.loc[:, 'N'].sum()\n",
    "mean_choice_counts = text_quality_label_counts.apply(lambda x: x.loc[model_cols] * x.loc['N'], axis=1).sum(axis=0)\n",
    "mean_choice_pct = mean_choice_counts / N_total\n",
    "mean_choice_pct.loc['label_type'] = 'combined'\n",
    "mean_choice_pct.loc['N'] = N_total\n",
    "mean_choice_pct.loc['binom_test'] = binom_test(mean_choice_counts, N_total)\n",
    "text_quality_label_counts = text_quality_label_counts.append(pd.DataFrame(mean_choice_pct).transpose())\n",
    "display(text_quality_label_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We see a slight preference for the reader-aware model overall, but no significant preference for `relevant` and `fluent` labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the reader group guessing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>group_1</th>\n",
       "      <th>group_2</th>\n",
       "      <th>question_type</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>post</th>\n",
       "      <th>group_type</th>\n",
       "      <th>group_type_choices</th>\n",
       "      <th>question_1_group_label_2</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>comments</th>\n",
       "      <th>question_1_group_label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg2q51</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>expert</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>real</td>\n",
       "      <td>What storage do you have?</td>\n",
       "      <td>This is going to sound ridiculous, but have yo...</td>\n",
       "      <td>So first off, I'm running an i5-7500, GTX1070,...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>novice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clojsj</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>expert</td>\n",
       "      <td>real</td>\n",
       "      <td>Is the graphics card getting enough airflow?</td>\n",
       "      <td>Did you change nvidia driver settings to displ...</td>\n",
       "      <td>I'm running the following and am unhappy with ...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>novice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d8yjen</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>expert</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>real</td>\n",
       "      <td>That may be an option instead of you paying to...</td>\n",
       "      <td>Hungarian here, I'm dropping my tuf505 for ser...</td>\n",
       "      <td>Just a few weeks ago I had an ROG STRIX 1070Ti...</td>\n",
       "      <td>EXPERT</td>\n",
       "      <td>expert vs. novice</td>\n",
       "      <td>non-expert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>expert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8mz1o1</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>high</td>\n",
       "      <td>slow</td>\n",
       "      <td>real</td>\n",
       "      <td>Is that temp changing at all?</td>\n",
       "      <td>What system is that, what motherboard with whi...</td>\n",
       "      <td>hey i'm in need of some helpi have a beefy pc ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8uajem</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>slow</td>\n",
       "      <td>high</td>\n",
       "      <td>real</td>\n",
       "      <td>Have you tried power cycling the monitor?</td>\n",
       "      <td>What do you mean the RAM was in the wrong way?</td>\n",
       "      <td>Hello, I was curious as to what kind of stuff ...</td>\n",
       "      <td>TIME</td>\n",
       "      <td>fast response vs. slow response</td>\n",
       "      <td>slow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id     subreddit     group_1     group_2 question_type  \\\n",
       "0     bg2q51  pcmasterrace      expert  non-expert          real   \n",
       "1     clojsj  pcmasterrace  non-expert      expert          real   \n",
       "2     d8yjen  pcmasterrace      expert  non-expert          real   \n",
       "3     8mz1o1  pcmasterrace        high        slow          real   \n",
       "4     8uajem  pcmasterrace        slow        high          real   \n",
       "\n",
       "                                          question_1  \\\n",
       "0                          What storage do you have?   \n",
       "1       Is the graphics card getting enough airflow?   \n",
       "2  That may be an option instead of you paying to...   \n",
       "3                      Is that temp changing at all?   \n",
       "4          Have you tried power cycling the monitor?   \n",
       "\n",
       "                                          question_2  \\\n",
       "0  This is going to sound ridiculous, but have yo...   \n",
       "1  Did you change nvidia driver settings to displ...   \n",
       "2  Hungarian here, I'm dropping my tuf505 for ser...   \n",
       "3  What system is that, what motherboard with whi...   \n",
       "4     What do you mean the RAM was in the wrong way?   \n",
       "\n",
       "                                                post group_type  \\\n",
       "0  So first off, I'm running an i5-7500, GTX1070,...     EXPERT   \n",
       "1  I'm running the following and am unhappy with ...     EXPERT   \n",
       "2  Just a few weeks ago I had an ROG STRIX 1070Ti...     EXPERT   \n",
       "3  hey i'm in need of some helpi have a beefy pc ...       TIME   \n",
       "4  Hello, I was curious as to what kind of stuff ...       TIME   \n",
       "\n",
       "                group_type_choices question_1_group_label_2  Unnamed: 8  \\\n",
       "0                expert vs. novice                   expert         NaN   \n",
       "1                expert vs. novice               non-expert         NaN   \n",
       "2                expert vs. novice               non-expert         NaN   \n",
       "3  fast response vs. slow response                     fast         NaN   \n",
       "4  fast response vs. slow response                     slow         NaN   \n",
       "\n",
       "  comments question_1_group_label_1  \n",
       "0      NaN                   novice  \n",
       "1      NaN                   novice  \n",
       "2      NaN                   expert  \n",
       "3      NaN                     fast  \n",
       "4      NaN                     slow  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reader_group_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group type | # correct | # incorrect | text type (real/generated)\n",
    "from scipy.stats import binom_test\n",
    "def get_guess_pct_data(data, label_cols):\n",
    "    combined_label_guesses = []\n",
    "    for label_col_i in label_cols:\n",
    "        label_guesses_i = (data.loc[:, 'group_1']==data.loc[:, label_col_i]).astype(int)\n",
    "        combined_label_guesses += label_guesses_i.values.tolist()\n",
    "    ## get aggregate T/F guess counts\n",
    "    combined_label_guess_counts = pd.Series(combined_label_guesses).value_counts()\n",
    "    combined_label_guess_pct = combined_label_guess_counts / combined_label_guess_counts.sum()\n",
    "    combined_label_guess_pct.loc['N'] = len(combined_label_guesses)\n",
    "    # binom test\n",
    "    p_val = binom_test(combined_label_guess_counts.values, n=len(combined_label_guess_counts))\n",
    "    combined_label_guess_pct.loc['p_val'] = p_val\n",
    "    return combined_label_guess_pct\n",
    "def get_guess_counts_per_type(data, label_cols, type_var):\n",
    "    combined_label_guess_data = []\n",
    "    for group_i, data_i in data.groupby(type_var):\n",
    "        combined_label_guess_pct = get_guess_pct_data(data_i, label_cols)\n",
    "        combined_label_guess_pct.loc['type_val'] = group_i\n",
    "        combined_label_guess_pct.loc['type'] = type_var\n",
    "        combined_label_guess_data.append(combined_label_guess_pct)\n",
    "    combined_label_guess_data = pd.concat(combined_label_guess_data, axis=1).transpose().rename(columns={0:'wrong', 1:'right'})\n",
    "    return combined_label_guess_data\n",
    "def get_aggregate_guess_counts(data, label_col='question_1_group_label'):\n",
    "    label_cols = list(filter(lambda x: re.match(f'{label_col}_\\d', x), data.columns))\n",
    "    combined_label_guess_data = []\n",
    "    ## overall guesses\n",
    "    combined_label_guess_pct = get_guess_pct_data(data, label_cols)\n",
    "    # clean up\n",
    "    combined_label_guess_pct.loc['type'] = 'combined'\n",
    "    combined_label_guess_pct.loc['type_val'] = 'combined'\n",
    "    combined_label_guess_pct = pd.DataFrame(combined_label_guess_pct).transpose()\n",
    "    combined_label_guess_pct.rename(columns={0:'wrong', 1:'right'}, inplace=True)\n",
    "    ## guesses by reader group\n",
    "    reader_group_guess_count_data = get_guess_counts_per_type(data, label_cols, 'group_type')\n",
    "    ## guesses by question type\n",
    "    question_type_guess_count_data = get_guess_counts_per_type(data, label_cols, 'question_type')\n",
    "    combined_label_guess_data = [\n",
    "        combined_label_guess_pct,\n",
    "        reader_group_guess_count_data,\n",
    "        question_type_guess_count_data\n",
    "    ]\n",
    "    combined_label_guess_data = pd.concat(combined_label_guess_data, axis=0)\n",
    "    # sort columns\n",
    "    ordered_guess_cols = ['type', 'type_val', 'N', 'right', 'wrong', 'p_val']\n",
    "    combined_label_guess_data = combined_label_guess_data.loc[:, ordered_guess_cols]\n",
    "    return combined_label_guess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            type            type_val   N     right     wrong      p_val\n",
      "0       combined            combined  72  0.347222  0.652778  0.0127746\n",
      "0     group_type              EXPERT  12       0.5       0.5          1\n",
      "1     group_type                TIME  60  0.316667  0.683333  0.0062176\n",
      "0  question_type  author_token_model  36  0.388889  0.611111   0.242985\n",
      "1  question_type                real  36  0.305556  0.694444  0.0288167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-14478c8070d6>:23: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  combined_label_guess_data = pd.concat(combined_label_guess_data, axis=1).transpose().rename(columns={0:'wrong', 1:'right'})\n",
      "<ipython-input-41-14478c8070d6>:44: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  combined_label_guess_data = pd.concat(combined_label_guess_data, axis=0)\n"
     ]
    }
   ],
   "source": [
    "label_col = 'question_1_group_label'\n",
    "reader_group_guess_counts = get_aggregate_guess_counts(clean_reader_group_data, label_col=label_col)\n",
    "print(reader_group_guess_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So overall the annotators did not do well on the task except for the `EXPERT` data and for the `author_token_model` data (less bad than `real` data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 300])\n",
      "torch.Size([1025, 1, 300])\n",
      "torch.Size([300, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "## tmp\n",
    "import torch\n",
    "input_dim = 1024\n",
    "embed_dim = 300\n",
    "fake_text_data = torch.Tensor(input_dim, embed_dim).unsqueeze(1)\n",
    "fake_author_data = torch.Tensor(1, embed_dim).unsqueeze(1)\n",
    "print(fake_text_data.shape)\n",
    "combine_network = torch.nn.Linear(input_dim+1, input_dim)\n",
    "fake_text_author_data = torch.cat([fake_text_data, fake_author_data])\n",
    "print(fake_text_author_data.shape)\n",
    "fake_output = combine_network(fake_text_author_data.squeeze(1).T).unsqueeze(1)\n",
    "print(fake_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect generated text data: expert vs non-expert\n",
    "For a pilot test, let's collect examples of questions from expert and non-expert authors to rate for fluency/coherence/utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/py3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (1,2,8,9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "100%|██████████| 127373/127373 [01:22<00:00, 1552.50it/s]\n",
      "100%|██████████| 26913/26913 [00:14<00:00, 1799.30it/s]\n",
      "100%|██████████| 127373/127373 [01:24<00:00, 1498.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after paired sampling: question data has label distribution = 0.0      100636\n",
      "1.0      100636\n",
      "US         4306\n",
      "NONUS      4306\n",
      "Name: author_group, dtype: int64\n",
      "after paired sampling: question data has subreddit distribution = AmItheAsshole      160736\n",
      "personalfinance     20056\n",
      "Advice              12546\n",
      "legaladvice         11080\n",
      "pcmasterrace         5466\n",
      "Name: subreddit, dtype: int64\n",
      "(209884, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if('..' not in sys.path):\n",
    "    sys.path.append('..')\n",
    "from data_processing.data_helpers import load_sample_data\n",
    "sample_question_data = load_sample_data(sample_type='paired')\n",
    "print(sample_question_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load generated data\n",
    "import gzip\n",
    "import torch\n",
    "test_data = torch.load('../../data/reddit_data/combined_data_test_data.pt')\n",
    "test_data_df = test_data.data.to_pandas()\n",
    "test_data_df = test_data_df.loc[:, ['article_id', 'question_id', 'author', 'id']]\n",
    "text_only_model_data = list(map(lambda x: x.strip(), gzip.open('../../data/reddit_data/text_only_model/test_data_output_text.gz', 'rt')))\n",
    "reader_model_data = list(map(lambda x: x.strip(), gzip.open('../../data/reddit_data/author_text_data/author_attention_data/test_data_output_text.gz', 'rt')))\n",
    "test_data_df = test_data_df.assign(**{\n",
    "    'text_model' : text_only_model_data,\n",
    "    'reader_model' : reader_model_data,\n",
    "})\n",
    "test_data_df.rename(columns={'article_id' : 'parent_id'}, inplace=True)\n",
    "# combine with sample data\n",
    "import pandas as pd\n",
    "sample_data = pd.merge(sample_question_data, test_data_df, on=['question_id', 'parent_id', 'author'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# post | subreddit | expert Q (real) | expert Q (reader-attention model) | novice Q (reader-attention model) | Q (text-only model)\n",
    "import random\n",
    "random.seed(123)\n",
    "paired_group_data = []\n",
    "author_group_category_vals = {\n",
    "    'relative_time_bin' : [0, 1],\n",
    "    'expert_pct_bin' : [0, 1],\n",
    "    'location_region' : ['US', 'NONUS'],\n",
    "}\n",
    "num_group_vals = 2\n",
    "for (parent_id_i, group_category_i), data_i in sample_data.groupby(['parent_id', 'group_category']):\n",
    "    if(len(set(data_i.loc[:, 'author_group'].unique()) & set(author_group_category_vals[group_category_i]))==num_group_vals):\n",
    "        group_vals_i = author_group_category_vals[group_category_i]\n",
    "        random.shuffle(group_vals_i)\n",
    "        # randomly swap values for annotation!!\n",
    "        post_i = data_i.loc[:, 'post'].iloc[0]\n",
    "        text_output_i = data_i.loc[:, 'text_model'].iloc[0]\n",
    "        subreddit_i = data_i.loc[:, 'subreddit'].iloc[0]\n",
    "        paired_group_data_i = []\n",
    "        for j, group_val_j in enumerate(group_vals_i):\n",
    "            data_j = data_i[data_i.loc[:, 'author_group']==group_val_j].iloc[0, :]\n",
    "            # get real text, reader-aware text\n",
    "            paired_group_data_i.append(pd.Series([group_val_j, data_j.loc['reader_model'], data_j.loc['question'], data_j.loc['id']], \n",
    "                                                 index=[f'reader_group_{j+1}', f'reader_model_output_group_{j+1}', f'question_group_{j+1}', f'question_id_{j+1}']))\n",
    "        paired_group_data_i = pd.concat(paired_group_data_i, axis=0)\n",
    "        paired_group_data_i = paired_group_data_i.append(pd.Series([parent_id_i, post_i, subreddit_i, text_output_i, group_category_i], \n",
    "                                                                   index=['post_id', 'post_text', 'subreddit', 'text_model_output', 'reader_group_category']))\n",
    "#         paired_group_data_i = paired_group_data_i.append(pd.Series(group_vals_i, index=[f'group_{x+1}' for x in range(len(group_vals_i))]))\n",
    "        paired_group_data.append(paired_group_data_i)\n",
    "paired_group_data = pd.concat(paired_group_data, axis=1).transpose()\n",
    "# remove duplicate questions\n",
    "paired_group_data = paired_group_data[paired_group_data.loc[:, 'question_group_1']!=paired_group_data.loc[:, 'question_group_2']]\n",
    "paired_group_data = paired_group_data[paired_group_data.loc[:, 'reader_model_output_group_1']!=paired_group_data.loc[:, 'reader_model_output_group_2']]\n",
    "# add per-pair ID\n",
    "paired_group_data = paired_group_data.assign(**{\n",
    "    'pair_id' : paired_group_data.apply(lambda x: hash(x.loc['question_group_1']+x.loc['question_group_2']), axis=1)\n",
    "})\n",
    "# filter long posts\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "max_post_word_count = 300\n",
    "paired_group_data = paired_group_data.assign(**{\n",
    "    'post_len' : paired_group_data.loc[:, 'post_text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "})\n",
    "paired_group_data = paired_group_data[paired_group_data.loc[:, 'post_len']<=max_post_word_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmItheAsshole      222\n",
      "personalfinance     68\n",
      "Advice              51\n",
      "legaladvice         38\n",
      "pcmasterrace        21\n",
      "Name: subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(paired_group_data.loc[:, 'subreddit'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's organize this into annotation-friendly format for Qualtrics:\n",
    "\n",
    "- post\n",
    "\n",
    "`{post text}`\n",
    "\n",
    "- question quality annotation\n",
    "\n",
    "```\n",
    "[[Question:Matrix]]\n",
    "1. {question text}\n",
    "\n",
    "[[Choices]]\n",
    "ma Coherent\n",
    "mb Fluent\n",
    "mc Answerable\n",
    "\n",
    "[[Answers]]\n",
    "m1 Very\n",
    "m2 Somewhat\n",
    "m3 Neutral\n",
    "m4 Not very\n",
    "m5 Not at all\n",
    "```\n",
    "\n",
    "- question reader group comparison\n",
    "\n",
    "```\n",
    "[[Question:MC]]\n",
    "2. \n",
    "Q1: {question1 text}\n",
    "Q2: {question2 text}\n",
    "Which question was more likely written by a {group1} reader?\n",
    "\n",
    "[[Choices]]\n",
    "a Q1\n",
    "b Q2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "DEFAULT_GROUP_VAL_LOOKUP = {\n",
    "    'location_region' : 'US',\n",
    "    'expert_pct_bin' : 1,\n",
    "    'relative_time_bin' : 1,\n",
    "}\n",
    "DEFAULT_GROUP_VAL_NAME_LOOKUP = {\n",
    "    'location_region' : 'US',\n",
    "    'expert_pct_bin' : 'expert',\n",
    "    'relative_time_bin' : 'slow-response',\n",
    "}\n",
    "GROUP_VAL_ALL_NAME_LOOKUP = {\n",
    "    'location_region' : ['NONUS', 'US'],\n",
    "    'expert_pct_bin' : ['novice', 'expert'],\n",
    "    'relative_time_bin' : ['fast-response', 'slow-response']\n",
    "    \n",
    "}\n",
    "def convert_question_data_to_txt(data, question_vals=['question_group', 'reader_model_output_group'], question_num=1):\n",
    "    text = [f\"\"\"\n",
    "    [[Question:DB]]\n",
    "    Please read the following post and first rate the questions that follow the post according to how (1) relevant to the post, (2) understandable, and (3) answerable you believe that they are.\n",
    "    Next, read two additional questions and determine who wrote the questions.<br></br>\n",
    "    \n",
    "    Post:\\n\\n{data.loc['post_text']}\n",
    "    \"\"\"]\n",
    "    all_question_vals = ['text_model_output'] + [f'{q}_{question_num}' for q in question_vals]\n",
    "    # shuffle order\n",
    "    \n",
    "    combined_question_id = ','.join(data.loc[['question_id_1', 'question_id_2']].values)\n",
    "    reader_group_category = data.loc['reader_group_category']\n",
    "    question_id_base = f'post={data.loc[\"post_id\"]}_question={combined_question_id}_group={reader_group_category}_'\n",
    "    # question quality\n",
    "    q_ctr = 1\n",
    "    for i, question_val_i in enumerate(all_question_vals):\n",
    "        question_txt_i = f\"\"\"\n",
    "        [[Question:Matrix]]\n",
    "        [[ID:{question_id_base+'question='+question_val_i+'_quality_'+str(i+1)}]]\n",
    "        {q_ctr}. {data.loc[question_val_i]}\n",
    "        \n",
    "        [[Choices]]\n",
    "        Relevant\n",
    "        Understandable\n",
    "        Answerable\n",
    "        \n",
    "        [[Answers]]\n",
    "        Very\n",
    "        Somewhat\n",
    "        Neutral\n",
    "        Not very\n",
    "        Not at all\n",
    "        \"\"\"\n",
    "        text.append(question_txt_i)\n",
    "        q_ctr += 1\n",
    "    # reader groups\n",
    "    num_reader_groups = 2\n",
    "    default_group_val = DEFAULT_GROUP_VAL_LOOKUP[reader_group_category]\n",
    "    default_group_val_name = DEFAULT_GROUP_VAL_NAME_LOOKUP[reader_group_category]\n",
    "    group_val_names = GROUP_VAL_ALL_NAME_LOOKUP[reader_group_category]\n",
    "    reader_group_question_txt = f\"\"\"\n",
    "    [[Question:MC]]\n",
    "    [[ID:{question_id_base+'group'}]]\n",
    "    {q_ctr}. One of the following questions was written by a {group_val_names[0]} reader and the other question was written by a {group_val_names[1]} reader.\n",
    "    Which question is more likely to be written by a ***{default_group_val_name} reader?***\n",
    "    \n",
    "    [[Choices]]\n",
    "    \"\"\"\n",
    "    for i in range(1, num_reader_groups+1):\n",
    "        reader_group_question_txt += f\"\"\"\n",
    "        Q{i}: {data.loc['reader_model_output_group_'+str(i)]}\"\"\"\n",
    "    text.append(reader_group_question_txt)\n",
    "    text = ''.join(text)\n",
    "    text = re.sub('(?<=\\n)( ){3,}', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[Question:DB]]\n",
      "Please read the following post and first rate the questions that follow the post according to how (1) relevant to the post, (2) understandable, and (3) answerable you believe that they are.\n",
      "Next, read two additional questions and determine who wrote the questions.<br></br>\n",
      "\n",
      "Post:\n",
      "\n",
      "We're considering paying off our loans as early as tonight or tomorrow night.  The only thing we're worried about is dropping our emergency fund down to $10k.  Are we being irrational?  \n",
      "\n",
      "Wife's loans: $7k @ 6.8%\n",
      "\n",
      "My loans: $15k @ 5% (Variable)\n",
      "\n",
      "We both have pretty secure jobs, so I imagine we would be able to build the fund back up by the end of the year.  It's just tough to lose that large cushion.\n",
      "\n",
      "We're also planning to move to a new apartment by the end of May and we'll incur some expenses there.\n",
      "\n",
      "[[Question:Matrix]]\n",
      "[[ID:post=86gyim_question=dw6l7fr,dw5pl8p_group=location_region_question=text_model_output_quality_1]]\n",
      "1. What’s your risk appetite and do you want returns in 3/5/10 years?\n",
      "\n",
      "[[Choices]]\n",
      "Relevant\n",
      "Understandable\n",
      "Answerable\n",
      "\n",
      "[[Answers]]\n",
      "Very\n",
      "Somewhat\n",
      "Neutral\n",
      "Not very\n",
      "Not at all\n",
      "\n",
      "[[Question:Matrix]]\n",
      "[[ID:post=86gyim_question=dw6l7fr,dw5pl8p_group=location_region_question=question_group_1_quality_2]]\n",
      "2. Well, to start, is 32k 6 months of necessary living expenses for the two of you?\n",
      "\n",
      "[[Choices]]\n",
      "Relevant\n",
      "Understandable\n",
      "Answerable\n",
      "\n",
      "[[Answers]]\n",
      "Very\n",
      "Somewhat\n",
      "Neutral\n",
      "Not very\n",
      "Not at all\n",
      "\n",
      "[[Question:Matrix]]\n",
      "[[ID:post=86gyim_question=dw6l7fr,dw5pl8p_group=location_region_question=reader_model_output_group_1_quality_3]]\n",
      "3. What are the interest rates on your loans?\n",
      "\n",
      "[[Choices]]\n",
      "Relevant\n",
      "Understandable\n",
      "Answerable\n",
      "\n",
      "[[Answers]]\n",
      "Very\n",
      "Somewhat\n",
      "Neutral\n",
      "Not very\n",
      "Not at all\n",
      "\n",
      "[[Question:MC]]\n",
      "[[ID:post=86gyim_question=dw6l7fr,dw5pl8p_group=location_region_group]]\n",
      "4. One of the following questions was written by a NONUS reader and the other question was written by a US reader.\n",
      "Which question is more likely to be written by a ***US reader?***\n",
      "\n",
      "[[Choices]]\n",
      "\n",
      "Q1: What are the interest rates on your loans?\n",
      "Q2: Do you need to buy a new car and use extra money to pay off the loan?\n"
     ]
    }
   ],
   "source": [
    "print(convert_question_data_to_txt(paired_group_data.iloc[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's organize these into blocks according to subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_question_txt_by_subreddit(data):\n",
    "    subreddit_txt = []\n",
    "    for subreddit_i, data_i in data.groupby('subreddit'):\n",
    "        subreddit_question_data_i = data_i.apply(lambda x: convert_question_data_to_txt(x), axis=1).values\n",
    "        subreddit_question_data_txt_i = '[[AdvancedFormat]]'\n",
    "        subreddit_question_data_txt_i = '\\n'.join([\n",
    "            subreddit_question_data_txt_i,\n",
    "            '\\n\\n[[PageBreak]]\\n\\n'.join(subreddit_question_data_i),\n",
    "        ])\n",
    "        subreddit_question_data_txt_i = '\\n'.join([\n",
    "            f'[[Block:subreddit={subreddit_i}]]', \n",
    "            subreddit_question_data_txt_i],\n",
    "        )\n",
    "        subreddit_txt.append([subreddit_i, subreddit_question_data_txt_i])\n",
    "    subreddits, subreddit_question_data_txt = zip(*subreddit_txt)\n",
    "    return subreddits, subreddit_question_data_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to expert data for simplicity\n",
    "reader_group_category = 'expert_pct_bin'\n",
    "expert_paired_group_data = paired_group_data[paired_group_data.loc[:, 'reader_group_category']==reader_group_category]\n",
    "## sample data per subreddit\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "sample_size = 10\n",
    "annotation_data = []\n",
    "for subreddit_i, data_i in expert_paired_group_data.groupby('subreddit'):\n",
    "    data_i = data_i.loc[np.random.choice(data_i.index, sample_size, replace=(data_i.shape[0] < sample_size))]\n",
    "    annotation_data.append(data_i)\n",
    "annotation_data = pd.concat(annotation_data, axis=0)\n",
    "data_subreddits, data_subreddit_txt = group_question_txt_by_subreddit(annotation_data)\n",
    "## write everything\n",
    "import os\n",
    "out_dir = '../../data/reddit_data/annotation_data/generated_text_evaluation/compare_model_output/'\n",
    "annotation_data_out_file = os.path.join(out_dir, f'reader_group={reader_group_category}_annotation_data.tsv')\n",
    "annotation_data.to_csv(annotation_data_out_file, sep='\\t', index=False)\n",
    "for subreddit_i, data_txt_i in zip(data_subreddits, data_subreddit_txt):\n",
    "    txt_out_file_i = os.path.join(out_dir, f'subreddit={subreddit_i}_reader_group={reader_group_category}_qualtrics_survey.txt')\n",
    "    with open(txt_out_file_i, 'w') as txt_out_i:\n",
    "        txt_out_i.write(data_txt_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
